{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Date: 22-5-2017\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: caution: 十一黃金周\n",
    "\n",
    "df_merged_volume = pd.read_csv(\"../data/preprocessed_input_traffic_time_and_weather_interpolate_20min_phase1and2_train.csv\")\n",
    "\n",
    "# change \"Date\" to datetime object\n",
    "df_merged_volume['date'] = pd.to_datetime(df_merged_volume['date'])\n",
    "\n",
    "# construct \"time of day\"\n",
    "df_merged_volume['timeofday'] = df_merged_volume.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# Select the phase 1 day\n",
    "\n",
    "end_day = datetime.datetime(year=2016, month=10, day=18, hour=0, minute=0, second=0)\n",
    "\n",
    "df_merged_volume = df_merged_volume[(df_merged_volume['date'] < end_day)]\n",
    "\n",
    "# check any unreasonable rows\n",
    "df_merged_volume.tail(30)\n",
    "\n",
    "''' Cut some rows (proprecessing)'''\n",
    "df_merged_volume = df_merged_volume[4:]  # Cut of NaN rows at the beginning\n",
    "df_merged_volume = df_merged_volume.reset_index(drop=True)  # reindexing\n",
    "df_merged_volume\n",
    "\n",
    "''' Make the dataset stationary '''\n",
    "\n",
    "station_cols = 6  # select the first 6 columns for stationary\n",
    "\n",
    "df_merged_volume_copy = df_merged_volume.copy()\n",
    "\n",
    "for i in range(1, len(df_merged_volume_copy)):\n",
    "    df_merged_volume_copy.loc[i, df_merged_volume_copy.columns[0:station_cols]] = df_merged_volume.loc[i, df_merged_volume.columns[0:station_cols]] - df_merged_volume.loc[i-1, df_merged_volume.columns[0:station_cols]]\n",
    "\n",
    "# Check Stationary dataframe\n",
    "\n",
    "df_merged_volume_copy.tail()\n",
    "\n",
    "## Hidden the selecting time\n",
    "# select the time for training: 6:20-10:00 (5 + 6 timestamp) and 15:20-19:00 (5 + 6 timestamp)\n",
    "# sel_rows = df_merged_volume_copy[ ((df_merged_volume_copy.timeofday>= 6.3) & (df_merged_volume_copy.timeofday<10)) |\n",
    "#                             ((df_merged_volume_copy.timeofday>=15.3) & (df_merged_volume_copy.timeofday<19))]\n",
    "\n",
    "## This time, training all time (24hrs) except the first non-stationary row\n",
    "sel_rows = df_merged_volume_copy[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAFkCAYAAADoo9t2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKUpeREREJFP+P7OLHBpPnvU1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1211d4898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sel_rows.plot.box()  # box plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('A', 2)\n",
      "1 ('A', 3)\n",
      "2 ('B', 1)\n",
      "3 ('B', 3)\n",
      "4 ('C', 1)\n",
      "5 ('C', 3)\n",
      "6 date\n",
      "7 hour\n",
      "8 pressure\n",
      "9 sea_pressure\n",
      "10 wind_direction\n",
      "11 wind_speed\n",
      "12 temperature\n",
      "13 rel_humidity\n",
      "14 precipitation\n",
      "15 dayofweek\n",
      "16 is_holiday\n",
      "17 timeofday\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot for hour\n",
    "\n",
    "# for i in range(24):\n",
    "#     sel_rows['{}:00'.format(i)] = np.where(sel_rows.hour == i, 1, 0)\n",
    "\n",
    "# Check all the columns\n",
    "for idx, i in enumerate(sel_rows.columns):\n",
    "    print(idx, i)\n",
    "\n",
    "# select using columns\n",
    "\n",
    "using_cols = [\n",
    "#                 \"(1, 0, 'cargocar')\",\n",
    "#                 \"(1, 0, 'etc')\",\n",
    "#                 \"(1, 0, 'motorcycle')\",\n",
    "#                 \"(1, 0, 'privatecar')\",\n",
    "#                 \"(1, 0, 'tot')\",\n",
    "#                 \"(1, 0, 'unknowncar')\",\n",
    "#                 \"(1, 1, 'cargocar')\",\n",
    "#                 \"(1, 1, 'etc')\",\n",
    "#                 \"(1, 1, 'motorcycle')\",\n",
    "#                 \"(1, 1, 'privatecar')\",\n",
    "#                 \"(1, 1, 'tot')\",\n",
    "#                 \"(1, 1, 'unknowncar')\",\n",
    "#                 \"(2, 0, 'cargocar')\",\n",
    "#                 \"(2, 0, 'etc')\",\n",
    "#                 \"(2, 0, 'motorcycle')\",\n",
    "#                 \"(2, 0, 'privatecar')\",\n",
    "#                 \"(2, 0, 'tot')\",\n",
    "#                 \"(2, 0, 'unknowncar')\",\n",
    "#                 \"(3, 0, 'cargocar')\",\n",
    "#                 \"(3, 0, 'etc')\",\n",
    "#                 \"(3, 0, 'motorcycle')\",\n",
    "#                 \"(3, 0, 'privatecar')\",\n",
    "#                 \"(3, 0, 'tot')\",\n",
    "#                 \"(3, 0, 'unknowncar')\",\n",
    "#                 \"(3, 1, 'cargocar')\",\n",
    "#                 \"(3, 1, 'etc')\",\n",
    "#                 \"(3, 1, 'motorcycle')\",\n",
    "#                 \"(3, 1, 'privatecar')\",\n",
    "#                 \"(3, 1, 'tot')\",\n",
    "#                 \"(3, 1, 'unknowncar')\",\n",
    "#                 \"('A', 2)\",\n",
    "#                 \"('A', 3)\",\n",
    "                \"('B', 1)\",\n",
    "#                 \"('B', 3)\",\n",
    "                \"('C', 1)\",\n",
    "#                 \"('C', 3)\",\n",
    "#                 'date',  # <== Notice this\n",
    "                'hour',\n",
    "                'pressure',\n",
    "#                 'sea_pressure',\n",
    "#                 'wind_direction',\n",
    "#                 'wind_speed',\n",
    "                'temperature',\n",
    "#                 'rel_humidity',\n",
    "                'precipitation',\n",
    "                'dayofweek',\n",
    "                'is_holiday',\n",
    "                'timeofday',\n",
    "#                 '0:00',\n",
    "#                 '1:00',\n",
    "#                 '2:00',\n",
    "#                 '3:00',\n",
    "#                 '4:00',\n",
    "#                 '5:00',\n",
    "#                 '6:00',\n",
    "#                 '7:00',\n",
    "#                 '8:00',\n",
    "#                 '9:00',\n",
    "#                 '10:00',\n",
    "#                 '11:00',\n",
    "#                 '12:00',\n",
    "#                 '13:00',\n",
    "#                 '14:00',\n",
    "#                 '15:00',\n",
    "#                 '16:00',\n",
    "#                 '17:00',\n",
    "#                 '18:00',\n",
    "#                 '19:00',\n",
    "#                 '20:00',\n",
    "#                 '21:00',\n",
    "#                 '22:00',\n",
    "#                 '23:00',\n",
    "              ]\n",
    "\n",
    "sel_rows = sel_rows[using_cols]\n",
    "\n",
    "# split to train and valid set\n",
    "train_rows = sel_rows[: -24*3*7]\n",
    "valid_rows = sel_rows[-24*3*7:] # reserve 7 days for validation\n",
    "\n",
    "# get numpy array from panda dataframe\n",
    "train_arr = train_rows.values\n",
    "valid_arr = valid_rows.values\n",
    "\n",
    "# np.shape(train_arr)\n",
    "# Out:\n",
    "# (726, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats\n",
    "\n",
    "# train_arr.mean()\n",
    "# train_rows.loc[abs(train_rows[\"('A', 2)\"] + 814.225)<0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale feature array to range -1 to 1\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(train_arr)\n",
    "train_scaled_arr = scaler.transform(train_arr)\n",
    "\n",
    "valid_scaled_arr = scaler.transform(valid_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' save the scaler to another file use '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' save the scaler to another file use '''\n",
    "# import pickle\n",
    "# scalerfile = 'scaler-A-2_phase1.sav'\n",
    "# pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample subsequence from the time series\n",
    "train_seqs = []\n",
    "len_seqs = len(train_scaled_arr) - 6 + 1  # 6 is window size\n",
    "for i in range(len_seqs):\n",
    "    train_seqs.append(train_scaled_arr[i: i+6])  # append 6 timestamps each time (5 timestamps for x, 1 timestamp for y)\n",
    "train_seqs = np.stack(train_seqs)\n",
    "\n",
    "valid_seqs = []\n",
    "len_v_seqs = len(valid_scaled_arr) - 6 + 1  # 6 is window size\n",
    "for i in range(len_v_seqs):\n",
    "    valid_seqs.append(valid_scaled_arr[i: i+6])\n",
    "valid_seqs = np.stack(valid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37374879, -0.16327664, -0.82608696, -0.48043818,  0.28063241,\n",
       "        -1.        , -0.66666667, -1.        , -0.83098592],\n",
       "       [ 0.37374879, -0.16327664, -0.82608696, -0.48461137,  0.31752306,\n",
       "        -1.        , -0.66666667, -1.        , -0.8028169 ],\n",
       "       [ 0.37374879, -0.16327664, -0.82608696, -0.48878456,  0.3544137 ,\n",
       "        -1.        , -0.66666667, -1.        , -0.77464789],\n",
       "       [ 0.37374879, -0.16327664, -0.73913043, -0.49295775,  0.39130435,\n",
       "        -1.        , -0.66666667, -1.        , -0.74647887],\n",
       "       [ 0.37374879, -0.16327664, -0.73913043, -0.50130412,  0.3921827 ,\n",
       "        -1.        , -0.66666667, -1.        , -0.71830986],\n",
       "       [ 0.37374879, -0.16327664, -0.73913043, -0.5096505 ,  0.39306105,\n",
       "        -1.        , -0.66666667, -1.        , -0.69014085]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "train_seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras\n",
    "#https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "input_dim = len(using_cols)  # The features\n",
    "output_dim = 2  # \n",
    "timesteps = 5 # use 5 timesteps to predict the 6th\n",
    "\n",
    "x_train, y_train = train_seqs[:, 0:-1], train_seqs[:, -1, 0:output_dim]  # 0:output_dim is for deciding the output features\n",
    "x_valid , y_valid  =  valid_seqs[:, 0:-1],  valid_seqs[:, -1, 0:output_dim]  # 0:output_dim is for deciding the output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6038, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 120\n",
    "loss_fuc = 'mean_squared_error'\n",
    "\n",
    "# construct the callback\n",
    "filepath=\"best_epoch_T.M._B-1_C-1_phase1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(output_dim))\n",
    "model.compile(loss=loss_fuc, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 128)            70656     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 202,498\n",
      "Trainable params: 202,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6038 samples, validate on 499 samples\n",
      "Epoch 1/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00000: val_loss improved from inf to 0.00801, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 7s - loss: 0.0090 - val_loss: 0.0080\n",
      "Epoch 2/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0052Epoch 00001: val_loss improved from 0.00801 to 0.00793, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 3/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0051Epoch 00002: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0051 - val_loss: 0.0082\n",
      "Epoch 4/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00003: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0050 - val_loss: 0.0080\n",
      "Epoch 5/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00004: val_loss improved from 0.00793 to 0.00779, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0050 - val_loss: 0.0078\n",
      "Epoch 6/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00005: val_loss improved from 0.00779 to 0.00776, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0050 - val_loss: 0.0078\n",
      "Epoch 7/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0049Epoch 00006: val_loss improved from 0.00776 to 0.00769, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0049 - val_loss: 0.0077\n",
      "Epoch 8/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0049Epoch 00007: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0049 - val_loss: 0.0078\n",
      "Epoch 9/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0050Epoch 00008: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 10/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0049Epoch 00009: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0049 - val_loss: 0.0078\n",
      "Epoch 11/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0049Epoch 00010: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0049 - val_loss: 0.0078\n",
      "Epoch 12/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0048Epoch 00011: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0048 - val_loss: 0.0078\n",
      "Epoch 13/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0048Epoch 00012: val_loss improved from 0.00769 to 0.00753, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0048 - val_loss: 0.0075\n",
      "Epoch 14/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0048Epoch 00013: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0048 - val_loss: 0.0078\n",
      "Epoch 15/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0048Epoch 00014: val_loss improved from 0.00753 to 0.00740, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0048 - val_loss: 0.0074\n",
      "Epoch 16/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0048Epoch 00015: val_loss improved from 0.00740 to 0.00729, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0047 - val_loss: 0.0073\n",
      "Epoch 17/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0047Epoch 00016: val_loss improved from 0.00729 to 0.00724, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0047 - val_loss: 0.0072\n",
      "Epoch 18/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0047Epoch 00017: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0047 - val_loss: 0.0073\n",
      "Epoch 19/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0046Epoch 00018: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0046 - val_loss: 0.0073\n",
      "Epoch 20/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0046Epoch 00019: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0046 - val_loss: 0.0074\n",
      "Epoch 21/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0046Epoch 00020: val_loss improved from 0.00724 to 0.00711, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0046 - val_loss: 0.0071\n",
      "Epoch 22/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0046Epoch 00021: val_loss improved from 0.00711 to 0.00704, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 23/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00022: val_loss improved from 0.00704 to 0.00686, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0045 - val_loss: 0.0069\n",
      "Epoch 24/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00023: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 25/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00024: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0045 - val_loss: 0.0069\n",
      "Epoch 26/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00025: val_loss improved from 0.00686 to 0.00671, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0045 - val_loss: 0.0067\n",
      "Epoch 27/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00026: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0045 - val_loss: 0.0067\n",
      "Epoch 28/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00027: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0044 - val_loss: 0.0073\n",
      "Epoch 29/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0045Epoch 00028: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0045 - val_loss: 0.0070\n",
      "Epoch 30/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00029: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0044 - val_loss: 0.0069\n",
      "Epoch 31/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00030: val_loss improved from 0.00671 to 0.00643, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0044 - val_loss: 0.0064\n",
      "Epoch 32/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00031: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0044 - val_loss: 0.0065\n",
      "Epoch 33/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0044Epoch 00032: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0044 - val_loss: 0.0070\n",
      "Epoch 34/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00033: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0069\n",
      "Epoch 35/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00034: val_loss improved from 0.00643 to 0.00641, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0064\n",
      "Epoch 36/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00035: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0064\n",
      "Epoch 37/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0042Epoch 00036: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0068\n",
      "Epoch 38/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00037: val_loss improved from 0.00641 to 0.00595, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 39/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0043Epoch 00038: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0043 - val_loss: 0.0062\n",
      "Epoch 40/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0042Epoch 00039: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0042 - val_loss: 0.0062\n",
      "Epoch 41/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0042Epoch 00040: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0042 - val_loss: 0.0062\n",
      "Epoch 42/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0042Epoch 00041: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0042 - val_loss: 0.0069\n",
      "Epoch 43/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00042: val_loss improved from 0.00595 to 0.00592, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0042 - val_loss: 0.0059\n",
      "Epoch 44/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0042Epoch 00043: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0042 - val_loss: 0.0063\n",
      "Epoch 45/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00044: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 46/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00045: val_loss improved from 0.00592 to 0.00572, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0041 - val_loss: 0.0057\n",
      "Epoch 47/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00046: val_loss improved from 0.00572 to 0.00560, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 48/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00047: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0041 - val_loss: 0.0057\n",
      "Epoch 49/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00048: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 50/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00049: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 51/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00050: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0057\n",
      "Epoch 52/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00051: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 53/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0040Epoch 00052: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 54/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00053: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 55/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00054: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0059\n",
      "Epoch 56/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00055: val_loss improved from 0.00560 to 0.00548, saving model to best_epoch_T.M._B-1_C-1_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 57/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00056: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 58/120\n",
      "5952/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00057: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0062\n",
      "Epoch 59/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00058: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 60/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0041Epoch 00059: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0041 - val_loss: 0.0058\n",
      "Epoch 61/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00060: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0038 - val_loss: 0.0058\n",
      "Epoch 62/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00061: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0039 - val_loss: 0.0066\n",
      "Epoch 63/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0039Epoch 00062: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0039 - val_loss: 0.0061\n",
      "Epoch 64/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0038Epoch 00063: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 65/120\n",
      "4160/6038 [===================>..........] - ETA: 1s - loss: 0.0037"
     ]
    }
   ],
   "source": [
    "## Start Training\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=64, validation_data=(x_valid, y_valid))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Save Model '''\n",
    "\n",
    "# serialize model to JSON\n",
    "\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model_T.M._A-2-p1.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Load the saved model '''\n",
    "\n",
    "# load json and create model\n",
    "\n",
    "# json_file = open('model_500x5_300e.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "\n",
    "# loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Notes: last best model: 0.00175\n",
    "# load weights into the model\n",
    "model.load_weights(\"best_epoch_T.M._B-1_C-1_phase1.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' === Prediction ===\n",
    "Procedure:\n",
    "1. Load CSV\n",
    "2. to_datetime\n",
    "3. create timeofday column\n",
    "4. select the time for training: 6:00-8:00 (6 timestamps) and 15:00-17:00 (6 timestamps)\n",
    "5. change it to stationary\n",
    "6. Use using_cols to select the features\n",
    "7. change to np array\n",
    "8. MinMaxScaler\n",
    "9. make the sequences tensor as input\n",
    "10. make a forloop for prediction\n",
    "\n",
    "'''\n",
    "# 1. Load CSV - Vol + Route + Weather (Only Weather is 24-hour data)\n",
    "df_pred = pd.read_csv('../data/preprocessed_input_interpolate_20min_phase1and2_train.csv')\n",
    "\n",
    "# 2. to_datetime\n",
    "df_pred['date'] = pd.to_datetime(df_pred['date'])\n",
    "\n",
    "# 3. create timeofday column\n",
    "df_pred['timeofday'] = df_pred.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# Select the checking days (No need to real final test)\n",
    "\n",
    "start_day = datetime.datetime(year=2016, month=10, day=18, hour=1, minute=0, second=0)\n",
    "end_day = datetime.datetime(year=2016, month=10, day=24, hour=23, minute=0, second=0)\n",
    "\n",
    "df_pred_sel = df_pred[(df_pred['date'] > start_day) & (df_pred['date'] < end_day)]\n",
    "\n",
    "# 4. select the time for training\n",
    "\n",
    "df_pred_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 6) & (df_pred_sel.timeofday<8)) |\n",
    "                            ((df_pred_sel.timeofday>=15) & (df_pred_sel.timeofday<17))]\n",
    "\n",
    "df_feedin_weather_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 8) & (df_pred_sel.timeofday<10)) |\n",
    "                            ((df_pred_sel.timeofday>=17) & (df_pred_sel.timeofday<19))]\n",
    "\n",
    "## 4.1 (For checking Phase1 Test_answer only)\n",
    "''' For checking Answer'''\n",
    "df_check_answer = df_pred_sel[ ((df_pred_sel.timeofday>= 8) & (df_pred_sel.timeofday<10)) |\n",
    "                            ((df_pred_sel.timeofday>=17) & (df_pred_sel.timeofday<19))]\n",
    "\n",
    "df_check_answer = df_check_answer[using_cols]\n",
    "df_check_answer = df_check_answer[\"('B', 1)\"]\n",
    "check_ans_arr = df_check_answer.values\n",
    "\n",
    "check_ans_arr[0]\n",
    "\n",
    "\n",
    "# Checking\n",
    "df_pred_sel_time.iloc[12]\n",
    "\n",
    "# 5. change it to stationary\n",
    "df_pred_sel_time = df_pred_sel_time.reset_index(drop=True)\n",
    "\n",
    "df_pred_sel_time_copy = df_pred_sel_time.copy()\n",
    "\n",
    "for i in range(len(df_pred_sel_time_copy)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        start_idx = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot\n",
    "        df_pred_sel_time_copy.loc[start_idx, df_pred_sel_time_copy.columns[0:36]] = df_pred_sel_time.loc[start_idx, df_pred_sel_time.columns[0:36]] - df_pred_sel_time.loc[start_idx-1, df_pred_sel_time.columns[0:36]]\n",
    "\n",
    "# Create one-hot for it\n",
    "# for i in range(24):\n",
    "#     df_pred_sel_time_copy['{}:00'.format(i)] = np.where(df_pred_sel_time_copy.hour == i, 1, 0)\n",
    "#     df_feedin_weather_sel_time['{}:00'.format(i)] = np.where(df_feedin_weather_sel_time.hour == i, 1, 0)\n",
    "\n",
    "# 6. Use using_cols to select the features\n",
    "\n",
    "sel_rows_pred = df_pred_sel_time_copy[ using_cols ]\n",
    "\n",
    "sel_rows_feedin_weather = df_feedin_weather_sel_time[using_cols[output_dim:]]\n",
    "\n",
    "sel_rows_pred\n",
    "\n",
    "# 7. change to np array\n",
    "pred_arr = sel_rows_pred.values\n",
    "\n",
    "feedin_weather_arr = sel_rows_feedin_weather.values\n",
    "\n",
    "# 8. MinMaxScaler\n",
    "pred_arr_scaled = scaler.transform(pred_arr)\n",
    "\n",
    "# add some dummy cells in front of the weather_array for transform\n",
    "temp_arr = np.zeros((84,output_dim))\n",
    "feedin_weather_arr = np.concatenate([temp_arr, feedin_weather_arr], axis=1)\n",
    "\n",
    "feedin_weather_arr_scaled = scaler.transform(feedin_weather_arr)\n",
    "\n",
    "# Now pred_arr_scaled is (84 x features)\n",
    "\n",
    "# 9. make the sequences tensor as input\n",
    "# Put into the model to get the prediction\n",
    "\n",
    "ans_arr = []  # For holding the output answer\n",
    "    \n",
    "for i in range(len(pred_arr_scaled)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    # creating pre_seq\n",
    "    pred_seq = []\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        k = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot, to ignore the first index which is non-stationary\n",
    "        pred_seq.append(pred_arr_scaled[k])  # creating a sequence for a time slot\n",
    "    \n",
    "    # creating feedin_weather_seq\n",
    "    feedin_weather_seq = []\n",
    "    for t in range(6):  #  Do 6 times every loop\n",
    "        k = i*6 + t  #\n",
    "        feedin_weather_seq.append(feedin_weather_arr_scaled[k])\n",
    "\n",
    "\n",
    "    pred_seq = np.stack(pred_seq)  # change back to the numpy array (2D)\n",
    "    pred_seq = pred_seq.reshape(1, pred_seq.shape[0], pred_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    feedin_weather_seq = np.stack(feedin_weather_seq)  # change back to the numpy array (2D)\n",
    "    feedin_weather_seq = feedin_weather_seq.reshape(1, feedin_weather_seq.shape[0], feedin_weather_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    for q in range(6):\n",
    "        # predict next timestamp\n",
    "        output_pred = model.predict(pred_seq)  # get one prediction output (size (1 x output feature(s)))\n",
    "        ans_arr.append(output_pred)\n",
    "\n",
    "        # update the input seq\n",
    "        for j in range(1,5):\n",
    "            pred_seq[0][j-1] = pred_seq[0][j]\n",
    "        pred_seq[0][4] = feedin_weather_seq[0][q]\n",
    "        pred_seq[0][4][0:output_dim] = output_pred[0]\n",
    "\n",
    "# 10. Backward to the non-stationary, correct scale output\n",
    "\n",
    "#  Helper functions\n",
    "\n",
    "def backward_scaler(nn_output):\n",
    "    tmp = np.zeros(input_dim)\n",
    "    tmp[0:output_dim] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return tmp[0:output_dim]\n",
    "\n",
    "def decode(last_timestamp_values, nn_output):\n",
    "    tmp = np.zeros(input_dim)\n",
    "    tmp[0:output_dim] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return last_timestamp_values + tmp[0:output_dim]\n",
    "\n",
    "# create the non-stationary 6:40 and 16:40 for decoding\n",
    "df_non_station_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 7.5) & (df_pred_sel.timeofday<8)) |\n",
    "                            ((df_pred_sel.timeofday>=16.5) & (df_pred_sel.timeofday<17))]\n",
    "\n",
    "''' Output the non-stationary Answers (allAns)'''\n",
    "\n",
    "tmp = df_non_station_sel_time[using_cols[0:output_dim]].values\n",
    "allAns = []\n",
    "for i in range(len(tmp)):\n",
    "    seed = tmp[i]  # non-stationary for reconstructing a sequence\n",
    "    segmentAns = []\n",
    "    for timestep in range(6):\n",
    "        seed = decode(seed, ans_arr[i*6+timestep])\n",
    "        segmentAns.append(seed)\n",
    "    allAns.append(segmentAns)\n",
    "\n",
    "# Change back to np array for easy visualize\n",
    "allAns = np.array(allAns)\n",
    "\n",
    "# Checking\n",
    "for i in allAns:\n",
    "    print(i)\n",
    "\n",
    "# 11. Output the CSV file\n",
    "\n",
    "# create the datetime objects\n",
    "import datetime\n",
    "\n",
    "pred_start_date = 18\n",
    "\n",
    "\n",
    "start_8am = datetime.datetime(year=2016, month=10, day=pred_start_date, hour=8, minute=0, second=0)\n",
    "start_5pm = datetime.datetime(year=2016, month=10, day=pred_start_date, hour=17, minute=0, second=0)\n",
    "add_1_day = datetime.timedelta(days=1)\n",
    "add_20_min = datetime.timedelta(minutes=20)\n",
    "\n",
    "'''\n",
    "allAns[x,y,z]\n",
    "[x]: Segment (AM & PM, total 14)\n",
    "[y]: timestamp (6 [20mins])\n",
    "[z]: 3 features\n",
    "'''\n",
    "# allAns[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 62.63164379]\n",
      " [ 64.77199771]\n",
      " [ 66.40121635]\n",
      " [ 68.119003  ]\n",
      " [ 69.56480124]\n",
      " [ 71.09080095]]\n",
      "[[ 83.3359063 ]\n",
      " [ 84.58145641]\n",
      " [ 85.07630792]\n",
      " [ 86.32670382]\n",
      " [ 87.42048255]\n",
      " [ 88.03513465]]\n",
      "[[ 67.80033693]\n",
      " [ 69.16640692]\n",
      " [ 71.11126385]\n",
      " [ 74.38829422]\n",
      " [ 75.12304976]\n",
      " [ 76.29634852]]\n",
      "[[ 66.84773758]\n",
      " [ 68.06991058]\n",
      " [ 69.22613553]\n",
      " [ 70.56390692]\n",
      " [ 71.59124544]\n",
      " [ 72.72844692]]\n",
      "[[ 72.73233874]\n",
      " [ 74.37847976]\n",
      " [ 76.40692642]\n",
      " [ 78.29104072]\n",
      " [ 79.67511204]\n",
      " [ 81.34237763]]\n",
      "[[ 67.2253324 ]\n",
      " [ 67.96245405]\n",
      " [ 69.2956636 ]\n",
      " [ 71.09340563]\n",
      " [ 72.2806928 ]\n",
      " [ 73.77496778]]\n",
      "[[ 65.33603942]\n",
      " [ 66.81388417]\n",
      " [ 68.41838175]\n",
      " [ 70.57509019]\n",
      " [ 72.52091249]\n",
      " [ 74.22096282]]\n",
      "[[ 88.28200431]\n",
      " [ 90.2611035 ]\n",
      " [ 91.86463571]\n",
      " [ 94.5633145 ]\n",
      " [ 95.86057819]\n",
      " [ 97.55421165]]\n",
      "[[ 72.67613809]\n",
      " [ 75.13655965]\n",
      " [ 75.85363567]\n",
      " [ 78.17195848]\n",
      " [ 80.60976009]\n",
      " [ 82.48732502]]\n",
      "[[ 74.50351128]\n",
      " [ 76.01896763]\n",
      " [ 78.28252933]\n",
      " [ 81.13381237]\n",
      " [ 82.83660738]\n",
      " [ 85.02358302]]\n",
      "[[ 63.51245917]\n",
      " [ 66.5528034 ]\n",
      " [ 67.77785359]\n",
      " [ 69.12359401]\n",
      " [ 71.15344141]\n",
      " [ 73.33233443]]\n",
      "[[ 79.309225  ]\n",
      " [ 79.8268479 ]\n",
      " [ 81.41073198]\n",
      " [ 82.91174563]\n",
      " [ 84.28822648]\n",
      " [ 85.86896838]]\n",
      "[[ 74.10623159]\n",
      " [ 77.45888265]\n",
      " [ 80.58533414]\n",
      " [ 84.7649102 ]\n",
      " [ 87.438319  ]\n",
      " [ 91.15930622]]\n",
      "[[ 67.12541837]\n",
      " [ 66.92968926]\n",
      " [ 68.55775324]\n",
      " [ 70.73136512]\n",
      " [ 71.35904026]\n",
      " [ 72.43693769]]\n"
     ]
    }
   ],
   "source": [
    "for i in allAns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11.a [FOR TRAFFIC TIME] Output the CSV file\n",
    "\n",
    "route = 'B'\n",
    "checkpoint = '1'\n",
    "vol_or_traj = 0  # select the output cell\n",
    "\n",
    "with open('{}-{}_checking_B-1_phase1.csv'.format(route, checkpoint), 'w') as f:\n",
    "    for day in range(7):\n",
    "        for am_pm in range(2):\n",
    "            if am_pm == 0:\n",
    "                ref_time = start_8am\n",
    "            else:\n",
    "                ref_time = start_5pm\n",
    "            for timestep in range(6):\n",
    "                start_timestamp = ref_time + day*add_1_day + timestep*add_20_min\n",
    "                end_timestamp = start_timestamp + add_20_min\n",
    "                start_timestr = start_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                end_timestr = end_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                f.write('{},{},\"[{},{})\",{},{}\\n'.format(route,\n",
    "                                                      checkpoint,\n",
    "                                                      start_timestr,\n",
    "                                                      end_timestr,\n",
    "                                                      allAns[day*2+am_pm, timestep, vol_or_traj ],\n",
    "                                                      check_ans_arr[day*2*6 + am_pm*6 + timestep]))  # This last value is for checking answer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11.b [FOR VOLUME] Output the CSV file\n",
    "\n",
    "# checkpoint = '2'\n",
    "# direction = '0'\n",
    "# vol_or_traj = 1  # select the output cell\n",
    "\n",
    "# with open('{}-{}.csv'.format(checkpoint, direction), 'w') as f:\n",
    "#     for day in range(7):\n",
    "#         for am_pm in range(2):\n",
    "#             if am_pm == 0:\n",
    "#                 ref_time = start_8am\n",
    "#             else:\n",
    "#                 ref_time = start_5pm\n",
    "#             for timestep in range(6):\n",
    "#                 start_timestamp = ref_time + day*add_1_day + timestep*add_20_min\n",
    "#                 end_timestamp = start_timestamp + add_20_min\n",
    "#                 start_timestr = start_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "#                 end_timestr = end_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "#                 f.write('{},\"[{},{})\",{},{}\\n'.format(checkpoint,\n",
    "#                                                   start_timestr,\n",
    "#                                                   end_timestr,\n",
    "#                                                   direction,\n",
    "#                                                   allAns[day*2+am_pm, timestep, vol_or_traj ]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
