{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Date: 22-5-2017\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: caution: 十一黃金周\n",
    "\n",
    "df_merged_volume = pd.read_csv(\"../data/preprocessed_input_traffic_time_and_weather_interpolate_20min_phase1and2_train.csv\")\n",
    "\n",
    "# change \"Date\" to datetime object\n",
    "df_merged_volume['date'] = pd.to_datetime(df_merged_volume['date'])\n",
    "\n",
    "# construct \"time of day\"\n",
    "df_merged_volume['timeofday'] = df_merged_volume.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# Select the phase 1 day\n",
    "\n",
    "end_day = datetime.datetime(year=2016, month=10, day=18, hour=0, minute=0, second=0)\n",
    "\n",
    "df_merged_volume = df_merged_volume[(df_merged_volume['date'] < end_day)]\n",
    "\n",
    "# check any unreasonable rows\n",
    "df_merged_volume.tail(30)\n",
    "\n",
    "''' Cut some rows (proprecessing)'''\n",
    "df_merged_volume = df_merged_volume[4:]  # Cut of NaN rows at the beginning\n",
    "df_merged_volume = df_merged_volume.reset_index(drop=True)  # reindexing\n",
    "df_merged_volume\n",
    "\n",
    "''' Make the dataset stationary '''\n",
    "\n",
    "station_cols = 6  # select the first 6 columns for stationary\n",
    "\n",
    "df_merged_volume_copy = df_merged_volume.copy()\n",
    "\n",
    "for i in range(1, len(df_merged_volume_copy)):\n",
    "    df_merged_volume_copy.loc[i, df_merged_volume_copy.columns[0:station_cols]] = df_merged_volume.loc[i, df_merged_volume.columns[0:station_cols]] - df_merged_volume.loc[i-1, df_merged_volume.columns[0:station_cols]]\n",
    "\n",
    "# Check Stationary dataframe\n",
    "\n",
    "df_merged_volume_copy.tail()\n",
    "\n",
    "## Hidden the selecting time\n",
    "# select the time for training: 6:20-10:00 (5 + 6 timestamp) and 15:20-19:00 (5 + 6 timestamp)\n",
    "# sel_rows = df_merged_volume_copy[ ((df_merged_volume_copy.timeofday>= 6.3) & (df_merged_volume_copy.timeofday<10)) |\n",
    "#                             ((df_merged_volume_copy.timeofday>=15.3) & (df_merged_volume_copy.timeofday<19))]\n",
    "\n",
    "## This time, training all time (24hrs) except the first non-stationary row\n",
    "sel_rows = df_merged_volume_copy[1:]\n",
    "\n",
    "# Create one-hot for hour\n",
    "\n",
    "# for i in range(24):\n",
    "#     sel_rows['{}:00'.format(i)] = np.where(sel_rows.hour == i, 1, 0)\n",
    "\n",
    "# Check all the columns\n",
    "for idx, i in enumerate(sel_rows.columns):\n",
    "    print(idx, i)\n",
    "\n",
    "# select using columns\n",
    "\n",
    "using_cols = [\n",
    "#                 \"(1, 0, 'cargocar')\",\n",
    "#                 \"(1, 0, 'etc')\",\n",
    "#                 \"(1, 0, 'motorcycle')\",\n",
    "#                 \"(1, 0, 'privatecar')\",\n",
    "#                 \"(1, 0, 'tot')\",\n",
    "#                 \"(1, 0, 'unknowncar')\",\n",
    "#                 \"(1, 1, 'cargocar')\",\n",
    "#                 \"(1, 1, 'etc')\",\n",
    "#                 \"(1, 1, 'motorcycle')\",\n",
    "#                 \"(1, 1, 'privatecar')\",\n",
    "#                 \"(1, 1, 'tot')\",\n",
    "#                 \"(1, 1, 'unknowncar')\",\n",
    "#                 \"(2, 0, 'cargocar')\",\n",
    "#                 \"(2, 0, 'etc')\",\n",
    "#                 \"(2, 0, 'motorcycle')\",\n",
    "#                 \"(2, 0, 'privatecar')\",\n",
    "#                 \"(2, 0, 'tot')\",\n",
    "#                 \"(2, 0, 'unknowncar')\",\n",
    "#                 \"(3, 0, 'cargocar')\",\n",
    "#                 \"(3, 0, 'etc')\",\n",
    "#                 \"(3, 0, 'motorcycle')\",\n",
    "#                 \"(3, 0, 'privatecar')\",\n",
    "#                 \"(3, 0, 'tot')\",\n",
    "#                 \"(3, 0, 'unknowncar')\",\n",
    "#                 \"(3, 1, 'cargocar')\",\n",
    "#                 \"(3, 1, 'etc')\",\n",
    "#                 \"(3, 1, 'motorcycle')\",\n",
    "#                 \"(3, 1, 'privatecar')\",\n",
    "#                 \"(3, 1, 'tot')\",\n",
    "#                 \"(3, 1, 'unknowncar')\",\n",
    "                \"('A', 2)\",\n",
    "#                 \"('A', 3)\",\n",
    "#                 \"('B', 1)\",\n",
    "#                 \"('B', 3)\",\n",
    "#                 \"('C', 1)\",\n",
    "#                 \"('C', 3)\",\n",
    "#                 'date',  # <== Notice this\n",
    "                'hour',\n",
    "                'pressure',\n",
    "#                 'sea_pressure',\n",
    "#                 'wind_direction',\n",
    "#                 'wind_speed',\n",
    "                'temperature',\n",
    "#                 'rel_humidity',\n",
    "                'precipitation',\n",
    "                'dayofweek',\n",
    "                'is_holiday',\n",
    "                'timeofday',\n",
    "#                 '0:00',\n",
    "#                 '1:00',\n",
    "#                 '2:00',\n",
    "#                 '3:00',\n",
    "#                 '4:00',\n",
    "#                 '5:00',\n",
    "#                 '6:00',\n",
    "#                 '7:00',\n",
    "#                 '8:00',\n",
    "#                 '9:00',\n",
    "#                 '10:00',\n",
    "#                 '11:00',\n",
    "#                 '12:00',\n",
    "#                 '13:00',\n",
    "#                 '14:00',\n",
    "#                 '15:00',\n",
    "#                 '16:00',\n",
    "#                 '17:00',\n",
    "#                 '18:00',\n",
    "#                 '19:00',\n",
    "#                 '20:00',\n",
    "#                 '21:00',\n",
    "#                 '22:00',\n",
    "#                 '23:00',\n",
    "              ]\n",
    "\n",
    "sel_rows = sel_rows[using_cols]\n",
    "\n",
    "# split to train and valid set\n",
    "train_rows = sel_rows[: -24*3*7]\n",
    "valid_rows = sel_rows[-24*3*7:] # reserve 7 days for validation\n",
    "\n",
    "# get numpy array from panda dataframe\n",
    "train_arr = train_rows.values\n",
    "valid_arr = valid_rows.values\n",
    "\n",
    "# np.shape(train_arr)\n",
    "# Out:\n",
    "# (726, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale feature array to range -1 to 1\n",
    "\n",
    "\n",
    "scaler = scaler.fit(train_arr)\n",
    "train_scaled_arr = scaler.transform(train_arr)\n",
    "\n",
    "valid_scaled_arr = scaler.transform(valid_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' save the scaler to another file use '''\n",
    "import pickle\n",
    "scalerfile = 'scaler-A-2_phase1.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample subsequence from the time series\n",
    "train_seqs = []\n",
    "len_seqs = len(train_scaled_arr) - 6 + 1  # 6 is window size\n",
    "for i in range(len_seqs):\n",
    "    train_seqs.append(train_scaled_arr[i: i+6])  # append 6 timestamps each time (5 timestamps for x, 1 timestamp for y)\n",
    "train_seqs = np.stack(train_seqs)\n",
    "\n",
    "valid_seqs = []\n",
    "len_v_seqs = len(valid_scaled_arr) - 6 + 1  # 6 is window size\n",
    "for i in range(len_v_seqs):\n",
    "    valid_seqs.append(valid_scaled_arr[i: i+6])\n",
    "valid_seqs = np.stack(valid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25431741, -0.82608696, -0.48043818,  0.28063241, -1.        ,\n",
       "        -0.66666667, -1.        , -0.83098592],\n",
       "       [ 0.25431741, -0.82608696, -0.48461137,  0.31752306, -1.        ,\n",
       "        -0.66666667, -1.        , -0.8028169 ],\n",
       "       [ 0.27806522, -0.82608696, -0.48878456,  0.3544137 , -1.        ,\n",
       "        -0.66666667, -1.        , -0.77464789],\n",
       "       [ 0.28332385, -0.73913043, -0.49295775,  0.39130435, -1.        ,\n",
       "        -0.66666667, -1.        , -0.74647887],\n",
       "       [ 0.28332385, -0.73913043, -0.50130412,  0.3921827 , -1.        ,\n",
       "        -0.66666667, -1.        , -0.71830986],\n",
       "       [ 0.27802849, -0.73913043, -0.5096505 ,  0.39306105, -1.        ,\n",
       "        -0.66666667, -1.        , -0.69014085]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "train_seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras\n",
    "#https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "input_dim = len(using_cols)  # The features\n",
    "output_dim = 1  # \n",
    "timesteps = 5 # use 5 timesteps to predict the 6th\n",
    "\n",
    "x_train, y_train = train_seqs[:, 0:-1], train_seqs[:, -1, 0:output_dim]  # 0:output_dim is for deciding the output features\n",
    "x_valid , y_valid  =  valid_seqs[:, 0:-1],  valid_seqs[:, -1, 0:output_dim]  # 0:output_dim is for deciding the output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6038, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 120\n",
    "loss_fuc = 'mean_squared_error'\n",
    "\n",
    "# construct the callback\n",
    "filepath=\"best_epoch_T.M._A-2_phase1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(output_dim))\n",
    "model.compile(loss=loss_fuc, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 128)            70144     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 201,857\n",
      "Trainable params: 201,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6038 samples, validate on 499 samples\n",
      "Epoch 1/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0056Epoch 00000: val_loss improved from inf to 0.00196, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0057 - val_loss: 0.0020\n",
      "Epoch 2/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00001: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 3/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0033Epoch 00002: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0033 - val_loss: 0.0020\n",
      "Epoch 4/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00003: val_loss improved from 0.00196 to 0.00194, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 5/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00004: val_loss improved from 0.00194 to 0.00188, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 6/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0031Epoch 00005: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 7/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0032Epoch 00006: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 8/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0031Epoch 00007: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 9/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0030Epoch 00008: val_loss improved from 0.00188 to 0.00185, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 10/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0030Epoch 00009: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 11/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0030Epoch 00010: val_loss improved from 0.00185 to 0.00175, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0030 - val_loss: 0.0018\n",
      "Epoch 12/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0029Epoch 00011: val_loss improved from 0.00175 to 0.00169, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 13/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0028Epoch 00012: val_loss improved from 0.00169 to 0.00160, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 14/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0027Epoch 00013: val_loss improved from 0.00160 to 0.00158, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 15/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0027Epoch 00014: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 16/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0026Epoch 00015: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 17/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0026Epoch 00016: val_loss improved from 0.00158 to 0.00136, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 18/120\n",
      "5952/6038 [============================>.] - ETA: 0s - loss: 0.0025Epoch 00017: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 19/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0024Epoch 00018: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 20/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0024Epoch 00019: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 21/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0024Epoch 00020: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 22/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0024Epoch 00021: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 23/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00022: val_loss improved from 0.00136 to 0.00124, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 24/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00023: val_loss improved from 0.00124 to 0.00122, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 25/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00024: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 26/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0024Epoch 00025: val_loss improved from 0.00122 to 0.00122, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 27/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00026: val_loss improved from 0.00122 to 0.00120, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 28/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00027: val_loss improved from 0.00120 to 0.00120, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 29/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00028: val_loss improved from 0.00120 to 0.00119, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 30/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00029: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 31/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00030: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 32/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00031: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 33/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00032: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 34/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00033: val_loss improved from 0.00119 to 0.00118, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 35/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00034: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 36/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00035: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 37/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00036: val_loss improved from 0.00118 to 0.00118, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 38/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00037: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 39/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00038: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 40/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00039: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 41/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00040: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 42/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00041: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 43/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00042: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 44/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00043: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 45/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0023Epoch 00044: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 46/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00045: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 47/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00046: val_loss did not improve\n",
      "6038/6038 [==============================] - 7s - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 48/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00047: val_loss did not improve\n",
      "6038/6038 [==============================] - 7s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 49/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00048: val_loss did not improve\n",
      "6038/6038 [==============================] - 7s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 50/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00049: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 51/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00050: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 52/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00051: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 53/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00052: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 54/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00053: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 55/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00054: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 56/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00055: val_loss improved from 0.00118 to 0.00118, saving model to best_epoch_T.M._A-2_phase1.hdf5\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 57/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00056: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 58/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00057: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 59/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00058: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 60/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00059: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 61/120\n",
      "5952/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00060: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 62/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00061: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 63/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00062: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 64/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00063: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 65/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00064: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 66/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00065: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 67/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00066: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 68/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00067: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 69/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00068: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 70/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00069: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 71/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00070: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 72/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00071: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 73/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00072: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 74/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00073: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 75/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00074: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 76/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00075: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 77/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00076: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 78/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00077: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 79/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00078: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 80/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00079: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 81/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00080: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 82/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00081: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 83/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00082: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 84/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00083: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 85/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00084: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 86/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00085: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 87/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0022Epoch 00086: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 88/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00087: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 89/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0021Epoch 00088: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 90/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00089: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 91/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00090: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 92/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00091: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 93/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0020Epoch 00092: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 94/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00093: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 95/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00094: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 96/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00095: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 97/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00096: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 98/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00097: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 99/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00098: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 100/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00099: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 101/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00100: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 102/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00101: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 103/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00102: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 104/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00103: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 105/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00104: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 106/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00105: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 107/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00106: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 108/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00107: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 109/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00108: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 110/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00109: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 111/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00110: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 112/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00111: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 113/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00112: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 114/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00113: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 115/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00114: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 116/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00115: val_loss did not improve\n",
      "6038/6038 [==============================] - 6s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 117/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00116: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 118/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00117: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 119/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0019Epoch 00118: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 120/120\n",
      "6016/6038 [============================>.] - ETA: 0s - loss: 0.0018Epoch 00119: val_loss did not improve\n",
      "6038/6038 [==============================] - 5s - loss: 0.0018 - val_loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGHCAYAAACnPchFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npmlanff/AS9UBe/DOs2WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1216969e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Start Training\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=64, validation_data=(x_valid, y_valid))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Save Model '''\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_T.M._A-2-p1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Load the saved model '''\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model_500x5_300e.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Notes: last best model: 0.00175\n",
    "# load weights into the model\n",
    "model.load_weights(\"best_epoch_T.M._A-2_phase1.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' === Prediction ===\n",
    "Procedure:\n",
    "1. Load CSV\n",
    "2. to_datetime\n",
    "3. create timeofday column\n",
    "4. select the time for training: 6:00-8:00 (6 timestamps) and 15:00-17:00 (6 timestamps)\n",
    "5. change it to stationary\n",
    "6. Use using_cols to select the features\n",
    "7. change to np array\n",
    "8. MinMaxScaler\n",
    "9. make the sequences tensor as input\n",
    "10. make a forloop for prediction\n",
    "\n",
    "'''\n",
    "# 1. Load CSV - Vol + Route + Weather (Only Weather is 24-hour data)\n",
    "df_pred = pd.read_csv('../data/preprocessed_input_interpolate_20min_phase1and2_train.csv')\n",
    "\n",
    "# 2. to_datetime\n",
    "df_pred['date'] = pd.to_datetime(df_pred['date'])\n",
    "\n",
    "# 3. create timeofday column\n",
    "df_pred['timeofday'] = df_pred.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# Select the checking days (No need to real final test)\n",
    "\n",
    "start_day = datetime.datetime(year=2016, month=10, day=18, hour=1, minute=0, second=0)\n",
    "end_day = datetime.datetime(year=2016, month=10, day=24, hour=23, minute=0, second=0)\n",
    "\n",
    "df_pred_sel = df_pred[(df_pred['date'] > start_day) & (df_pred['date'] < end_day)]\n",
    "\n",
    "# 4. select the time for training\n",
    "\n",
    "df_pred_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 6) & (df_pred_sel.timeofday<8)) |\n",
    "                            ((df_pred_sel.timeofday>=15) & (df_pred_sel.timeofday<17))]\n",
    "\n",
    "df_feedin_weather_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 8) & (df_pred_sel.timeofday<10)) |\n",
    "                            ((df_pred_sel.timeofday>=17) & (df_pred_sel.timeofday<19))]\n",
    "\n",
    "# Checking\n",
    "df_pred_sel_time.iloc[12]\n",
    "\n",
    "# 5. change it to stationary\n",
    "df_pred_sel_time = df_pred_sel_time.reset_index(drop=True)\n",
    "\n",
    "df_pred_sel_time_copy = df_pred_sel_time.copy()\n",
    "\n",
    "for i in range(len(df_pred_sel_time_copy)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        start_idx = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot\n",
    "        df_pred_sel_time_copy.loc[start_idx, df_pred_sel_time_copy.columns[0:36]] = df_pred_sel_time.loc[start_idx, df_pred_sel_time.columns[0:36]] - df_pred_sel_time.loc[start_idx-1, df_pred_sel_time.columns[0:36]]\n",
    "\n",
    "# Create one-hot for it\n",
    "# for i in range(24):\n",
    "#     df_pred_sel_time_copy['{}:00'.format(i)] = np.where(df_pred_sel_time_copy.hour == i, 1, 0)\n",
    "#     df_feedin_weather_sel_time['{}:00'.format(i)] = np.where(df_feedin_weather_sel_time.hour == i, 1, 0)\n",
    "\n",
    "# 6. Use using_cols to select the features\n",
    "\n",
    "sel_rows_pred = df_pred_sel_time_copy[ using_cols ]\n",
    "\n",
    "sel_rows_feedin_weather = df_feedin_weather_sel_time[using_cols[output_dim:]]\n",
    "\n",
    "sel_rows_pred\n",
    "\n",
    "# 7. change to np array\n",
    "pred_arr = sel_rows_pred.values\n",
    "\n",
    "feedin_weather_arr = sel_rows_feedin_weather.values\n",
    "\n",
    "# 8. MinMaxScaler\n",
    "pred_arr_scaled = scaler.transform(pred_arr)\n",
    "\n",
    "# add some dummy cells in front of the weather_array for transform\n",
    "temp_arr = np.zeros((84,output_dim))\n",
    "feedin_weather_arr = np.concatenate([temp_arr, feedin_weather_arr], axis=1)\n",
    "\n",
    "feedin_weather_arr_scaled = scaler.transform(feedin_weather_arr)\n",
    "\n",
    "# Now pred_arr_scaled is (84 x features)\n",
    "\n",
    "# 9. make the sequences tensor as input\n",
    "# Put into the model to get the prediction\n",
    "\n",
    "ans_arr = []  # For holding the output answer\n",
    "    \n",
    "for i in range(len(pred_arr_scaled)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    # creating pre_seq\n",
    "    pred_seq = []\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        k = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot, to ignore the first index which is non-stationary\n",
    "        pred_seq.append(pred_arr_scaled[k])  # creating a sequence for a time slot\n",
    "    \n",
    "    # creating feedin_weather_seq\n",
    "    feedin_weather_seq = []\n",
    "    for t in range(6):  #  Do 6 times every loop\n",
    "        k = i*6 + t  #\n",
    "        feedin_weather_seq.append(feedin_weather_arr_scaled[k])\n",
    "\n",
    "\n",
    "    pred_seq = np.stack(pred_seq)  # change back to the numpy array (2D)\n",
    "    pred_seq = pred_seq.reshape(1, pred_seq.shape[0], pred_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    feedin_weather_seq = np.stack(feedin_weather_seq)  # change back to the numpy array (2D)\n",
    "    feedin_weather_seq = feedin_weather_seq.reshape(1, feedin_weather_seq.shape[0], feedin_weather_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    for q in range(6):\n",
    "        # predict next timestamp\n",
    "        output_pred = model.predict(pred_seq)  # get one prediction output (size (1 x output feature(s)))\n",
    "        ans_arr.append(output_pred)\n",
    "\n",
    "        # update the input seq\n",
    "        for j in range(1,5):\n",
    "            pred_seq[0][j-1] = pred_seq[0][j]\n",
    "        pred_seq[0][4] = feedin_weather_seq[0][q]\n",
    "        pred_seq[0][4][0:output_dim] = output_pred[0]\n",
    "\n",
    "# 10. Backward to the non-stationary, correct scale output\n",
    "\n",
    "#  Helper functions\n",
    "\n",
    "def backward_scaler(nn_output):\n",
    "    tmp = np.zeros(14)\n",
    "    tmp[0:output_dim] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return tmp[0:output_dim]\n",
    "\n",
    "def decode(last_timestamp_values, nn_output):\n",
    "    tmp = np.zeros(input_dim)\n",
    "    tmp[0:output_dim] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return last_timestamp_values + tmp[0:output_dim]\n",
    "\n",
    "# create the non-stationary 6:40 and 16:40 for decoding\n",
    "df_non_station_sel_time = df_pred_sel[ ((df_pred_sel.timeofday>= 7.5) & (df_pred_sel.timeofday<8)) |\n",
    "                            ((df_pred_sel.timeofday>=16.5) & (df_pred_sel.timeofday<17))]\n",
    "\n",
    "''' Output the non-stationary Answers (allAns)'''\n",
    "\n",
    "tmp = df_non_station_sel_time[using_cols[0:output_dim]].values\n",
    "allAns = []\n",
    "for i in range(len(tmp)):\n",
    "    seed = tmp[i]  # non-stationary for reconstructing a sequence\n",
    "    segmentAns = []\n",
    "    for timestep in range(6):\n",
    "        seed = decode(seed, ans_arr[i*6+timestep])\n",
    "        segmentAns.append(seed)\n",
    "    allAns.append(segmentAns)\n",
    "\n",
    "# Change back to np array for easy visualize\n",
    "allAns = np.array(allAns)\n",
    "\n",
    "# Checking\n",
    "for i in allAns:\n",
    "    print(i)\n",
    "\n",
    "# 11. Output the CSV file\n",
    "\n",
    "# create the datetime objects\n",
    "import datetime\n",
    "\n",
    "pred_start_date = 18\n",
    "\n",
    "\n",
    "start_8am = datetime.datetime(year=2016, month=10, day=pred_start_date, hour=8, minute=0, second=0)\n",
    "start_5pm = datetime.datetime(year=2016, month=10, day=pred_start_date, hour=17, minute=0, second=0)\n",
    "add_1_day = datetime.timedelta(days=1)\n",
    "add_20_min = datetime.timedelta(minutes=20)\n",
    "\n",
    "'''\n",
    "allAns[x,y,z]\n",
    "[x]: Segment (AM & PM, total 14)\n",
    "[y]: timestamp (6 [20mins])\n",
    "[z]: 3 features\n",
    "'''\n",
    "# allAns[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11.a [FOR TRAFFIC TIME] Output the CSV file\n",
    "\n",
    "route = 'A'\n",
    "checkpoint = '2'\n",
    "vol_or_traj = 0  # select the output cell\n",
    "\n",
    "with open('{}-{}_checking_A-2_phase1.csv'.format(route, checkpoint), 'w') as f:\n",
    "    for day in range(7):\n",
    "        for am_pm in range(2):\n",
    "            if am_pm == 0:\n",
    "                ref_time = start_8am\n",
    "            else:\n",
    "                ref_time = start_5pm\n",
    "            for timestep in range(6):\n",
    "                start_timestamp = ref_time + day*add_1_day + timestep*add_20_min\n",
    "                end_timestamp = start_timestamp + add_20_min\n",
    "                start_timestr = start_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                end_timestr = end_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                f.write('{},{},\"[{},{})\",{}\\n'.format(route,\n",
    "                                                      checkpoint,\n",
    "                                                      start_timestr,\n",
    "                                                      end_timestr,\n",
    "                                                      allAns[day*2+am_pm, timestep, vol_or_traj ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11.b [FOR VOLUME] Output the CSV file\n",
    "\n",
    "checkpoint = '2'\n",
    "direction = '0'\n",
    "vol_or_traj = 1  # select the output cell\n",
    "\n",
    "with open('{}-{}.csv'.format(checkpoint, direction), 'w') as f:\n",
    "    for day in range(7):\n",
    "        for am_pm in range(2):\n",
    "            if am_pm == 0:\n",
    "                ref_time = start_8am\n",
    "            else:\n",
    "                ref_time = start_5pm\n",
    "            for timestep in range(6):\n",
    "                start_timestamp = ref_time + day*add_1_day + timestep*add_20_min\n",
    "                end_timestamp = start_timestamp + add_20_min\n",
    "                start_timestr = start_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                end_timestr = end_timestamp.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "                f.write('{},\"[{},{})\",{},{}\\n'.format(checkpoint,\n",
    "                                                  start_timestr,\n",
    "                                                  end_timestr,\n",
    "                                                  direction,\n",
    "                                                  allAns[day*2+am_pm, timestep, vol_or_traj ]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
