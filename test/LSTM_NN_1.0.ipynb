{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Date: 22-5-2017\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note:\n",
    "# Because of the 十一黃金周. sth is strange with (2,0,'tot') and others vol from 1 Oct 00:00 to 7 Oct 23:59\n",
    "\n",
    "df_merged_volume = pd.read_csv(\"../data/preprocessed_input_interpolate_20min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change \"Date\" to datetime object\n",
    "df_merged_volume['date'] = pd.to_datetime(df_merged_volume['date'])\n",
    "\n",
    "# construct \"time of day\"\n",
    "df_merged_volume['timeofday'] = df_merged_volume.date.apply( lambda d : d.hour+d.minute/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 0, 'cargocar')</th>\n",
       "      <th>(1, 0, 'etc')</th>\n",
       "      <th>(1, 0, 'motorcycle')</th>\n",
       "      <th>(1, 0, 'privatecar')</th>\n",
       "      <th>(1, 0, 'tot')</th>\n",
       "      <th>(1, 0, 'unknowncar')</th>\n",
       "      <th>(1, 1, 'cargocar')</th>\n",
       "      <th>(1, 1, 'etc')</th>\n",
       "      <th>(1, 1, 'motorcycle')</th>\n",
       "      <th>(1, 1, 'privatecar')</th>\n",
       "      <th>...</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.200000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.233333</td>\n",
       "      <td>1018.233333</td>\n",
       "      <td>342.444444</td>\n",
       "      <td>3.4</td>\n",
       "      <td>21.411111</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.266667</td>\n",
       "      <td>1018.266667</td>\n",
       "      <td>343.888889</td>\n",
       "      <td>3.3</td>\n",
       "      <td>21.722222</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.300000</td>\n",
       "      <td>1018.300000</td>\n",
       "      <td>345.333333</td>\n",
       "      <td>3.2</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>1018.333333</td>\n",
       "      <td>346.777778</td>\n",
       "      <td>3.1</td>\n",
       "      <td>22.344444</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.366667</td>\n",
       "      <td>1018.366667</td>\n",
       "      <td>348.222222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.655556</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.400000</td>\n",
       "      <td>1018.400000</td>\n",
       "      <td>349.666667</td>\n",
       "      <td>2.9</td>\n",
       "      <td>22.966667</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.433333</td>\n",
       "      <td>1018.433333</td>\n",
       "      <td>351.111111</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.277778</td>\n",
       "      <td>60.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.466667</td>\n",
       "      <td>1018.466667</td>\n",
       "      <td>352.555556</td>\n",
       "      <td>2.7</td>\n",
       "      <td>23.588889</td>\n",
       "      <td>59.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.500000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (1, 0, 'cargocar')  (1, 0, 'etc')  (1, 0, 'motorcycle')  \\\n",
       "0                   0            1.0                     0   \n",
       "1                   0            0.0                     0   \n",
       "2                   0            1.0                     0   \n",
       "3                   0            2.0                     0   \n",
       "4                   0            1.0                     0   \n",
       "5                   0            1.0                     0   \n",
       "6                   0            0.0                     0   \n",
       "7                   0            2.0                     0   \n",
       "8                   0            0.0                     0   \n",
       "9                   0            0.0                     0   \n",
       "\n",
       "   (1, 0, 'privatecar')  (1, 0, 'tot')  (1, 0, 'unknowncar')  \\\n",
       "0                     0           14.0                  14.0   \n",
       "1                     0           13.0                  13.0   \n",
       "2                     0            7.0                   7.0   \n",
       "3                     0            6.0                   6.0   \n",
       "4                     0            5.0                   5.0   \n",
       "5                     0            5.0                   5.0   \n",
       "6                     0            6.0                   6.0   \n",
       "7                     0            9.0                   9.0   \n",
       "8                     0            7.0                   7.0   \n",
       "9                     0           10.0                  10.0   \n",
       "\n",
       "   (1, 1, 'cargocar')  (1, 1, 'etc')  (1, 1, 'motorcycle')  \\\n",
       "0                38.0           25.0                  89.0   \n",
       "1                24.0           11.0                  41.0   \n",
       "2                10.0            7.0                  22.0   \n",
       "3                 3.0            0.0                   3.0   \n",
       "4                 5.0            0.0                   3.0   \n",
       "5                 3.0            1.0                   8.0   \n",
       "6                 6.0            1.0                   2.0   \n",
       "7                 8.0            1.0                   4.0   \n",
       "8                 4.0            1.0                   4.0   \n",
       "9                 2.0            0.0                   0.0   \n",
       "\n",
       "   (1, 1, 'privatecar')    ...         pressure  sea_pressure  wind_direction  \\\n",
       "0                  12.0    ...      1013.200000   1018.200000      341.000000   \n",
       "1                  15.0    ...      1013.233333   1018.233333      342.444444   \n",
       "2                   5.0    ...      1013.266667   1018.266667      343.888889   \n",
       "3                   0.0    ...      1013.300000   1018.300000      345.333333   \n",
       "4                   0.0    ...      1013.333333   1018.333333      346.777778   \n",
       "5                   0.0    ...      1013.366667   1018.366667      348.222222   \n",
       "6                   0.0    ...      1013.400000   1018.400000      349.666667   \n",
       "7                   0.0    ...      1013.433333   1018.433333      351.111111   \n",
       "8                   0.0    ...      1013.466667   1018.466667      352.555556   \n",
       "9                   0.0    ...      1013.500000   1018.500000      354.000000   \n",
       "\n",
       "   wind_speed  temperature  rel_humidity  precipitation  dayofweek  \\\n",
       "0         3.5    21.100000     68.000000            0.0          1   \n",
       "1         3.4    21.411111     66.888889            0.0          1   \n",
       "2         3.3    21.722222     65.777778            0.0          1   \n",
       "3         3.2    22.033333     64.666667            0.0          1   \n",
       "4         3.1    22.344444     63.555556            0.0          1   \n",
       "5         3.0    22.655556     62.444444            0.0          1   \n",
       "6         2.9    22.966667     61.333333            0.0          1   \n",
       "7         2.8    23.277778     60.222222            0.0          1   \n",
       "8         2.7    23.588889     59.111111            0.0          1   \n",
       "9         2.6    23.900000     58.000000            0.0          1   \n",
       "\n",
       "   is_holiday  timeofday  \n",
       "0           0   0.000000  \n",
       "1           0   0.333333  \n",
       "2           0   0.666667  \n",
       "3           0   1.000000  \n",
       "4           0   1.333333  \n",
       "5           0   1.666667  \n",
       "6           0   2.000000  \n",
       "7           0   2.333333  \n",
       "8           0   2.666667  \n",
       "9           0   3.000000  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_volume.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged_volume_copy = df_merged_volume.copy()\n",
    "\n",
    "# df_merged_volume_copy = df_merged_volume_copy.diff()\n",
    "for i in range(1, len(df_merged_volume_copy)):\n",
    "    df_merged_volume_copy.loc[i, df_merged_volume_copy.columns[0:36]] = df_merged_volume.loc[i, df_merged_volume.columns[0:36]] - df_merged_volume.loc[i-1, df_merged_volume.columns[0:36]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 0, 'cargocar')</th>\n",
       "      <th>(1, 0, 'etc')</th>\n",
       "      <th>(1, 0, 'motorcycle')</th>\n",
       "      <th>(1, 0, 'privatecar')</th>\n",
       "      <th>(1, 0, 'tot')</th>\n",
       "      <th>(1, 0, 'unknowncar')</th>\n",
       "      <th>(1, 1, 'cargocar')</th>\n",
       "      <th>(1, 1, 'etc')</th>\n",
       "      <th>(1, 1, 'motorcycle')</th>\n",
       "      <th>(1, 1, 'privatecar')</th>\n",
       "      <th>...</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.200000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.233333</td>\n",
       "      <td>1018.233333</td>\n",
       "      <td>342.444444</td>\n",
       "      <td>3.4</td>\n",
       "      <td>21.411111</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.266667</td>\n",
       "      <td>1018.266667</td>\n",
       "      <td>343.888889</td>\n",
       "      <td>3.3</td>\n",
       "      <td>21.722222</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.300000</td>\n",
       "      <td>1018.300000</td>\n",
       "      <td>345.333333</td>\n",
       "      <td>3.2</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>1018.333333</td>\n",
       "      <td>346.777778</td>\n",
       "      <td>3.1</td>\n",
       "      <td>22.344444</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.366667</td>\n",
       "      <td>1018.366667</td>\n",
       "      <td>348.222222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.655556</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.400000</td>\n",
       "      <td>1018.400000</td>\n",
       "      <td>349.666667</td>\n",
       "      <td>2.9</td>\n",
       "      <td>22.966667</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.433333</td>\n",
       "      <td>1018.433333</td>\n",
       "      <td>351.111111</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.277778</td>\n",
       "      <td>60.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.466667</td>\n",
       "      <td>1018.466667</td>\n",
       "      <td>352.555556</td>\n",
       "      <td>2.7</td>\n",
       "      <td>23.588889</td>\n",
       "      <td>59.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.500000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (1, 0, 'cargocar')  (1, 0, 'etc')  (1, 0, 'motorcycle')  \\\n",
       "0                   0            1.0                     0   \n",
       "1                   0           -1.0                     0   \n",
       "2                   0            1.0                     0   \n",
       "3                   0            1.0                     0   \n",
       "4                   0           -1.0                     0   \n",
       "5                   0            0.0                     0   \n",
       "6                   0           -1.0                     0   \n",
       "7                   0            2.0                     0   \n",
       "8                   0           -2.0                     0   \n",
       "9                   0            0.0                     0   \n",
       "\n",
       "   (1, 0, 'privatecar')  (1, 0, 'tot')  (1, 0, 'unknowncar')  \\\n",
       "0                     0           14.0                  14.0   \n",
       "1                     0           -1.0                  -1.0   \n",
       "2                     0           -6.0                  -6.0   \n",
       "3                     0           -1.0                  -1.0   \n",
       "4                     0           -1.0                  -1.0   \n",
       "5                     0            0.0                   0.0   \n",
       "6                     0            1.0                   1.0   \n",
       "7                     0            3.0                   3.0   \n",
       "8                     0           -2.0                  -2.0   \n",
       "9                     0            3.0                   3.0   \n",
       "\n",
       "   (1, 1, 'cargocar')  (1, 1, 'etc')  (1, 1, 'motorcycle')  \\\n",
       "0                38.0           25.0                  89.0   \n",
       "1               -14.0          -14.0                 -48.0   \n",
       "2               -14.0           -4.0                 -19.0   \n",
       "3                -7.0           -7.0                 -19.0   \n",
       "4                 2.0            0.0                   0.0   \n",
       "5                -2.0            1.0                   5.0   \n",
       "6                 3.0            0.0                  -6.0   \n",
       "7                 2.0            0.0                   2.0   \n",
       "8                -4.0            0.0                   0.0   \n",
       "9                -2.0           -1.0                  -4.0   \n",
       "\n",
       "   (1, 1, 'privatecar')    ...         pressure  sea_pressure  wind_direction  \\\n",
       "0                  12.0    ...      1013.200000   1018.200000      341.000000   \n",
       "1                   3.0    ...      1013.233333   1018.233333      342.444444   \n",
       "2                 -10.0    ...      1013.266667   1018.266667      343.888889   \n",
       "3                  -5.0    ...      1013.300000   1018.300000      345.333333   \n",
       "4                   0.0    ...      1013.333333   1018.333333      346.777778   \n",
       "5                   0.0    ...      1013.366667   1018.366667      348.222222   \n",
       "6                   0.0    ...      1013.400000   1018.400000      349.666667   \n",
       "7                   0.0    ...      1013.433333   1018.433333      351.111111   \n",
       "8                   0.0    ...      1013.466667   1018.466667      352.555556   \n",
       "9                   0.0    ...      1013.500000   1018.500000      354.000000   \n",
       "\n",
       "   wind_speed  temperature  rel_humidity  precipitation  dayofweek  \\\n",
       "0         3.5    21.100000     68.000000            0.0          1   \n",
       "1         3.4    21.411111     66.888889            0.0          1   \n",
       "2         3.3    21.722222     65.777778            0.0          1   \n",
       "3         3.2    22.033333     64.666667            0.0          1   \n",
       "4         3.1    22.344444     63.555556            0.0          1   \n",
       "5         3.0    22.655556     62.444444            0.0          1   \n",
       "6         2.9    22.966667     61.333333            0.0          1   \n",
       "7         2.8    23.277778     60.222222            0.0          1   \n",
       "8         2.7    23.588889     59.111111            0.0          1   \n",
       "9         2.6    23.900000     58.000000            0.0          1   \n",
       "\n",
       "   is_holiday  timeofday  \n",
       "0           0   0.000000  \n",
       "1           0   0.333333  \n",
       "2           0   0.666667  \n",
       "3           0   1.000000  \n",
       "4           0   1.333333  \n",
       "5           0   1.666667  \n",
       "6           0   2.000000  \n",
       "7           0   2.333333  \n",
       "8           0   2.666667  \n",
       "9           0   3.000000  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_volume_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 0, 'cargocar')\n",
      "1 (1, 0, 'etc')\n",
      "2 (1, 0, 'motorcycle')\n",
      "3 (1, 0, 'privatecar')\n",
      "4 (1, 0, 'tot')\n",
      "5 (1, 0, 'unknowncar')\n",
      "6 (1, 1, 'cargocar')\n",
      "7 (1, 1, 'etc')\n",
      "8 (1, 1, 'motorcycle')\n",
      "9 (1, 1, 'privatecar')\n",
      "10 (1, 1, 'tot')\n",
      "11 (1, 1, 'unknowncar')\n",
      "12 (2, 0, 'cargocar')\n",
      "13 (2, 0, 'etc')\n",
      "14 (2, 0, 'motorcycle')\n",
      "15 (2, 0, 'privatecar')\n",
      "16 (2, 0, 'tot')\n",
      "17 (2, 0, 'unknowncar')\n",
      "18 (3, 0, 'cargocar')\n",
      "19 (3, 0, 'etc')\n",
      "20 (3, 0, 'motorcycle')\n",
      "21 (3, 0, 'privatecar')\n",
      "22 (3, 0, 'tot')\n",
      "23 (3, 0, 'unknowncar')\n",
      "24 (3, 1, 'cargocar')\n",
      "25 (3, 1, 'etc')\n",
      "26 (3, 1, 'motorcycle')\n",
      "27 (3, 1, 'privatecar')\n",
      "28 (3, 1, 'tot')\n",
      "29 (3, 1, 'unknowncar')\n",
      "30 ('A', 2)\n",
      "31 ('A', 3)\n",
      "32 ('B', 1)\n",
      "33 ('B', 3)\n",
      "34 ('C', 1)\n",
      "35 ('C', 3)\n",
      "36 date\n",
      "37 hour\n",
      "38 pressure\n",
      "39 sea_pressure\n",
      "40 wind_direction\n",
      "41 wind_speed\n",
      "42 temperature\n",
      "43 rel_humidity\n",
      "44 precipitation\n",
      "45 dayofweek\n",
      "46 is_holiday\n",
      "47 timeofday\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(df_merged_volume.columns):\n",
    "    print(idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function for converting it as stationary data'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a differenced series for Volumn and traffic time [modified from http://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/]\n",
    "def difference(dataset, interval=1):\n",
    "    diff_array = np.ndarray(np.shape(dataset))\n",
    "    for i in range(interval, len(dataset)):\n",
    "        diff_array[i][0:2] = dataset[i][0:2] - dataset[i - interval][0:2]  # only select index 0 & 1\n",
    "    return diff_array[1:]  # eliminate the first row (all zeros)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, y_hat, interval=1):\n",
    "    return y_hat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the time for training: 6-10 and 15-19\n",
    "sel_rows = df_merged_volume[ lambda r : ((r.timeofday>= 6) & (r.timeofday<10)) |\n",
    "                            ((r.timeofday>=15) & (r.timeofday<19))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select using columns\n",
    "\n",
    "using_cols = [\n",
    "#                 \"(1, 0, 'cargocar')\",\n",
    "#                 \"(1, 0, 'etc')\",\n",
    "#                 \"(1, 0, 'motorcycle')\",\n",
    "#                 \"(1, 0, 'privatecar')\",\n",
    "#                 \"(1, 0, 'tot')\",\n",
    "#                 \"(1, 0, 'unknowncar')\",\n",
    "#                 \"(1, 1, 'cargocar')\",\n",
    "#                 \"(1, 1, 'etc')\",\n",
    "#                 \"(1, 1, 'motorcycle')\",\n",
    "#                 \"(1, 1, 'privatecar')\",\n",
    "#                 \"(1, 1, 'tot')\",\n",
    "#                 \"(1, 1, 'unknowncar')\",\n",
    "#                 \"(2, 0, 'cargocar')\",\n",
    "#                 \"(2, 0, 'etc')\",\n",
    "#                 \"(2, 0, 'motorcycle')\",\n",
    "#                 \"(2, 0, 'privatecar')\",\n",
    "                \"(2, 0, 'tot')\",\n",
    "#                 \"(2, 0, 'unknowncar')\",\n",
    "#                 \"(3, 0, 'cargocar')\",\n",
    "#                 \"(3, 0, 'etc')\",\n",
    "#                 \"(3, 0, 'motorcycle')\",\n",
    "#                 \"(3, 0, 'privatecar')\",\n",
    "#                 \"(3, 0, 'tot')\",\n",
    "#                 \"(3, 0, 'unknowncar')\",\n",
    "#                 \"(3, 1, 'cargocar')\",\n",
    "#                 \"(3, 1, 'etc')\",\n",
    "#                 \"(3, 1, 'motorcycle')\",\n",
    "#                 \"(3, 1, 'privatecar')\",\n",
    "#                 \"(3, 1, 'tot')\",\n",
    "#                 \"(3, 1, 'unknowncar')\",\n",
    "                \"('A', 2)\",\n",
    "#                 \"('A', 3)\",\n",
    "#                 \"('B', 1)\",\n",
    "#                 \"('B', 3)\",\n",
    "#                 \"('C', 1)\",\n",
    "#                 \"('C', 3)\",\n",
    "                'date',\n",
    "                'hour',\n",
    "                'pressure',\n",
    "                'sea_pressure',\n",
    "                'wind_direction',\n",
    "                'wind_speed',\n",
    "                'temperature',\n",
    "                'rel_humidity',\n",
    "                'precipitation',\n",
    "                'dayofweek',\n",
    "                'is_holiday',\n",
    "                'timeofday'\n",
    "              ]\n",
    "\n",
    "sel_rows = sel_rows[using_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(2, 0, 'tot')</th>\n",
       "      <th>('A', 2)</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33.0</td>\n",
       "      <td>70.4825</td>\n",
       "      <td>2016-09-20 06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1012.200000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31.0</td>\n",
       "      <td>93.1900</td>\n",
       "      <td>2016-09-20 06:20:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1012.244444</td>\n",
       "      <td>1017.244444</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>4.588889</td>\n",
       "      <td>26.733333</td>\n",
       "      <td>42.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54.0</td>\n",
       "      <td>41.0900</td>\n",
       "      <td>2016-09-20 06:40:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1012.288889</td>\n",
       "      <td>1017.288889</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>4.577778</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>42.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>79.0</td>\n",
       "      <td>71.1400</td>\n",
       "      <td>2016-09-20 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1012.333333</td>\n",
       "      <td>1017.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>86.0</td>\n",
       "      <td>55.6600</td>\n",
       "      <td>2016-09-20 07:20:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1012.377778</td>\n",
       "      <td>1017.377778</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>42.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    (2, 0, 'tot')  ('A', 2)                date  hour     pressure  \\\n",
       "18           33.0   70.4825 2016-09-20 06:00:00     6  1012.200000   \n",
       "19           31.0   93.1900 2016-09-20 06:20:00     6  1012.244444   \n",
       "20           54.0   41.0900 2016-09-20 06:40:00     6  1012.288889   \n",
       "21           79.0   71.1400 2016-09-20 07:00:00     7  1012.333333   \n",
       "22           86.0   55.6600 2016-09-20 07:20:00     7  1012.377778   \n",
       "\n",
       "    sea_pressure  wind_direction  wind_speed  temperature  rel_humidity  \\\n",
       "18   1017.200000        7.000000    4.600000    26.900000     43.000000   \n",
       "19   1017.244444        7.444444    4.588889    26.733333     42.888889   \n",
       "20   1017.288889        7.888889    4.577778    26.566667     42.777778   \n",
       "21   1017.333333        8.333333    4.566667    26.400000     42.666667   \n",
       "22   1017.377778        8.777778    4.555556    26.233333     42.555556   \n",
       "\n",
       "    precipitation  dayofweek  is_holiday  timeofday  \n",
       "18            0.0          1           0   6.000000  \n",
       "19            0.0          1           0   6.333333  \n",
       "20            0.0          1           0   6.666667  \n",
       "21            0.0          1           0   7.000000  \n",
       "22            0.0          1           0   7.333333  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split to train and test set\n",
    "train_rows = sel_rows[: -24*7]\n",
    "test_rows = sel_rows[-24*7:] #reserve 1 week for test\n",
    "\n",
    "# del date column for eliminate datetime\n",
    "del train_rows['date']\n",
    "del test_rows['date']\n",
    "\n",
    "# get numpy array from panda dataframe\n",
    "train_arr = train_rows.values\n",
    "test_arr = test_rows.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.        ,  70.4825    ,   6.        , ...,   1.        ,\n",
       "          0.        ,   6.        ],\n",
       "       [ 31.        ,  93.19      ,   6.        , ...,   1.        ,\n",
       "          0.        ,   6.33333333],\n",
       "       [ 54.        ,  41.09      ,   6.        , ...,   1.        ,\n",
       "          0.        ,   6.66666667],\n",
       "       ..., \n",
       "       [ 59.        ,  83.91      ,  18.        , ...,   0.        ,\n",
       "          1.        ,  18.        ],\n",
       "       [ 53.        ,  66.03      ,  18.        , ...,   0.        ,\n",
       "          1.        ,  18.33333333],\n",
       "       [ 41.        ,  46.78      ,  18.        , ...,   0.        ,\n",
       "          1.        ,  18.66666667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a differenced series for Volumn and traffic time [modified from http://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/]\n",
    "def difference(dataset, interval=1):\n",
    "    diff_array = np.ndarray(np.shape(dataset))\n",
    "    for i in range(interval, len(dataset)):\n",
    "        diff_array[i][0:2] = dataset[i][0:2] - dataset[i - interval][0:2]  # only select index 0 & 1\n",
    "    return diff_array[1:]  # eliminate the first row (all zeros)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, y_hat, interval=1):\n",
    "    return y_hat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   ,  -6.48 ,   0.   , ...,   0.   ,   0.   ,   2.   ],\n",
       "       [ 21.   ,  19.4  ,   2.   , ...,   7.   ,  10.   ,  11.   ],\n",
       "       [ 42.   ,  -0.745,   5.   , ...,  18.   ,  12.   ,  23.   ],\n",
       "       ..., \n",
       "       [-25.   ,   1.2  ,   0.   , ...,   1.   ,   2.   ,   2.   ],\n",
       "       [ -2.   ,   8.27 ,  15.   , ...,  38.   ,  37.   ,  30.   ],\n",
       "       [ -3.   , -18.11 ,  25.   , ...,  27.   ,  24.   ,  21.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform to be stationary\n",
    "train_arr_w_station = difference(train_arr, 1)\n",
    "train_arr_w_station\n",
    "\n",
    "test_arr_w_station = difference(test_arr, 1)\n",
    "test_arr_w_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale feature array to range -1 to 1\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(train_arr)\n",
    "train_scaled_arr = scaler.transform(train_arr_w_station)\n",
    "\n",
    "test_scaled_arr = scaler.transform(test_arr_w_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.10810811e-01,  -1.07387608e+00,  -5.00000000e-01,\n",
       "         -1.52328244e+02,  -1.51541353e+02,  -9.60451977e-01,\n",
       "          8.50746269e-01,  -2.36934673e+00,  -2.07142857e+00,\n",
       "          3.00000000e+00,   2.00000000e+00,   1.70000000e+01,\n",
       "         -5.26315789e-01],\n",
       "       [ -1.04054054e+00,  -1.26684778e+00,  -1.50000000e+00,\n",
       "         -1.53854962e+02,  -1.52443609e+02,  -9.94350282e-01,\n",
       "         -1.23880597e+00,  -2.48241206e+00,  -2.35714286e+00,\n",
       "          9.09090909e-02,   0.00000000e+00,  -1.00000000e+00,\n",
       "         -1.63157895e+00],\n",
       "       [ -1.33783784e+00,  -1.16028394e+00,  -2.00000000e+00,\n",
       "         -1.54007634e+02,  -1.52293233e+02,  -1.01694915e+00,\n",
       "         -9.40298507e-01,  -2.59547739e+00,  -2.46428571e+00,\n",
       "         -1.00000000e+00,  -6.66666667e-01,   3.00000000e+00,\n",
       "         -1.63157895e+00],\n",
       "       [ -1.02702703e+00,  -1.09832618e+00,   5.00000000e-01,\n",
       "         -1.49885496e+02,  -1.45977444e+02,  -7.79661017e-01,\n",
       "          1.30895522e+01,   2.15326633e+00,  -9.28571429e-01,\n",
       "          1.60909091e+01,   1.16666667e+01,   7.30000000e+01,\n",
       "          2.78947368e+00],\n",
       "       [ -1.04054054e+00,  -1.32950662e+00,   2.16666667e+00,\n",
       "         -1.51564885e+02,  -1.49436090e+02,  -9.32203390e-01,\n",
       "          2.34328358e+00,  -3.34170854e-01,  -1.71428571e+00,\n",
       "          4.81818182e+00,   8.00000000e+00,   4.70000000e+01,\n",
       "          1.36842105e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled_arr[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample subsequence from the time series\n",
    "train_seqs = []\n",
    "nSegments = train_scaled_arr.shape[0]//12 # each segment holds 4hr data (12 datapoints, 20min each)\n",
    "for segment in range(nSegments):\n",
    "    for t in range(6):\n",
    "        startIdx = segment*12 + t\n",
    "        train_seqs.append(train_scaled_arr[startIdx: startIdx+7])\n",
    "train_seqs = np.stack(train_seqs)\n",
    "\n",
    "test_seqs = []\n",
    "nSegments = test_scaled_arr.shape[0]//12 # each segment holds 4hr data (12 datapoints, 20min each)\n",
    "for segment in range(nSegments):\n",
    "    for t in range(6):\n",
    "        startIdx = segment*12 + t\n",
    "        test_seqs.append(test_scaled_arr[startIdx: startIdx+7])\n",
    "test_seqs = np.stack(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 7, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras\n",
    "#https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "input_dim = len(using_cols) - 1  # The minus 1 is for deleted \"date\" feature\n",
    "output_dim = len(using_cols) -1\n",
    "timesteps = 6 # use 6 timesteps to predict the 7th\n",
    "\n",
    "x_train, y_train = train_seqs[:, 0:-1], train_seqs[:, -1]\n",
    "x_test , y_test  =  test_seqs[:, 0:-1],  test_seqs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 6, 128)            72704     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 205,965\n",
      "Trainable params: 205,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 246 samples, validate on 78 samples\n",
      "Epoch 1/1000\n",
      "246/246 [==============================] - 1s - loss: 3975.4618 - val_loss: 3914.5950\n",
      "Epoch 2/1000\n",
      "246/246 [==============================] - 0s - loss: 3908.4085 - val_loss: 3838.1294\n",
      "Epoch 3/1000\n",
      "246/246 [==============================] - 0s - loss: 3826.6027 - val_loss: 3750.8158\n",
      "Epoch 4/1000\n",
      "246/246 [==============================] - 0s - loss: 3737.0962 - val_loss: 3657.9270\n",
      "Epoch 5/1000\n",
      "246/246 [==============================] - 0s - loss: 3643.4627 - val_loss: 3570.9381\n",
      "Epoch 6/1000\n",
      "246/246 [==============================] - 0s - loss: 3567.9279 - val_loss: 3515.0657\n",
      "Epoch 7/1000\n",
      "246/246 [==============================] - 0s - loss: 3518.4204 - val_loss: 3474.3140\n",
      "Epoch 8/1000\n",
      "246/246 [==============================] - 0s - loss: 3480.9487 - val_loss: 3440.7394\n",
      "Epoch 9/1000\n",
      "246/246 [==============================] - 0s - loss: 3447.9785 - val_loss: 3409.8346\n",
      "Epoch 10/1000\n",
      "246/246 [==============================] - 0s - loss: 3417.0149 - val_loss: 3379.4421\n",
      "Epoch 11/1000\n",
      "246/246 [==============================] - 0s - loss: 3386.7746 - val_loss: 3349.4041\n",
      "Epoch 12/1000\n",
      "246/246 [==============================] - 0s - loss: 3356.9872 - val_loss: 3319.9006\n",
      "Epoch 13/1000\n",
      "246/246 [==============================] - 0s - loss: 3327.5457 - val_loss: 3290.8959\n",
      "Epoch 14/1000\n",
      "246/246 [==============================] - 0s - loss: 3298.9568 - val_loss: 3262.3516\n",
      "Epoch 15/1000\n",
      "246/246 [==============================] - 0s - loss: 3270.5384 - val_loss: 3234.3999\n",
      "Epoch 16/1000\n",
      "246/246 [==============================] - 0s - loss: 3243.0004 - val_loss: 3206.8794\n",
      "Epoch 17/1000\n",
      "246/246 [==============================] - 0s - loss: 3215.9305 - val_loss: 3179.8023\n",
      "Epoch 18/1000\n",
      "246/246 [==============================] - 0s - loss: 3188.9379 - val_loss: 3153.2066\n",
      "Epoch 19/1000\n",
      "246/246 [==============================] - 0s - loss: 3162.7794 - val_loss: 3126.9933\n",
      "Epoch 20/1000\n",
      "246/246 [==============================] - 0s - loss: 3136.8538 - val_loss: 3101.1554\n",
      "Epoch 21/1000\n",
      "246/246 [==============================] - 0s - loss: 3111.1341 - val_loss: 3075.7152\n",
      "Epoch 22/1000\n",
      "246/246 [==============================] - 0s - loss: 3086.1135 - val_loss: 3050.5637\n",
      "Epoch 23/1000\n",
      "246/246 [==============================] - 0s - loss: 3061.1234 - val_loss: 3025.7698\n",
      "Epoch 24/1000\n",
      "246/246 [==============================] - 0s - loss: 3036.6784 - val_loss: 3001.2959\n",
      "Epoch 25/1000\n",
      "246/246 [==============================] - 0s - loss: 3012.4312 - val_loss: 2977.1045\n",
      "Epoch 26/1000\n",
      "246/246 [==============================] - 0s - loss: 2988.5234 - val_loss: 2953.1797\n",
      "Epoch 27/1000\n",
      "246/246 [==============================] - 0s - loss: 2964.7784 - val_loss: 2929.5414\n",
      "Epoch 28/1000\n",
      "246/246 [==============================] - 0s - loss: 2941.5814 - val_loss: 2906.1245\n",
      "Epoch 29/1000\n",
      "246/246 [==============================] - 0s - loss: 2918.3755 - val_loss: 2882.9908\n",
      "Epoch 30/1000\n",
      "246/246 [==============================] - 0s - loss: 2895.3712 - val_loss: 2860.0778\n",
      "Epoch 31/1000\n",
      "246/246 [==============================] - 0s - loss: 2872.7508 - val_loss: 2837.4031\n",
      "Epoch 32/1000\n",
      "246/246 [==============================] - 0s - loss: 2850.2799 - val_loss: 2814.9755\n",
      "Epoch 33/1000\n",
      "246/246 [==============================] - 0s - loss: 2827.9869 - val_loss: 2792.7707\n",
      "Epoch 34/1000\n",
      "246/246 [==============================] - 0s - loss: 2806.1739 - val_loss: 2770.7714\n",
      "Epoch 35/1000\n",
      "246/246 [==============================] - 0s - loss: 2784.1943 - val_loss: 2748.9981\n",
      "Epoch 36/1000\n",
      "246/246 [==============================] - 0s - loss: 2762.7010 - val_loss: 2727.4243\n",
      "Epoch 37/1000\n",
      "246/246 [==============================] - 0s - loss: 2741.2989 - val_loss: 2706.0731\n",
      "Epoch 38/1000\n",
      "246/246 [==============================] - 0s - loss: 2720.1392 - val_loss: 2684.9311\n",
      "Epoch 39/1000\n",
      "246/246 [==============================] - 0s - loss: 2699.2486 - val_loss: 2663.9687\n",
      "Epoch 40/1000\n",
      "246/246 [==============================] - 0s - loss: 2678.5060 - val_loss: 2643.2127\n",
      "Epoch 41/1000\n",
      "246/246 [==============================] - 0s - loss: 2657.8073 - val_loss: 2622.6611\n",
      "Epoch 42/1000\n",
      "246/246 [==============================] - 0s - loss: 2637.4797 - val_loss: 2602.2775\n",
      "Epoch 43/1000\n",
      "246/246 [==============================] - 0s - loss: 2617.3093 - val_loss: 2582.0675\n",
      "Epoch 44/1000\n",
      "246/246 [==============================] - 0s - loss: 2597.3183 - val_loss: 2562.0452\n",
      "Epoch 45/1000\n",
      "246/246 [==============================] - 0s - loss: 2577.4803 - val_loss: 2542.2236\n",
      "Epoch 46/1000\n",
      "246/246 [==============================] - 0s - loss: 2557.8166 - val_loss: 2522.5614\n",
      "Epoch 47/1000\n",
      "246/246 [==============================] - 0s - loss: 2538.2597 - val_loss: 2503.0929\n",
      "Epoch 48/1000\n",
      "246/246 [==============================] - 0s - loss: 2519.1324 - val_loss: 2483.7688\n",
      "Epoch 49/1000\n",
      "246/246 [==============================] - 0s - loss: 2499.9038 - val_loss: 2464.6166\n",
      "Epoch 50/1000\n",
      "246/246 [==============================] - 0s - loss: 2480.8960 - val_loss: 2445.6258\n",
      "Epoch 51/1000\n",
      "246/246 [==============================] - 0s - loss: 2462.1344 - val_loss: 2426.8136\n",
      "Epoch 52/1000\n",
      "246/246 [==============================] - 0s - loss: 2443.4794 - val_loss: 2408.1626\n",
      "Epoch 53/1000\n",
      "246/246 [==============================] - 0s - loss: 2424.9770 - val_loss: 2389.6921\n",
      "Epoch 54/1000\n",
      "246/246 [==============================] - 0s - loss: 2406.6293 - val_loss: 2371.3586\n",
      "Epoch 55/1000\n",
      "246/246 [==============================] - 0s - loss: 2388.5031 - val_loss: 2353.1809\n",
      "Epoch 56/1000\n",
      "246/246 [==============================] - 0s - loss: 2370.5376 - val_loss: 2335.1360\n",
      "Epoch 57/1000\n",
      "246/246 [==============================] - 0s - loss: 2352.7254 - val_loss: 2317.2378\n",
      "Epoch 58/1000\n",
      "246/246 [==============================] - 0s - loss: 2334.8745 - val_loss: 2299.5023\n",
      "Epoch 59/1000\n",
      "246/246 [==============================] - 0s - loss: 2317.3863 - val_loss: 2281.9422\n",
      "Epoch 60/1000\n",
      "246/246 [==============================] - 0s - loss: 2299.8638 - val_loss: 2264.5147\n",
      "Epoch 61/1000\n",
      "246/246 [==============================] - 0s - loss: 2282.6341 - val_loss: 2247.2315\n",
      "Epoch 62/1000\n",
      "246/246 [==============================] - 0s - loss: 2265.5708 - val_loss: 2230.0867\n",
      "Epoch 63/1000\n",
      "246/246 [==============================] - 0s - loss: 2248.5461 - val_loss: 2213.0935\n",
      "Epoch 64/1000\n",
      "246/246 [==============================] - 0s - loss: 2231.7291 - val_loss: 2196.2167\n",
      "Epoch 65/1000\n",
      "246/246 [==============================] - 0s - loss: 2215.0682 - val_loss: 2179.4775\n",
      "Epoch 66/1000\n",
      "246/246 [==============================] - 0s - loss: 2198.3732 - val_loss: 2162.8930\n",
      "Epoch 67/1000\n",
      "246/246 [==============================] - 0s - loss: 2182.0182 - val_loss: 2146.4478\n",
      "Epoch 68/1000\n",
      "246/246 [==============================] - 0s - loss: 2165.7251 - val_loss: 2130.1376\n",
      "Epoch 69/1000\n",
      "246/246 [==============================] - 0s - loss: 2149.5706 - val_loss: 2113.9568\n",
      "Epoch 70/1000\n",
      "246/246 [==============================] - 0s - loss: 2133.5479 - val_loss: 2097.9166\n",
      "Epoch 71/1000\n",
      "246/246 [==============================] - 0s - loss: 2117.6309 - val_loss: 2081.9878\n",
      "Epoch 72/1000\n",
      "246/246 [==============================] - 0s - loss: 2101.8806 - val_loss: 2066.1911\n",
      "Epoch 73/1000\n",
      "246/246 [==============================] - 0s - loss: 2086.2201 - val_loss: 2050.5158\n",
      "Epoch 74/1000\n",
      "246/246 [==============================] - 0s - loss: 2070.7189 - val_loss: 2034.9797\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 2055.3811 - val_loss: 2019.5715\n",
      "Epoch 76/1000\n",
      "246/246 [==============================] - 0s - loss: 2040.0406 - val_loss: 2004.3051\n",
      "Epoch 77/1000\n",
      "246/246 [==============================] - 0s - loss: 2024.9379 - val_loss: 1989.1299\n",
      "Epoch 78/1000\n",
      "246/246 [==============================] - 0s - loss: 2009.9276 - val_loss: 1974.1135\n",
      "Epoch 79/1000\n",
      "246/246 [==============================] - 0s - loss: 1995.0364 - val_loss: 1959.1949\n",
      "Epoch 80/1000\n",
      "246/246 [==============================] - 0s - loss: 1980.2285 - val_loss: 1944.3912\n",
      "Epoch 81/1000\n",
      "246/246 [==============================] - 0s - loss: 1965.6338 - val_loss: 1929.7079\n",
      "Epoch 82/1000\n",
      "246/246 [==============================] - 0s - loss: 1951.0928 - val_loss: 1915.1267\n",
      "Epoch 83/1000\n",
      "246/246 [==============================] - 0s - loss: 1936.6351 - val_loss: 1900.6581\n",
      "Epoch 84/1000\n",
      "246/246 [==============================] - 0s - loss: 1922.2991 - val_loss: 1886.3150\n",
      "Epoch 85/1000\n",
      "246/246 [==============================] - 0s - loss: 1908.2379 - val_loss: 1872.0952\n",
      "Epoch 86/1000\n",
      "246/246 [==============================] - 0s - loss: 1894.0637 - val_loss: 1857.9933\n",
      "Epoch 87/1000\n",
      "246/246 [==============================] - 0s - loss: 1880.1432 - val_loss: 1844.0156\n",
      "Epoch 88/1000\n",
      "246/246 [==============================] - 0s - loss: 1866.2605 - val_loss: 1830.1312\n",
      "Epoch 89/1000\n",
      "246/246 [==============================] - 0s - loss: 1852.5258 - val_loss: 1816.3657\n",
      "Epoch 90/1000\n",
      "246/246 [==============================] - 0s - loss: 1838.8795 - val_loss: 1802.6888\n",
      "Epoch 91/1000\n",
      "246/246 [==============================] - 0s - loss: 1825.3491 - val_loss: 1789.1240\n",
      "Epoch 92/1000\n",
      "246/246 [==============================] - 0s - loss: 1811.9781 - val_loss: 1775.6953\n",
      "Epoch 93/1000\n",
      "246/246 [==============================] - 0s - loss: 1798.6260 - val_loss: 1762.3668\n",
      "Epoch 94/1000\n",
      "246/246 [==============================] - 0s - loss: 1785.4050 - val_loss: 1749.1464\n",
      "Epoch 95/1000\n",
      "246/246 [==============================] - 0s - loss: 1772.3265 - val_loss: 1736.0044\n",
      "Epoch 96/1000\n",
      "246/246 [==============================] - 0s - loss: 1759.3037 - val_loss: 1722.9980\n",
      "Epoch 97/1000\n",
      "246/246 [==============================] - 0s - loss: 1746.5016 - val_loss: 1710.0890\n",
      "Epoch 98/1000\n",
      "246/246 [==============================] - 0s - loss: 1733.6692 - val_loss: 1697.2679\n",
      "Epoch 99/1000\n",
      "246/246 [==============================] - 0s - loss: 1720.9197 - val_loss: 1684.5297\n",
      "Epoch 100/1000\n",
      "246/246 [==============================] - 0s - loss: 1708.3359 - val_loss: 1671.8789\n",
      "Epoch 101/1000\n",
      "246/246 [==============================] - 0s - loss: 1695.8765 - val_loss: 1659.3213\n",
      "Epoch 102/1000\n",
      "246/246 [==============================] - 0s - loss: 1683.4921 - val_loss: 1646.9099\n",
      "Epoch 103/1000\n",
      "246/246 [==============================] - 0s - loss: 1671.1525 - val_loss: 1634.5970\n",
      "Epoch 104/1000\n",
      "246/246 [==============================] - 0s - loss: 1658.9893 - val_loss: 1622.4004\n",
      "Epoch 105/1000\n",
      "246/246 [==============================] - 0s - loss: 1646.8953 - val_loss: 1610.2649\n",
      "Epoch 106/1000\n",
      "246/246 [==============================] - 0s - loss: 1634.8305 - val_loss: 1598.2196\n",
      "Epoch 107/1000\n",
      "246/246 [==============================] - 0s - loss: 1622.9521 - val_loss: 1586.2799\n",
      "Epoch 108/1000\n",
      "246/246 [==============================] - 0s - loss: 1611.1481 - val_loss: 1574.4400\n",
      "Epoch 109/1000\n",
      "246/246 [==============================] - 0s - loss: 1599.4012 - val_loss: 1562.6644\n",
      "Epoch 110/1000\n",
      "246/246 [==============================] - 0s - loss: 1587.7835 - val_loss: 1551.0178\n",
      "Epoch 111/1000\n",
      "246/246 [==============================] - 0s - loss: 1576.2188 - val_loss: 1539.4420\n",
      "Epoch 112/1000\n",
      "246/246 [==============================] - 0s - loss: 1564.8319 - val_loss: 1527.9748\n",
      "Epoch 113/1000\n",
      "246/246 [==============================] - 0s - loss: 1553.4413 - val_loss: 1516.5557\n",
      "Epoch 114/1000\n",
      "246/246 [==============================] - 0s - loss: 1542.1528 - val_loss: 1505.2749\n",
      "Epoch 115/1000\n",
      "246/246 [==============================] - 0s - loss: 1530.9907 - val_loss: 1494.0673\n",
      "Epoch 116/1000\n",
      "246/246 [==============================] - 0s - loss: 1519.9704 - val_loss: 1482.9787\n",
      "Epoch 117/1000\n",
      "246/246 [==============================] - 0s - loss: 1508.8668 - val_loss: 1471.9325\n",
      "Epoch 118/1000\n",
      "246/246 [==============================] - 0s - loss: 1497.9844 - val_loss: 1460.9503\n",
      "Epoch 119/1000\n",
      "246/246 [==============================] - 0s - loss: 1487.1607 - val_loss: 1450.0900\n",
      "Epoch 120/1000\n",
      "246/246 [==============================] - 0s - loss: 1476.3824 - val_loss: 1439.3377\n",
      "Epoch 121/1000\n",
      "246/246 [==============================] - 0s - loss: 1465.7598 - val_loss: 1428.6639\n",
      "Epoch 122/1000\n",
      "246/246 [==============================] - 0s - loss: 1455.1715 - val_loss: 1418.0701\n",
      "Epoch 123/1000\n",
      "246/246 [==============================] - 0s - loss: 1444.6779 - val_loss: 1407.5460\n",
      "Epoch 124/1000\n",
      "246/246 [==============================] - 0s - loss: 1434.2723 - val_loss: 1397.0900\n",
      "Epoch 125/1000\n",
      "246/246 [==============================] - 0s - loss: 1423.9324 - val_loss: 1386.7170\n",
      "Epoch 126/1000\n",
      "246/246 [==============================] - 0s - loss: 1413.6792 - val_loss: 1376.4482\n",
      "Epoch 127/1000\n",
      "246/246 [==============================] - 0s - loss: 1403.5334 - val_loss: 1366.2504\n",
      "Epoch 128/1000\n",
      "246/246 [==============================] - 0s - loss: 1393.4570 - val_loss: 1356.1620\n",
      "Epoch 129/1000\n",
      "246/246 [==============================] - 0s - loss: 1383.4717 - val_loss: 1346.1105\n",
      "Epoch 130/1000\n",
      "246/246 [==============================] - 0s - loss: 1373.5636 - val_loss: 1336.1916\n",
      "Epoch 131/1000\n",
      "246/246 [==============================] - 0s - loss: 1363.6919 - val_loss: 1326.3313\n",
      "Epoch 132/1000\n",
      "246/246 [==============================] - 0s - loss: 1353.9179 - val_loss: 1316.5486\n",
      "Epoch 133/1000\n",
      "246/246 [==============================] - 0s - loss: 1344.2335 - val_loss: 1306.8426\n",
      "Epoch 134/1000\n",
      "246/246 [==============================] - 0s - loss: 1334.6405 - val_loss: 1297.1905\n",
      "Epoch 135/1000\n",
      "246/246 [==============================] - 0s - loss: 1325.1105 - val_loss: 1287.6333\n",
      "Epoch 136/1000\n",
      "246/246 [==============================] - 0s - loss: 1315.6488 - val_loss: 1278.1406\n",
      "Epoch 137/1000\n",
      "246/246 [==============================] - 0s - loss: 1306.2945 - val_loss: 1268.7571\n",
      "Epoch 138/1000\n",
      "246/246 [==============================] - 0s - loss: 1296.9828 - val_loss: 1259.4192\n",
      "Epoch 139/1000\n",
      "246/246 [==============================] - 0s - loss: 1287.7586 - val_loss: 1250.1580\n",
      "Epoch 140/1000\n",
      "246/246 [==============================] - 0s - loss: 1278.5932 - val_loss: 1240.9689\n",
      "Epoch 141/1000\n",
      "246/246 [==============================] - 0s - loss: 1269.5365 - val_loss: 1231.8468\n",
      "Epoch 142/1000\n",
      "246/246 [==============================] - 0s - loss: 1260.5127 - val_loss: 1222.8236\n",
      "Epoch 143/1000\n",
      "246/246 [==============================] - 0s - loss: 1251.5849 - val_loss: 1213.8844\n",
      "Epoch 144/1000\n",
      "246/246 [==============================] - 0s - loss: 1242.7218 - val_loss: 1204.9962\n",
      "Epoch 145/1000\n",
      "246/246 [==============================] - 0s - loss: 1233.9672 - val_loss: 1196.1605\n",
      "Epoch 146/1000\n",
      "246/246 [==============================] - 0s - loss: 1225.2258 - val_loss: 1187.4524\n",
      "Epoch 147/1000\n",
      "246/246 [==============================] - 0s - loss: 1216.5978 - val_loss: 1178.7665\n",
      "Epoch 148/1000\n",
      "246/246 [==============================] - 0s - loss: 1208.0210 - val_loss: 1170.1742\n",
      "Epoch 149/1000\n",
      "246/246 [==============================] - 0s - loss: 1199.5015 - val_loss: 1161.6418\n",
      "Epoch 150/1000\n",
      "246/246 [==============================] - 0s - loss: 1191.0909 - val_loss: 1153.1928\n",
      "Epoch 151/1000\n",
      "246/246 [==============================] - 0s - loss: 1182.7066 - val_loss: 1144.7833\n",
      "Epoch 152/1000\n",
      "246/246 [==============================] - 0s - loss: 1174.4315 - val_loss: 1136.4373\n",
      "Epoch 153/1000\n",
      "246/246 [==============================] - 0s - loss: 1166.2020 - val_loss: 1128.1602\n",
      "Epoch 154/1000\n",
      "246/246 [==============================] - 0s - loss: 1158.0230 - val_loss: 1119.9780\n",
      "Epoch 155/1000\n",
      "246/246 [==============================] - 0s - loss: 1149.9724 - val_loss: 1111.8958\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 1141.9931 - val_loss: 1103.8660\n",
      "Epoch 157/1000\n",
      "246/246 [==============================] - 0s - loss: 1133.9644 - val_loss: 1095.8249\n",
      "Epoch 158/1000\n",
      "246/246 [==============================] - 0s - loss: 1126.0583 - val_loss: 1087.8889\n",
      "Epoch 159/1000\n",
      "246/246 [==============================] - 0s - loss: 1118.2331 - val_loss: 1080.0210\n",
      "Epoch 160/1000\n",
      "246/246 [==============================] - 0s - loss: 1110.4709 - val_loss: 1072.2272\n",
      "Epoch 161/1000\n",
      "246/246 [==============================] - 0s - loss: 1102.7830 - val_loss: 1064.5050\n",
      "Epoch 162/1000\n",
      "246/246 [==============================] - 0s - loss: 1095.1589 - val_loss: 1056.8428\n",
      "Epoch 163/1000\n",
      "246/246 [==============================] - 0s - loss: 1087.5857 - val_loss: 1049.2510\n",
      "Epoch 164/1000\n",
      "246/246 [==============================] - 0s - loss: 1080.1221 - val_loss: 1041.7672\n",
      "Epoch 165/1000\n",
      "246/246 [==============================] - 0s - loss: 1072.6507 - val_loss: 1034.2743\n",
      "Epoch 166/1000\n",
      "246/246 [==============================] - 0s - loss: 1065.2671 - val_loss: 1026.8675\n",
      "Epoch 167/1000\n",
      "246/246 [==============================] - 0s - loss: 1057.9535 - val_loss: 1019.5161\n",
      "Epoch 168/1000\n",
      "246/246 [==============================] - 0s - loss: 1050.6944 - val_loss: 1012.2563\n",
      "Epoch 169/1000\n",
      "246/246 [==============================] - 0s - loss: 1043.5102 - val_loss: 1005.0248\n",
      "Epoch 170/1000\n",
      "246/246 [==============================] - 0s - loss: 1036.3866 - val_loss: 997.8532\n",
      "Epoch 171/1000\n",
      "246/246 [==============================] - 0s - loss: 1029.3168 - val_loss: 990.7598\n",
      "Epoch 172/1000\n",
      "246/246 [==============================] - 0s - loss: 1022.3021 - val_loss: 983.7372\n",
      "Epoch 173/1000\n",
      "246/246 [==============================] - 0s - loss: 1015.3565 - val_loss: 976.7734\n",
      "Epoch 174/1000\n",
      "246/246 [==============================] - 0s - loss: 1008.4861 - val_loss: 969.8653\n",
      "Epoch 175/1000\n",
      "246/246 [==============================] - 0s - loss: 1001.6339 - val_loss: 963.0290\n",
      "Epoch 176/1000\n",
      "246/246 [==============================] - 0s - loss: 994.8854 - val_loss: 956.2053\n",
      "Epoch 177/1000\n",
      "246/246 [==============================] - 0s - loss: 988.1483 - val_loss: 949.4757\n",
      "Epoch 178/1000\n",
      "246/246 [==============================] - 0s - loss: 981.5061 - val_loss: 942.7814\n",
      "Epoch 179/1000\n",
      "246/246 [==============================] - 0s - loss: 974.8992 - val_loss: 936.1653\n",
      "Epoch 180/1000\n",
      "246/246 [==============================] - 0s - loss: 968.3725 - val_loss: 929.6090\n",
      "Epoch 181/1000\n",
      "246/246 [==============================] - 0s - loss: 961.8852 - val_loss: 923.1214\n",
      "Epoch 182/1000\n",
      "246/246 [==============================] - 0s - loss: 955.4621 - val_loss: 916.6565\n",
      "Epoch 183/1000\n",
      "246/246 [==============================] - 0s - loss: 949.0906 - val_loss: 910.2795\n",
      "Epoch 184/1000\n",
      "246/246 [==============================] - 0s - loss: 942.7873 - val_loss: 903.9169\n",
      "Epoch 185/1000\n",
      "246/246 [==============================] - 0s - loss: 936.5246 - val_loss: 897.6040\n",
      "Epoch 186/1000\n",
      "246/246 [==============================] - 0s - loss: 930.3496 - val_loss: 891.3419\n",
      "Epoch 187/1000\n",
      "246/246 [==============================] - 0s - loss: 924.1752 - val_loss: 885.1923\n",
      "Epoch 188/1000\n",
      "246/246 [==============================] - 0s - loss: 918.1267 - val_loss: 879.0463\n",
      "Epoch 189/1000\n",
      "246/246 [==============================] - 0s - loss: 912.0687 - val_loss: 873.0284\n",
      "Epoch 190/1000\n",
      "246/246 [==============================] - 0s - loss: 906.0675 - val_loss: 867.0318\n",
      "Epoch 191/1000\n",
      "246/246 [==============================] - 0s - loss: 900.1403 - val_loss: 861.0844\n",
      "Epoch 192/1000\n",
      "246/246 [==============================] - 0s - loss: 894.2611 - val_loss: 855.1620\n",
      "Epoch 193/1000\n",
      "246/246 [==============================] - 0s - loss: 888.4396 - val_loss: 849.3211\n",
      "Epoch 194/1000\n",
      "246/246 [==============================] - 0s - loss: 882.6704 - val_loss: 843.4992\n",
      "Epoch 195/1000\n",
      "246/246 [==============================] - 0s - loss: 876.9490 - val_loss: 837.7433\n",
      "Epoch 196/1000\n",
      "246/246 [==============================] - 0s - loss: 871.2958 - val_loss: 832.0334\n",
      "Epoch 197/1000\n",
      "246/246 [==============================] - 0s - loss: 865.6653 - val_loss: 826.4117\n",
      "Epoch 198/1000\n",
      "246/246 [==============================] - 0s - loss: 860.1032 - val_loss: 820.8092\n",
      "Epoch 199/1000\n",
      "246/246 [==============================] - 0s - loss: 854.6096 - val_loss: 815.2421\n",
      "Epoch 200/1000\n",
      "246/246 [==============================] - 0s - loss: 849.1138 - val_loss: 809.7729\n",
      "Epoch 201/1000\n",
      "246/246 [==============================] - 0s - loss: 843.7058 - val_loss: 804.3460\n",
      "Epoch 202/1000\n",
      "246/246 [==============================] - 0s - loss: 838.3661 - val_loss: 798.9814\n",
      "Epoch 203/1000\n",
      "246/246 [==============================] - 0s - loss: 833.0253 - val_loss: 793.6344\n",
      "Epoch 204/1000\n",
      "246/246 [==============================] - 0s - loss: 827.7663 - val_loss: 788.3116\n",
      "Epoch 205/1000\n",
      "246/246 [==============================] - 0s - loss: 822.5486 - val_loss: 783.0541\n",
      "Epoch 206/1000\n",
      "246/246 [==============================] - 0s - loss: 817.3676 - val_loss: 777.8724\n",
      "Epoch 207/1000\n",
      "246/246 [==============================] - 0s - loss: 812.3102 - val_loss: 772.7687\n",
      "Epoch 208/1000\n",
      "246/246 [==============================] - 0s - loss: 807.2025 - val_loss: 767.6202\n",
      "Epoch 209/1000\n",
      "246/246 [==============================] - 0s - loss: 802.1514 - val_loss: 762.5656\n",
      "Epoch 210/1000\n",
      "246/246 [==============================] - 0s - loss: 797.2217 - val_loss: 757.5251\n",
      "Epoch 211/1000\n",
      "246/246 [==============================] - 0s - loss: 792.2626 - val_loss: 752.6078\n",
      "Epoch 212/1000\n",
      "246/246 [==============================] - 0s - loss: 787.3630 - val_loss: 747.7008\n",
      "Epoch 213/1000\n",
      "246/246 [==============================] - 0s - loss: 782.5346 - val_loss: 742.8368\n",
      "Epoch 214/1000\n",
      "246/246 [==============================] - 0s - loss: 777.7872 - val_loss: 737.9904\n",
      "Epoch 215/1000\n",
      "246/246 [==============================] - 0s - loss: 772.9897 - val_loss: 733.2327\n",
      "Epoch 216/1000\n",
      "246/246 [==============================] - 0s - loss: 768.2986 - val_loss: 728.5403\n",
      "Epoch 217/1000\n",
      "246/246 [==============================] - 0s - loss: 763.6599 - val_loss: 723.8294\n",
      "Epoch 218/1000\n",
      "246/246 [==============================] - 0s - loss: 759.0238 - val_loss: 719.1885\n",
      "Epoch 219/1000\n",
      "246/246 [==============================] - 0s - loss: 754.4599 - val_loss: 714.6063\n",
      "Epoch 220/1000\n",
      "246/246 [==============================] - 0s - loss: 749.9330 - val_loss: 710.0814\n",
      "Epoch 221/1000\n",
      "246/246 [==============================] - 0s - loss: 745.4499 - val_loss: 705.5644\n",
      "Epoch 222/1000\n",
      "246/246 [==============================] - 0s - loss: 741.0313 - val_loss: 701.1169\n",
      "Epoch 223/1000\n",
      "246/246 [==============================] - 0s - loss: 736.6295 - val_loss: 696.6720\n",
      "Epoch 224/1000\n",
      "246/246 [==============================] - 0s - loss: 732.2749 - val_loss: 692.2956\n",
      "Epoch 225/1000\n",
      "246/246 [==============================] - 0s - loss: 727.9623 - val_loss: 687.9698\n",
      "Epoch 226/1000\n",
      "246/246 [==============================] - 0s - loss: 723.7060 - val_loss: 683.6884\n",
      "Epoch 227/1000\n",
      "246/246 [==============================] - 0s - loss: 719.4749 - val_loss: 679.4189\n",
      "Epoch 228/1000\n",
      "246/246 [==============================] - 0s - loss: 715.3125 - val_loss: 675.2144\n",
      "Epoch 229/1000\n",
      "246/246 [==============================] - 0s - loss: 711.1853 - val_loss: 671.0030\n",
      "Epoch 230/1000\n",
      "246/246 [==============================] - 0s - loss: 707.0949 - val_loss: 666.8484\n",
      "Epoch 231/1000\n",
      "246/246 [==============================] - 0s - loss: 702.9933 - val_loss: 662.7928\n",
      "Epoch 232/1000\n",
      "246/246 [==============================] - 0s - loss: 698.9856 - val_loss: 658.7709\n",
      "Epoch 233/1000\n",
      "246/246 [==============================] - 0s - loss: 695.0118 - val_loss: 654.7741\n",
      "Epoch 234/1000\n",
      "246/246 [==============================] - 0s - loss: 691.0610 - val_loss: 650.8399\n",
      "Epoch 235/1000\n",
      "246/246 [==============================] - 0s - loss: 687.1785 - val_loss: 646.9123\n",
      "Epoch 236/1000\n",
      "246/246 [==============================] - 0s - loss: 683.3064 - val_loss: 643.0419\n",
      "Epoch 237/1000\n",
      "246/246 [==============================] - 0s - loss: 679.5033 - val_loss: 639.1829\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 675.7174 - val_loss: 635.3673\n",
      "Epoch 239/1000\n",
      "246/246 [==============================] - 0s - loss: 671.9727 - val_loss: 631.6023\n",
      "Epoch 240/1000\n",
      "246/246 [==============================] - 0s - loss: 668.2810 - val_loss: 627.9274\n",
      "Epoch 241/1000\n",
      "246/246 [==============================] - 0s - loss: 664.6112 - val_loss: 624.2461\n",
      "Epoch 242/1000\n",
      "246/246 [==============================] - 0s - loss: 660.9626 - val_loss: 620.5754\n",
      "Epoch 243/1000\n",
      "246/246 [==============================] - 0s - loss: 657.3696 - val_loss: 616.9429\n",
      "Epoch 244/1000\n",
      "246/246 [==============================] - 0s - loss: 653.8475 - val_loss: 613.3166\n",
      "Epoch 245/1000\n",
      "246/246 [==============================] - 0s - loss: 650.3553 - val_loss: 609.7265\n",
      "Epoch 246/1000\n",
      "246/246 [==============================] - 0s - loss: 646.8337 - val_loss: 606.2162\n",
      "Epoch 247/1000\n",
      "246/246 [==============================] - 0s - loss: 643.3789 - val_loss: 602.7977\n",
      "Epoch 248/1000\n",
      "246/246 [==============================] - 0s - loss: 639.9773 - val_loss: 599.3690\n",
      "Epoch 249/1000\n",
      "246/246 [==============================] - 0s - loss: 636.5944 - val_loss: 595.9724\n",
      "Epoch 250/1000\n",
      "246/246 [==============================] - 0s - loss: 633.2515 - val_loss: 592.6088\n",
      "Epoch 251/1000\n",
      "246/246 [==============================] - 0s - loss: 629.9466 - val_loss: 589.2894\n",
      "Epoch 252/1000\n",
      "246/246 [==============================] - 0s - loss: 626.6908 - val_loss: 585.9934\n",
      "Epoch 253/1000\n",
      "246/246 [==============================] - 0s - loss: 623.4584 - val_loss: 582.7062\n",
      "Epoch 254/1000\n",
      "246/246 [==============================] - 0s - loss: 620.2746 - val_loss: 579.4583\n",
      "Epoch 255/1000\n",
      "246/246 [==============================] - 0s - loss: 617.0846 - val_loss: 576.2980\n",
      "Epoch 256/1000\n",
      "246/246 [==============================] - 0s - loss: 613.9457 - val_loss: 573.1318\n",
      "Epoch 257/1000\n",
      "246/246 [==============================] - 0s - loss: 610.8457 - val_loss: 569.9915\n",
      "Epoch 258/1000\n",
      "246/246 [==============================] - 0s - loss: 607.7756 - val_loss: 566.8947\n",
      "Epoch 259/1000\n",
      "246/246 [==============================] - 0s - loss: 604.8040 - val_loss: 563.7994\n",
      "Epoch 260/1000\n",
      "246/246 [==============================] - 0s - loss: 601.7529 - val_loss: 560.8076\n",
      "Epoch 261/1000\n",
      "246/246 [==============================] - 0s - loss: 598.7738 - val_loss: 557.8423\n",
      "Epoch 262/1000\n",
      "246/246 [==============================] - 0s - loss: 595.8505 - val_loss: 554.8698\n",
      "Epoch 263/1000\n",
      "246/246 [==============================] - 0s - loss: 592.9564 - val_loss: 551.9225\n",
      "Epoch 264/1000\n",
      "246/246 [==============================] - 0s - loss: 590.0655 - val_loss: 549.0433\n",
      "Epoch 265/1000\n",
      "246/246 [==============================] - 0s - loss: 587.2250 - val_loss: 546.2117\n",
      "Epoch 266/1000\n",
      "246/246 [==============================] - 0s - loss: 584.4406 - val_loss: 543.3426\n",
      "Epoch 267/1000\n",
      "246/246 [==============================] - 0s - loss: 581.6604 - val_loss: 540.5744\n",
      "Epoch 268/1000\n",
      "246/246 [==============================] - 0s - loss: 578.8904 - val_loss: 537.7811\n",
      "Epoch 269/1000\n",
      "246/246 [==============================] - 0s - loss: 576.1701 - val_loss: 535.0150\n",
      "Epoch 270/1000\n",
      "246/246 [==============================] - 0s - loss: 573.4727 - val_loss: 532.2976\n",
      "Epoch 271/1000\n",
      "246/246 [==============================] - 0s - loss: 570.8239 - val_loss: 529.5917\n",
      "Epoch 272/1000\n",
      "246/246 [==============================] - 0s - loss: 568.2000 - val_loss: 526.9476\n",
      "Epoch 273/1000\n",
      "246/246 [==============================] - 0s - loss: 565.5854 - val_loss: 524.3285\n",
      "Epoch 274/1000\n",
      "246/246 [==============================] - 0s - loss: 563.0158 - val_loss: 521.7387\n",
      "Epoch 275/1000\n",
      "246/246 [==============================] - 0s - loss: 560.4924 - val_loss: 519.1834\n",
      "Epoch 276/1000\n",
      "246/246 [==============================] - 0s - loss: 557.9962 - val_loss: 516.6179\n",
      "Epoch 277/1000\n",
      "246/246 [==============================] - 0s - loss: 555.4726 - val_loss: 514.0898\n",
      "Epoch 278/1000\n",
      "246/246 [==============================] - 0s - loss: 553.0173 - val_loss: 511.6061\n",
      "Epoch 279/1000\n",
      "246/246 [==============================] - 0s - loss: 550.6033 - val_loss: 509.1267\n",
      "Epoch 280/1000\n",
      "246/246 [==============================] - 0s - loss: 548.1798 - val_loss: 506.7190\n",
      "Epoch 281/1000\n",
      "246/246 [==============================] - 0s - loss: 545.8521 - val_loss: 504.3013\n",
      "Epoch 282/1000\n",
      "246/246 [==============================] - 0s - loss: 543.4665 - val_loss: 501.9598\n",
      "Epoch 283/1000\n",
      "246/246 [==============================] - 0s - loss: 541.1617 - val_loss: 499.6695\n",
      "Epoch 284/1000\n",
      "246/246 [==============================] - 0s - loss: 538.8671 - val_loss: 497.3471\n",
      "Epoch 285/1000\n",
      "246/246 [==============================] - 0s - loss: 536.5954 - val_loss: 495.0652\n",
      "Epoch 286/1000\n",
      "246/246 [==============================] - 0s - loss: 534.3508 - val_loss: 492.8220\n",
      "Epoch 287/1000\n",
      "246/246 [==============================] - 0s - loss: 532.1615 - val_loss: 490.6022\n",
      "Epoch 288/1000\n",
      "246/246 [==============================] - 0s - loss: 529.9804 - val_loss: 488.3532\n",
      "Epoch 289/1000\n",
      "246/246 [==============================] - 0s - loss: 527.8213 - val_loss: 486.1269\n",
      "Epoch 290/1000\n",
      "246/246 [==============================] - 0s - loss: 525.6929 - val_loss: 483.9463\n",
      "Epoch 291/1000\n",
      "246/246 [==============================] - 0s - loss: 523.5434 - val_loss: 481.8464\n",
      "Epoch 292/1000\n",
      "246/246 [==============================] - 0s - loss: 521.4519 - val_loss: 479.7520\n",
      "Epoch 293/1000\n",
      "246/246 [==============================] - 0s - loss: 519.4088 - val_loss: 477.6756\n",
      "Epoch 294/1000\n",
      "246/246 [==============================] - 0s - loss: 517.3707 - val_loss: 475.6464\n",
      "Epoch 295/1000\n",
      "246/246 [==============================] - 0s - loss: 515.3551 - val_loss: 473.6070\n",
      "Epoch 296/1000\n",
      "246/246 [==============================] - 0s - loss: 513.3801 - val_loss: 471.5628\n",
      "Epoch 297/1000\n",
      "246/246 [==============================] - 0s - loss: 511.3932 - val_loss: 469.5592\n",
      "Epoch 298/1000\n",
      "246/246 [==============================] - 0s - loss: 509.4602 - val_loss: 467.6170\n",
      "Epoch 299/1000\n",
      "246/246 [==============================] - 0s - loss: 507.5382 - val_loss: 465.6695\n",
      "Epoch 300/1000\n",
      "246/246 [==============================] - 0s - loss: 505.6604 - val_loss: 463.7302\n",
      "Epoch 301/1000\n",
      "246/246 [==============================] - 0s - loss: 503.7747 - val_loss: 461.8351\n",
      "Epoch 302/1000\n",
      "246/246 [==============================] - 0s - loss: 501.9176 - val_loss: 459.9853\n",
      "Epoch 303/1000\n",
      "246/246 [==============================] - 0s - loss: 500.1143 - val_loss: 458.1773\n",
      "Epoch 304/1000\n",
      "246/246 [==============================] - 0s - loss: 498.2985 - val_loss: 456.3175\n",
      "Epoch 305/1000\n",
      "246/246 [==============================] - 0s - loss: 496.5091 - val_loss: 454.4924\n",
      "Epoch 306/1000\n",
      "246/246 [==============================] - 0s - loss: 494.7436 - val_loss: 452.7046\n",
      "Epoch 307/1000\n",
      "246/246 [==============================] - 0s - loss: 493.0161 - val_loss: 450.9193\n",
      "Epoch 308/1000\n",
      "246/246 [==============================] - 0s - loss: 491.2873 - val_loss: 449.1796\n",
      "Epoch 309/1000\n",
      "246/246 [==============================] - 0s - loss: 489.5982 - val_loss: 447.4892\n",
      "Epoch 310/1000\n",
      "246/246 [==============================] - 0s - loss: 487.9169 - val_loss: 445.8016\n",
      "Epoch 311/1000\n",
      "246/246 [==============================] - 0s - loss: 486.2699 - val_loss: 444.1028\n",
      "Epoch 312/1000\n",
      "246/246 [==============================] - 0s - loss: 484.6929 - val_loss: 442.4079\n",
      "Epoch 313/1000\n",
      "246/246 [==============================] - 0s - loss: 483.0591 - val_loss: 440.8310\n",
      "Epoch 314/1000\n",
      "246/246 [==============================] - 0s - loss: 481.4310 - val_loss: 439.2101\n",
      "Epoch 315/1000\n",
      "246/246 [==============================] - 0s - loss: 479.8609 - val_loss: 437.5977\n",
      "Epoch 316/1000\n",
      "246/246 [==============================] - 0s - loss: 478.2976 - val_loss: 436.0243\n",
      "Epoch 317/1000\n",
      "246/246 [==============================] - 0s - loss: 476.7600 - val_loss: 434.4510\n",
      "Epoch 318/1000\n",
      "246/246 [==============================] - 0s - loss: 475.2566 - val_loss: 432.8957\n",
      "Epoch 319/1000\n",
      "246/246 [==============================] - 0s - loss: 473.7573 - val_loss: 431.3892\n",
      "Epoch 320/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 472.3210 - val_loss: 429.8658\n",
      "Epoch 321/1000\n",
      "246/246 [==============================] - 0s - loss: 470.8186 - val_loss: 428.4027\n",
      "Epoch 322/1000\n",
      "246/246 [==============================] - 0s - loss: 469.3880 - val_loss: 426.9719\n",
      "Epoch 323/1000\n",
      "246/246 [==============================] - 0s - loss: 468.0477 - val_loss: 425.4968\n",
      "Epoch 324/1000\n",
      "246/246 [==============================] - 0s - loss: 466.5706 - val_loss: 424.1073\n",
      "Epoch 325/1000\n",
      "246/246 [==============================] - 0s - loss: 465.1795 - val_loss: 422.7182\n",
      "Epoch 326/1000\n",
      "246/246 [==============================] - 0s - loss: 463.8310 - val_loss: 421.3626\n",
      "Epoch 327/1000\n",
      "246/246 [==============================] - 0s - loss: 462.5008 - val_loss: 419.9825\n",
      "Epoch 328/1000\n",
      "246/246 [==============================] - 0s - loss: 461.1444 - val_loss: 418.6253\n",
      "Epoch 329/1000\n",
      "246/246 [==============================] - 0s - loss: 459.8598 - val_loss: 417.3077\n",
      "Epoch 330/1000\n",
      "246/246 [==============================] - 0s - loss: 458.5459 - val_loss: 415.9799\n",
      "Epoch 331/1000\n",
      "246/246 [==============================] - 0s - loss: 457.2551 - val_loss: 414.6461\n",
      "Epoch 332/1000\n",
      "246/246 [==============================] - 0s - loss: 455.9936 - val_loss: 413.3511\n",
      "Epoch 333/1000\n",
      "246/246 [==============================] - 0s - loss: 454.7501 - val_loss: 412.0653\n",
      "Epoch 334/1000\n",
      "246/246 [==============================] - 0s - loss: 453.5488 - val_loss: 410.7979\n",
      "Epoch 335/1000\n",
      "246/246 [==============================] - 0s - loss: 452.3364 - val_loss: 409.5786\n",
      "Epoch 336/1000\n",
      "246/246 [==============================] - 0s - loss: 451.1246 - val_loss: 408.3856\n",
      "Epoch 337/1000\n",
      "246/246 [==============================] - 0s - loss: 449.9899 - val_loss: 407.1624\n",
      "Epoch 338/1000\n",
      "246/246 [==============================] - 0s - loss: 448.7765 - val_loss: 405.9886\n",
      "Epoch 339/1000\n",
      "246/246 [==============================] - 0s - loss: 447.6519 - val_loss: 404.8623\n",
      "Epoch 340/1000\n",
      "246/246 [==============================] - 0s - loss: 446.5221 - val_loss: 403.6871\n",
      "Epoch 341/1000\n",
      "246/246 [==============================] - 0s - loss: 445.3826 - val_loss: 402.5533\n",
      "Epoch 342/1000\n",
      "246/246 [==============================] - 0s - loss: 444.3066 - val_loss: 401.4148\n",
      "Epoch 343/1000\n",
      "246/246 [==============================] - 0s - loss: 443.2088 - val_loss: 400.3484\n",
      "Epoch 344/1000\n",
      "246/246 [==============================] - 0s - loss: 442.1252 - val_loss: 399.2683\n",
      "Epoch 345/1000\n",
      "246/246 [==============================] - 0s - loss: 441.0845 - val_loss: 398.1701\n",
      "Epoch 346/1000\n",
      "246/246 [==============================] - 0s - loss: 440.0375 - val_loss: 397.1104\n",
      "Epoch 347/1000\n",
      "246/246 [==============================] - 0s - loss: 438.9927 - val_loss: 396.0739\n",
      "Epoch 348/1000\n",
      "246/246 [==============================] - 0s - loss: 438.0136 - val_loss: 395.0094\n",
      "Epoch 349/1000\n",
      "246/246 [==============================] - 0s - loss: 437.0038 - val_loss: 394.0256\n",
      "Epoch 350/1000\n",
      "246/246 [==============================] - 0s - loss: 436.0204 - val_loss: 392.9909\n",
      "Epoch 351/1000\n",
      "246/246 [==============================] - 0s - loss: 435.0614 - val_loss: 391.9764\n",
      "Epoch 352/1000\n",
      "246/246 [==============================] - 0s - loss: 434.0828 - val_loss: 391.0082\n",
      "Epoch 353/1000\n",
      "246/246 [==============================] - 0s - loss: 433.1391 - val_loss: 390.0582\n",
      "Epoch 354/1000\n",
      "246/246 [==============================] - 0s - loss: 432.2054 - val_loss: 389.1070\n",
      "Epoch 355/1000\n",
      "246/246 [==============================] - 0s - loss: 431.2956 - val_loss: 388.1660\n",
      "Epoch 356/1000\n",
      "246/246 [==============================] - 0s - loss: 430.3767 - val_loss: 387.2558\n",
      "Epoch 357/1000\n",
      "246/246 [==============================] - 0s - loss: 429.5271 - val_loss: 386.3736\n",
      "Epoch 358/1000\n",
      "246/246 [==============================] - 0s - loss: 428.6183 - val_loss: 385.4336\n",
      "Epoch 359/1000\n",
      "246/246 [==============================] - 0s - loss: 427.7444 - val_loss: 384.5158\n",
      "Epoch 360/1000\n",
      "246/246 [==============================] - 0s - loss: 426.8866 - val_loss: 383.6200\n",
      "Epoch 361/1000\n",
      "246/246 [==============================] - 0s - loss: 426.0380 - val_loss: 382.7537\n",
      "Epoch 362/1000\n",
      "246/246 [==============================] - 0s - loss: 425.2070 - val_loss: 381.9240\n",
      "Epoch 363/1000\n",
      "246/246 [==============================] - 0s - loss: 424.4130 - val_loss: 381.1237\n",
      "Epoch 364/1000\n",
      "246/246 [==============================] - 0s - loss: 423.5803 - val_loss: 380.2857\n",
      "Epoch 365/1000\n",
      "246/246 [==============================] - 0s - loss: 422.8005 - val_loss: 379.4635\n",
      "Epoch 366/1000\n",
      "246/246 [==============================] - 0s - loss: 422.0115 - val_loss: 378.6575\n",
      "Epoch 367/1000\n",
      "246/246 [==============================] - 0s - loss: 421.2402 - val_loss: 377.8943\n",
      "Epoch 368/1000\n",
      "246/246 [==============================] - 0s - loss: 420.4771 - val_loss: 377.1261\n",
      "Epoch 369/1000\n",
      "246/246 [==============================] - 0s - loss: 419.7355 - val_loss: 376.3412\n",
      "Epoch 370/1000\n",
      "246/246 [==============================] - 0s - loss: 418.9906 - val_loss: 375.5758\n",
      "Epoch 371/1000\n",
      "246/246 [==============================] - 0s - loss: 418.2734 - val_loss: 374.8460\n",
      "Epoch 372/1000\n",
      "246/246 [==============================] - 0s - loss: 417.5460 - val_loss: 374.1000\n",
      "Epoch 373/1000\n",
      "246/246 [==============================] - 0s - loss: 416.8388 - val_loss: 373.3722\n",
      "Epoch 374/1000\n",
      "246/246 [==============================] - 0s - loss: 416.1470 - val_loss: 372.6495\n",
      "Epoch 375/1000\n",
      "246/246 [==============================] - 0s - loss: 415.4708 - val_loss: 371.9127\n",
      "Epoch 376/1000\n",
      "246/246 [==============================] - 0s - loss: 414.7883 - val_loss: 371.2192\n",
      "Epoch 377/1000\n",
      "246/246 [==============================] - 0s - loss: 414.1382 - val_loss: 370.5624\n",
      "Epoch 378/1000\n",
      "246/246 [==============================] - 0s - loss: 413.4891 - val_loss: 369.9086\n",
      "Epoch 379/1000\n",
      "246/246 [==============================] - 0s - loss: 412.8345 - val_loss: 369.2160\n",
      "Epoch 380/1000\n",
      "246/246 [==============================] - 0s - loss: 412.1964 - val_loss: 368.5718\n",
      "Epoch 381/1000\n",
      "246/246 [==============================] - 0s - loss: 411.5663 - val_loss: 367.9232\n",
      "Epoch 382/1000\n",
      "246/246 [==============================] - 0s - loss: 410.9564 - val_loss: 367.2823\n",
      "Epoch 383/1000\n",
      "246/246 [==============================] - 0s - loss: 410.3368 - val_loss: 366.6695\n",
      "Epoch 384/1000\n",
      "246/246 [==============================] - 0s - loss: 409.7566 - val_loss: 366.0770\n",
      "Epoch 385/1000\n",
      "246/246 [==============================] - 0s - loss: 409.1539 - val_loss: 365.4755\n",
      "Epoch 386/1000\n",
      "246/246 [==============================] - 0s - loss: 408.5804 - val_loss: 364.8852\n",
      "Epoch 387/1000\n",
      "246/246 [==============================] - 0s - loss: 408.0079 - val_loss: 364.2883\n",
      "Epoch 388/1000\n",
      "246/246 [==============================] - 0s - loss: 407.4581 - val_loss: 363.6901\n",
      "Epoch 389/1000\n",
      "246/246 [==============================] - 0s - loss: 406.8917 - val_loss: 363.1127\n",
      "Epoch 390/1000\n",
      "246/246 [==============================] - 0s - loss: 406.3638 - val_loss: 362.5431\n",
      "Epoch 391/1000\n",
      "246/246 [==============================] - 0s - loss: 405.8208 - val_loss: 361.9890\n",
      "Epoch 392/1000\n",
      "246/246 [==============================] - 0s - loss: 405.2945 - val_loss: 361.4508\n",
      "Epoch 393/1000\n",
      "246/246 [==============================] - 0s - loss: 404.7771 - val_loss: 360.9310\n",
      "Epoch 394/1000\n",
      "246/246 [==============================] - 0s - loss: 404.2816 - val_loss: 360.3974\n",
      "Epoch 395/1000\n",
      "246/246 [==============================] - 0s - loss: 403.7741 - val_loss: 359.9197\n",
      "Epoch 396/1000\n",
      "246/246 [==============================] - 0s - loss: 403.2783 - val_loss: 359.4183\n",
      "Epoch 397/1000\n",
      "246/246 [==============================] - 0s - loss: 402.7973 - val_loss: 358.9010\n",
      "Epoch 398/1000\n",
      "246/246 [==============================] - 0s - loss: 402.3632 - val_loss: 358.4451\n",
      "Epoch 399/1000\n",
      "246/246 [==============================] - 0s - loss: 401.8493 - val_loss: 357.9400\n",
      "Epoch 400/1000\n",
      "246/246 [==============================] - 0s - loss: 401.3912 - val_loss: 357.4709\n",
      "Epoch 401/1000\n",
      "246/246 [==============================] - 0s - loss: 400.9216 - val_loss: 356.9696\n",
      "Epoch 402/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 400.4898 - val_loss: 356.4838\n",
      "Epoch 403/1000\n",
      "246/246 [==============================] - 0s - loss: 400.0424 - val_loss: 356.0064\n",
      "Epoch 404/1000\n",
      "246/246 [==============================] - 0s - loss: 399.5947 - val_loss: 355.5781\n",
      "Epoch 405/1000\n",
      "246/246 [==============================] - 0s - loss: 399.1868 - val_loss: 355.1308\n",
      "Epoch 406/1000\n",
      "246/246 [==============================] - 0s - loss: 398.7470 - val_loss: 354.7037\n",
      "Epoch 407/1000\n",
      "246/246 [==============================] - 0s - loss: 398.3357 - val_loss: 354.2903\n",
      "Epoch 408/1000\n",
      "246/246 [==============================] - 0s - loss: 397.9284 - val_loss: 353.8713\n",
      "Epoch 409/1000\n",
      "246/246 [==============================] - 0s - loss: 397.5375 - val_loss: 353.4569\n",
      "Epoch 410/1000\n",
      "246/246 [==============================] - 0s - loss: 397.1506 - val_loss: 353.0333\n",
      "Epoch 411/1000\n",
      "246/246 [==============================] - 0s - loss: 396.7536 - val_loss: 352.6259\n",
      "Epoch 412/1000\n",
      "246/246 [==============================] - 0s - loss: 396.3845 - val_loss: 352.2387\n",
      "Epoch 413/1000\n",
      "246/246 [==============================] - 0s - loss: 395.9983 - val_loss: 351.8466\n",
      "Epoch 414/1000\n",
      "246/246 [==============================] - 0s - loss: 395.6383 - val_loss: 351.4801\n",
      "Epoch 415/1000\n",
      "246/246 [==============================] - 0s - loss: 395.2775 - val_loss: 351.0925\n",
      "Epoch 416/1000\n",
      "246/246 [==============================] - 0s - loss: 394.9334 - val_loss: 350.7282\n",
      "Epoch 417/1000\n",
      "246/246 [==============================] - 0s - loss: 394.5734 - val_loss: 350.3553\n",
      "Epoch 418/1000\n",
      "246/246 [==============================] - 0s - loss: 394.2420 - val_loss: 349.9722\n",
      "Epoch 419/1000\n",
      "246/246 [==============================] - 0s - loss: 393.9021 - val_loss: 349.6069\n",
      "Epoch 420/1000\n",
      "246/246 [==============================] - 0s - loss: 393.5600 - val_loss: 349.2757\n",
      "Epoch 421/1000\n",
      "246/246 [==============================] - 0s - loss: 393.2306 - val_loss: 348.9264\n",
      "Epoch 422/1000\n",
      "246/246 [==============================] - 0s - loss: 392.9084 - val_loss: 348.5909\n",
      "Epoch 423/1000\n",
      "246/246 [==============================] - 0s - loss: 392.5932 - val_loss: 348.2702\n",
      "Epoch 424/1000\n",
      "246/246 [==============================] - 0s - loss: 392.2992 - val_loss: 347.9471\n",
      "Epoch 425/1000\n",
      "246/246 [==============================] - 0s - loss: 391.9858 - val_loss: 347.6057\n",
      "Epoch 426/1000\n",
      "246/246 [==============================] - 0s - loss: 391.7076 - val_loss: 347.2740\n",
      "Epoch 427/1000\n",
      "246/246 [==============================] - 0s - loss: 391.4039 - val_loss: 347.0129\n",
      "Epoch 428/1000\n",
      "246/246 [==============================] - 0s - loss: 391.0976 - val_loss: 346.7130\n",
      "Epoch 429/1000\n",
      "246/246 [==============================] - 0s - loss: 390.8507 - val_loss: 346.4029\n",
      "Epoch 430/1000\n",
      "246/246 [==============================] - 0s - loss: 390.5408 - val_loss: 346.1085\n",
      "Epoch 431/1000\n",
      "246/246 [==============================] - 0s - loss: 390.2619 - val_loss: 345.8481\n",
      "Epoch 432/1000\n",
      "246/246 [==============================] - 0s - loss: 390.0097 - val_loss: 345.5533\n",
      "Epoch 433/1000\n",
      "246/246 [==============================] - 0s - loss: 389.7524 - val_loss: 345.2657\n",
      "Epoch 434/1000\n",
      "246/246 [==============================] - 0s - loss: 389.4810 - val_loss: 345.0264\n",
      "Epoch 435/1000\n",
      "246/246 [==============================] - 0s - loss: 389.2171 - val_loss: 344.7784\n",
      "Epoch 436/1000\n",
      "246/246 [==============================] - 0s - loss: 388.9712 - val_loss: 344.5103\n",
      "Epoch 437/1000\n",
      "246/246 [==============================] - 0s - loss: 388.7517 - val_loss: 344.2788\n",
      "Epoch 438/1000\n",
      "246/246 [==============================] - 0s - loss: 388.4783 - val_loss: 344.0097\n",
      "Epoch 439/1000\n",
      "246/246 [==============================] - 0s - loss: 388.2399 - val_loss: 343.7424\n",
      "Epoch 440/1000\n",
      "246/246 [==============================] - 0s - loss: 388.0091 - val_loss: 343.4784\n",
      "Epoch 441/1000\n",
      "246/246 [==============================] - 0s - loss: 387.7845 - val_loss: 343.2353\n",
      "Epoch 442/1000\n",
      "246/246 [==============================] - 0s - loss: 387.6354 - val_loss: 342.9593\n",
      "Epoch 443/1000\n",
      "246/246 [==============================] - 0s - loss: 387.3587 - val_loss: 342.7602\n",
      "Epoch 444/1000\n",
      "246/246 [==============================] - 0s - loss: 387.1365 - val_loss: 342.5527\n",
      "Epoch 445/1000\n",
      "246/246 [==============================] - 0s - loss: 386.9085 - val_loss: 342.3256\n",
      "Epoch 446/1000\n",
      "246/246 [==============================] - 0s - loss: 386.7079 - val_loss: 342.0769\n",
      "Epoch 447/1000\n",
      "246/246 [==============================] - 0s - loss: 386.4966 - val_loss: 341.8690\n",
      "Epoch 448/1000\n",
      "246/246 [==============================] - 0s - loss: 386.3023 - val_loss: 341.6628\n",
      "Epoch 449/1000\n",
      "246/246 [==============================] - 0s - loss: 386.1141 - val_loss: 341.4662\n",
      "Epoch 450/1000\n",
      "246/246 [==============================] - 0s - loss: 385.9065 - val_loss: 341.2280\n",
      "Epoch 451/1000\n",
      "246/246 [==============================] - 0s - loss: 385.7480 - val_loss: 340.9891\n",
      "Epoch 452/1000\n",
      "246/246 [==============================] - 0s - loss: 385.5253 - val_loss: 340.7942\n",
      "Epoch 453/1000\n",
      "246/246 [==============================] - 0s - loss: 385.3391 - val_loss: 340.6071\n",
      "Epoch 454/1000\n",
      "246/246 [==============================] - 0s - loss: 385.1834 - val_loss: 340.4157\n",
      "Epoch 455/1000\n",
      "246/246 [==============================] - 0s - loss: 385.0109 - val_loss: 340.2704\n",
      "Epoch 456/1000\n",
      "246/246 [==============================] - 0s - loss: 384.8157 - val_loss: 340.0856\n",
      "Epoch 457/1000\n",
      "246/246 [==============================] - 0s - loss: 384.6562 - val_loss: 339.9015\n",
      "Epoch 458/1000\n",
      "246/246 [==============================] - 0s - loss: 384.4744 - val_loss: 339.7265\n",
      "Epoch 459/1000\n",
      "246/246 [==============================] - 0s - loss: 384.3171 - val_loss: 339.5510\n",
      "Epoch 460/1000\n",
      "246/246 [==============================] - 0s - loss: 384.1605 - val_loss: 339.3871\n",
      "Epoch 461/1000\n",
      "246/246 [==============================] - 0s - loss: 384.0153 - val_loss: 339.1978\n",
      "Epoch 462/1000\n",
      "246/246 [==============================] - 0s - loss: 383.8513 - val_loss: 339.0354\n",
      "Epoch 463/1000\n",
      "246/246 [==============================] - 0s - loss: 383.7312 - val_loss: 338.8620\n",
      "Epoch 464/1000\n",
      "246/246 [==============================] - 0s - loss: 383.5530 - val_loss: 338.7080\n",
      "Epoch 465/1000\n",
      "246/246 [==============================] - 0s - loss: 383.4256 - val_loss: 338.5980\n",
      "Epoch 466/1000\n",
      "246/246 [==============================] - 0s - loss: 383.2601 - val_loss: 338.4461\n",
      "Epoch 467/1000\n",
      "246/246 [==============================] - 0s - loss: 383.1113 - val_loss: 338.2835\n",
      "Epoch 468/1000\n",
      "246/246 [==============================] - 0s - loss: 382.9976 - val_loss: 338.0914\n",
      "Epoch 469/1000\n",
      "246/246 [==============================] - 0s - loss: 382.8470 - val_loss: 337.9361\n",
      "Epoch 470/1000\n",
      "246/246 [==============================] - 0s - loss: 382.7137 - val_loss: 337.8013\n",
      "Epoch 471/1000\n",
      "246/246 [==============================] - 0s - loss: 382.5847 - val_loss: 337.6691\n",
      "Epoch 472/1000\n",
      "246/246 [==============================] - 0s - loss: 382.4481 - val_loss: 337.5088\n",
      "Epoch 473/1000\n",
      "246/246 [==============================] - 0s - loss: 382.3398 - val_loss: 337.3520\n",
      "Epoch 474/1000\n",
      "246/246 [==============================] - 0s - loss: 382.2162 - val_loss: 337.2072\n",
      "Epoch 475/1000\n",
      "246/246 [==============================] - 0s - loss: 382.0845 - val_loss: 337.0833\n",
      "Epoch 476/1000\n",
      "246/246 [==============================] - 0s - loss: 381.9762 - val_loss: 336.9489\n",
      "Epoch 477/1000\n",
      "246/246 [==============================] - 0s - loss: 381.8752 - val_loss: 336.8222\n",
      "Epoch 478/1000\n",
      "246/246 [==============================] - 0s - loss: 381.7417 - val_loss: 336.7276\n",
      "Epoch 479/1000\n",
      "246/246 [==============================] - 0s - loss: 381.6397 - val_loss: 336.6363\n",
      "Epoch 480/1000\n",
      "246/246 [==============================] - 0s - loss: 381.5233 - val_loss: 336.5147\n",
      "Epoch 481/1000\n",
      "246/246 [==============================] - 0s - loss: 381.4346 - val_loss: 336.3778\n",
      "Epoch 482/1000\n",
      "246/246 [==============================] - 0s - loss: 381.3197 - val_loss: 336.2648\n",
      "Epoch 483/1000\n",
      "246/246 [==============================] - 0s - loss: 381.2237 - val_loss: 336.2011\n",
      "Epoch 484/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 381.1100 - val_loss: 336.0861\n",
      "Epoch 485/1000\n",
      "246/246 [==============================] - 0s - loss: 381.0290 - val_loss: 336.0056\n",
      "Epoch 486/1000\n",
      "246/246 [==============================] - 0s - loss: 380.9153 - val_loss: 335.8959\n",
      "Epoch 487/1000\n",
      "246/246 [==============================] - 0s - loss: 380.8191 - val_loss: 335.7712\n",
      "Epoch 488/1000\n",
      "246/246 [==============================] - 0s - loss: 380.7325 - val_loss: 335.6643\n",
      "Epoch 489/1000\n",
      "246/246 [==============================] - 0s - loss: 380.6443 - val_loss: 335.5351\n",
      "Epoch 490/1000\n",
      "246/246 [==============================] - 0s - loss: 380.5837 - val_loss: 335.4077\n",
      "Epoch 491/1000\n",
      "246/246 [==============================] - 0s - loss: 380.4839 - val_loss: 335.3112\n",
      "Epoch 492/1000\n",
      "246/246 [==============================] - 0s - loss: 380.4193 - val_loss: 335.2656\n",
      "Epoch 493/1000\n",
      "246/246 [==============================] - 0s - loss: 380.2990 - val_loss: 335.1827\n",
      "Epoch 494/1000\n",
      "246/246 [==============================] - 0s - loss: 380.2256 - val_loss: 335.0785\n",
      "Epoch 495/1000\n",
      "246/246 [==============================] - 0s - loss: 380.1485 - val_loss: 334.9897\n",
      "Epoch 496/1000\n",
      "246/246 [==============================] - 0s - loss: 380.0622 - val_loss: 334.8983\n",
      "Epoch 497/1000\n",
      "246/246 [==============================] - 0s - loss: 379.9866 - val_loss: 334.8290\n",
      "Epoch 498/1000\n",
      "246/246 [==============================] - 0s - loss: 379.9212 - val_loss: 334.7302\n",
      "Epoch 499/1000\n",
      "246/246 [==============================] - 0s - loss: 379.8316 - val_loss: 334.6656\n",
      "Epoch 500/1000\n",
      "246/246 [==============================] - 0s - loss: 379.7819 - val_loss: 334.5677\n",
      "Epoch 501/1000\n",
      "246/246 [==============================] - 0s - loss: 379.6985 - val_loss: 334.5239\n",
      "Epoch 502/1000\n",
      "246/246 [==============================] - 0s - loss: 379.6537 - val_loss: 334.4806\n",
      "Epoch 503/1000\n",
      "246/246 [==============================] - 0s - loss: 379.5724 - val_loss: 334.3808\n",
      "Epoch 504/1000\n",
      "246/246 [==============================] - 0s - loss: 379.5018 - val_loss: 334.2876\n",
      "Epoch 505/1000\n",
      "246/246 [==============================] - 0s - loss: 379.4282 - val_loss: 334.2138\n",
      "Epoch 506/1000\n",
      "246/246 [==============================] - 0s - loss: 379.3820 - val_loss: 334.1386\n",
      "Epoch 507/1000\n",
      "246/246 [==============================] - 0s - loss: 379.3248 - val_loss: 334.0501\n",
      "Epoch 508/1000\n",
      "246/246 [==============================] - 0s - loss: 379.2578 - val_loss: 333.9973\n",
      "Epoch 509/1000\n",
      "246/246 [==============================] - 0s - loss: 379.1903 - val_loss: 333.9274\n",
      "Epoch 510/1000\n",
      "246/246 [==============================] - 0s - loss: 379.1360 - val_loss: 333.8526\n",
      "Epoch 511/1000\n",
      "246/246 [==============================] - 0s - loss: 379.0966 - val_loss: 333.7838\n",
      "Epoch 512/1000\n",
      "246/246 [==============================] - 0s - loss: 379.0261 - val_loss: 333.7364\n",
      "Epoch 513/1000\n",
      "246/246 [==============================] - 0s - loss: 378.9627 - val_loss: 333.6802\n",
      "Epoch 514/1000\n",
      "246/246 [==============================] - 0s - loss: 378.9180 - val_loss: 333.6293\n",
      "Epoch 515/1000\n",
      "246/246 [==============================] - 0s - loss: 378.8628 - val_loss: 333.5595\n",
      "Epoch 516/1000\n",
      "246/246 [==============================] - 0s - loss: 378.8287 - val_loss: 333.5017\n",
      "Epoch 517/1000\n",
      "246/246 [==============================] - 0s - loss: 378.7972 - val_loss: 333.4225\n",
      "Epoch 518/1000\n",
      "246/246 [==============================] - 0s - loss: 378.7239 - val_loss: 333.3579\n",
      "Epoch 519/1000\n",
      "246/246 [==============================] - 0s - loss: 378.7095 - val_loss: 333.3523\n",
      "Epoch 520/1000\n",
      "246/246 [==============================] - 0s - loss: 378.6548 - val_loss: 333.3219\n",
      "Epoch 521/1000\n",
      "246/246 [==============================] - 0s - loss: 378.5850 - val_loss: 333.2561\n",
      "Epoch 522/1000\n",
      "246/246 [==============================] - 0s - loss: 378.5328 - val_loss: 333.1766\n",
      "Epoch 523/1000\n",
      "246/246 [==============================] - 0s - loss: 378.5036 - val_loss: 333.0961\n",
      "Epoch 524/1000\n",
      "246/246 [==============================] - 0s - loss: 378.4712 - val_loss: 333.0308\n",
      "Epoch 525/1000\n",
      "246/246 [==============================] - 0s - loss: 378.4098 - val_loss: 332.9878\n",
      "Epoch 526/1000\n",
      "246/246 [==============================] - 0s - loss: 378.3736 - val_loss: 332.9807\n",
      "Epoch 527/1000\n",
      "246/246 [==============================] - 0s - loss: 378.3590 - val_loss: 332.9702\n",
      "Epoch 528/1000\n",
      "246/246 [==============================] - 0s - loss: 378.2912 - val_loss: 332.9204\n",
      "Epoch 529/1000\n",
      "246/246 [==============================] - 0s - loss: 378.2524 - val_loss: 332.8564\n",
      "Epoch 530/1000\n",
      "246/246 [==============================] - 0s - loss: 378.2104 - val_loss: 332.7955\n",
      "Epoch 531/1000\n",
      "246/246 [==============================] - 0s - loss: 378.1788 - val_loss: 332.7385\n",
      "Epoch 532/1000\n",
      "246/246 [==============================] - 0s - loss: 378.1437 - val_loss: 332.7047\n",
      "Epoch 533/1000\n",
      "246/246 [==============================] - 0s - loss: 378.1160 - val_loss: 332.6372\n",
      "Epoch 534/1000\n",
      "246/246 [==============================] - 0s - loss: 378.1319 - val_loss: 332.5794\n",
      "Epoch 535/1000\n",
      "246/246 [==============================] - 0s - loss: 378.0610 - val_loss: 332.5711\n",
      "Epoch 536/1000\n",
      "246/246 [==============================] - 0s - loss: 378.0144 - val_loss: 332.5672\n",
      "Epoch 537/1000\n",
      "246/246 [==============================] - 0s - loss: 377.9868 - val_loss: 332.5327\n",
      "Epoch 538/1000\n",
      "246/246 [==============================] - 0s - loss: 377.9587 - val_loss: 332.5323\n",
      "Epoch 539/1000\n",
      "246/246 [==============================] - 0s - loss: 377.9288 - val_loss: 332.4919\n",
      "Epoch 540/1000\n",
      "246/246 [==============================] - 0s - loss: 377.8921 - val_loss: 332.4597\n",
      "Epoch 541/1000\n",
      "246/246 [==============================] - 0s - loss: 377.8675 - val_loss: 332.4135\n",
      "Epoch 542/1000\n",
      "246/246 [==============================] - 0s - loss: 377.8410 - val_loss: 332.3787\n",
      "Epoch 543/1000\n",
      "246/246 [==============================] - 0s - loss: 377.8199 - val_loss: 332.3478\n",
      "Epoch 544/1000\n",
      "246/246 [==============================] - 0s - loss: 377.7869 - val_loss: 332.2916\n",
      "Epoch 545/1000\n",
      "246/246 [==============================] - 0s - loss: 377.7602 - val_loss: 332.2281\n",
      "Epoch 546/1000\n",
      "246/246 [==============================] - 0s - loss: 377.7405 - val_loss: 332.1840\n",
      "Epoch 547/1000\n",
      "246/246 [==============================] - 0s - loss: 377.7271 - val_loss: 332.1678\n",
      "Epoch 548/1000\n",
      "246/246 [==============================] - 0s - loss: 377.6904 - val_loss: 332.1356\n",
      "Epoch 549/1000\n",
      "246/246 [==============================] - 0s - loss: 377.6657 - val_loss: 332.1076\n",
      "Epoch 550/1000\n",
      "246/246 [==============================] - 0s - loss: 377.6477 - val_loss: 332.0946\n",
      "Epoch 551/1000\n",
      "246/246 [==============================] - 0s - loss: 377.6333 - val_loss: 332.0442\n",
      "Epoch 552/1000\n",
      "246/246 [==============================] - 0s - loss: 377.6136 - val_loss: 332.0585\n",
      "Epoch 553/1000\n",
      "246/246 [==============================] - 0s - loss: 377.5761 - val_loss: 332.0453\n",
      "Epoch 554/1000\n",
      "246/246 [==============================] - 0s - loss: 377.5518 - val_loss: 332.0067\n",
      "Epoch 555/1000\n",
      "246/246 [==============================] - 0s - loss: 377.5375 - val_loss: 331.9677\n",
      "Epoch 556/1000\n",
      "246/246 [==============================] - 0s - loss: 377.5098 - val_loss: 331.9422\n",
      "Epoch 557/1000\n",
      "246/246 [==============================] - 0s - loss: 377.5115 - val_loss: 331.9468\n",
      "Epoch 558/1000\n",
      "246/246 [==============================] - 0s - loss: 377.4776 - val_loss: 331.9359\n",
      "Epoch 559/1000\n",
      "246/246 [==============================] - 0s - loss: 377.4679 - val_loss: 331.8858\n",
      "Epoch 560/1000\n",
      "246/246 [==============================] - 0s - loss: 377.4425 - val_loss: 331.8671\n",
      "Epoch 561/1000\n",
      "246/246 [==============================] - 0s - loss: 377.4195 - val_loss: 331.8304\n",
      "Epoch 562/1000\n",
      "246/246 [==============================] - 0s - loss: 377.4252 - val_loss: 331.7905\n",
      "Epoch 563/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3943 - val_loss: 331.7902\n",
      "Epoch 564/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3700 - val_loss: 331.7733\n",
      "Epoch 565/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3524 - val_loss: 331.7570\n",
      "Epoch 566/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 377.3616 - val_loss: 331.7225\n",
      "Epoch 567/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3233 - val_loss: 331.7037\n",
      "Epoch 568/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3067 - val_loss: 331.6968\n",
      "Epoch 569/1000\n",
      "246/246 [==============================] - 0s - loss: 377.3183 - val_loss: 331.6994\n",
      "Epoch 570/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2831 - val_loss: 331.6655\n",
      "Epoch 571/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2647 - val_loss: 331.6494\n",
      "Epoch 572/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2554 - val_loss: 331.6195\n",
      "Epoch 573/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2485 - val_loss: 331.5872\n",
      "Epoch 574/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2692 - val_loss: 331.6004\n",
      "Epoch 575/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2630 - val_loss: 331.6088\n",
      "Epoch 576/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2075 - val_loss: 331.5590\n",
      "Epoch 577/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2400 - val_loss: 331.4965\n",
      "Epoch 578/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1897 - val_loss: 331.4741\n",
      "Epoch 579/1000\n",
      "246/246 [==============================] - 0s - loss: 377.2029 - val_loss: 331.5106\n",
      "Epoch 580/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1583 - val_loss: 331.5021\n",
      "Epoch 581/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1574 - val_loss: 331.4945\n",
      "Epoch 582/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1391 - val_loss: 331.4710\n",
      "Epoch 583/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1316 - val_loss: 331.4537\n",
      "Epoch 584/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1222 - val_loss: 331.4321\n",
      "Epoch 585/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1174 - val_loss: 331.4189\n",
      "Epoch 586/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1080 - val_loss: 331.4202\n",
      "Epoch 587/1000\n",
      "246/246 [==============================] - 0s - loss: 377.1114 - val_loss: 331.3952\n",
      "Epoch 588/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0853 - val_loss: 331.3914\n",
      "Epoch 589/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0726 - val_loss: 331.3911\n",
      "Epoch 590/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0650 - val_loss: 331.3766\n",
      "Epoch 591/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0624 - val_loss: 331.3604\n",
      "Epoch 592/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0796 - val_loss: 331.3281\n",
      "Epoch 593/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0550 - val_loss: 331.3377\n",
      "Epoch 594/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0364 - val_loss: 331.3512\n",
      "Epoch 595/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0383 - val_loss: 331.3284\n",
      "Epoch 596/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0219 - val_loss: 331.3165\n",
      "Epoch 597/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0201 - val_loss: 331.3006\n",
      "Epoch 598/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0280 - val_loss: 331.3200\n",
      "Epoch 599/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0176 - val_loss: 331.3302\n",
      "Epoch 600/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0251 - val_loss: 331.2734\n",
      "Epoch 601/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9957 - val_loss: 331.2632\n",
      "Epoch 602/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9808 - val_loss: 331.2370\n",
      "Epoch 603/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0039 - val_loss: 331.1922\n",
      "Epoch 604/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9744 - val_loss: 331.1862\n",
      "Epoch 605/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0085 - val_loss: 331.2203\n",
      "Epoch 606/1000\n",
      "246/246 [==============================] - 0s - loss: 377.0433 - val_loss: 331.1703\n",
      "Epoch 607/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9557 - val_loss: 331.1761\n",
      "Epoch 608/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9591 - val_loss: 331.1816\n",
      "Epoch 609/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9462 - val_loss: 331.1891\n",
      "Epoch 610/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9627 - val_loss: 331.1844\n",
      "Epoch 611/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9501 - val_loss: 331.2078\n",
      "Epoch 612/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9315 - val_loss: 331.2009\n",
      "Epoch 613/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9354 - val_loss: 331.2137\n",
      "Epoch 614/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9267 - val_loss: 331.2067\n",
      "Epoch 615/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9307 - val_loss: 331.1794\n",
      "Epoch 616/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9255 - val_loss: 331.1537\n",
      "Epoch 617/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9121 - val_loss: 331.1429\n",
      "Epoch 618/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9238 - val_loss: 331.1248\n",
      "Epoch 619/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9088 - val_loss: 331.1325\n",
      "Epoch 620/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9070 - val_loss: 331.1273\n",
      "Epoch 621/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9094 - val_loss: 331.1073\n",
      "Epoch 622/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8972 - val_loss: 331.0897\n",
      "Epoch 623/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9018 - val_loss: 331.1060\n",
      "Epoch 624/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8874 - val_loss: 331.0876\n",
      "Epoch 625/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9058 - val_loss: 331.0651\n",
      "Epoch 626/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9001 - val_loss: 331.0536\n",
      "Epoch 627/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8768 - val_loss: 331.0689\n",
      "Epoch 628/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8708 - val_loss: 331.0821\n",
      "Epoch 629/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8875 - val_loss: 331.0986\n",
      "Epoch 630/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8725 - val_loss: 331.0939\n",
      "Epoch 631/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8748 - val_loss: 331.0764\n",
      "Epoch 632/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9022 - val_loss: 331.0982\n",
      "Epoch 633/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9179 - val_loss: 331.0422\n",
      "Epoch 634/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8659 - val_loss: 331.0294\n",
      "Epoch 635/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8642 - val_loss: 331.0331\n",
      "Epoch 636/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8725 - val_loss: 331.0195\n",
      "Epoch 637/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8629 - val_loss: 331.0180\n",
      "Epoch 638/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8566 - val_loss: 331.0014\n",
      "Epoch 639/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8555 - val_loss: 331.0058\n",
      "Epoch 640/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8638 - val_loss: 331.0049\n",
      "Epoch 641/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8662 - val_loss: 331.0349\n",
      "Epoch 642/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9006 - val_loss: 331.0565\n",
      "Epoch 643/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8561 - val_loss: 331.0302\n",
      "Epoch 644/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8411 - val_loss: 331.0312\n",
      "Epoch 645/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8470 - val_loss: 331.0133\n",
      "Epoch 646/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8446 - val_loss: 330.9869\n",
      "Epoch 647/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8688 - val_loss: 330.9664\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 376.8381 - val_loss: 330.9835\n",
      "Epoch 649/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8474 - val_loss: 331.0100\n",
      "Epoch 650/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8575 - val_loss: 330.9809\n",
      "Epoch 651/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8427 - val_loss: 330.9997\n",
      "Epoch 652/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8374 - val_loss: 331.0021\n",
      "Epoch 653/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8339 - val_loss: 330.9989\n",
      "Epoch 654/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8262 - val_loss: 330.9806\n",
      "Epoch 655/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8304 - val_loss: 330.9553\n",
      "Epoch 656/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8439 - val_loss: 330.9588\n",
      "Epoch 657/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8386 - val_loss: 330.9362\n",
      "Epoch 658/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8314 - val_loss: 330.9349\n",
      "Epoch 659/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8294 - val_loss: 330.9463\n",
      "Epoch 660/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8342 - val_loss: 330.9679\n",
      "Epoch 661/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8398 - val_loss: 330.9753\n",
      "Epoch 662/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8199 - val_loss: 330.9546\n",
      "Epoch 663/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8241 - val_loss: 330.9575\n",
      "Epoch 664/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8384 - val_loss: 330.9419\n",
      "Epoch 665/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8127 - val_loss: 330.9398\n",
      "Epoch 666/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8333 - val_loss: 330.9499\n",
      "Epoch 667/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8233 - val_loss: 330.9479\n",
      "Epoch 668/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8217 - val_loss: 330.9460\n",
      "Epoch 669/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8094 - val_loss: 330.9245\n",
      "Epoch 670/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8373 - val_loss: 330.9056\n",
      "Epoch 671/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8201 - val_loss: 330.9130\n",
      "Epoch 672/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8106 - val_loss: 330.9155\n",
      "Epoch 673/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8152 - val_loss: 330.9094\n",
      "Epoch 674/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8117 - val_loss: 330.9107\n",
      "Epoch 675/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8181 - val_loss: 330.9065\n",
      "Epoch 676/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8071 - val_loss: 330.9248\n",
      "Epoch 677/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8293 - val_loss: 330.9142\n",
      "Epoch 678/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8126 - val_loss: 330.9446\n",
      "Epoch 679/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8000 - val_loss: 330.9561\n",
      "Epoch 680/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8414 - val_loss: 330.9434\n",
      "Epoch 681/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8053 - val_loss: 330.9495\n",
      "Epoch 682/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8135 - val_loss: 330.9496\n",
      "Epoch 683/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8110 - val_loss: 330.9669\n",
      "Epoch 684/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8141 - val_loss: 330.9419\n",
      "Epoch 685/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8006 - val_loss: 330.9411\n",
      "Epoch 686/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8321 - val_loss: 330.9117\n",
      "Epoch 687/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8331 - val_loss: 330.9409\n",
      "Epoch 688/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8123 - val_loss: 330.9335\n",
      "Epoch 689/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8455 - val_loss: 330.8881\n",
      "Epoch 690/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8302 - val_loss: 330.9264\n",
      "Epoch 691/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8145 - val_loss: 330.9042\n",
      "Epoch 692/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8059 - val_loss: 330.9048\n",
      "Epoch 693/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8210 - val_loss: 330.9121\n",
      "Epoch 694/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8407 - val_loss: 330.9306\n",
      "Epoch 695/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8242 - val_loss: 330.8985\n",
      "Epoch 696/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8257 - val_loss: 330.8732\n",
      "Epoch 697/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8189 - val_loss: 330.8845\n",
      "Epoch 698/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7957 - val_loss: 330.8872\n",
      "Epoch 699/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8209 - val_loss: 330.9069\n",
      "Epoch 700/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7987 - val_loss: 330.9018\n",
      "Epoch 701/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7997 - val_loss: 330.8901\n",
      "Epoch 702/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8022 - val_loss: 330.8824\n",
      "Epoch 703/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8131 - val_loss: 330.8705\n",
      "Epoch 704/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7944 - val_loss: 330.8956\n",
      "Epoch 705/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7994 - val_loss: 330.9242\n",
      "Epoch 706/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7946 - val_loss: 330.9239\n",
      "Epoch 707/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7955 - val_loss: 330.9151\n",
      "Epoch 708/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7970 - val_loss: 330.9063\n",
      "Epoch 709/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8016 - val_loss: 330.8964\n",
      "Epoch 710/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8183 - val_loss: 330.8847\n",
      "Epoch 711/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8137 - val_loss: 330.9157\n",
      "Epoch 712/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8039 - val_loss: 330.9099\n",
      "Epoch 713/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7950 - val_loss: 330.8976\n",
      "Epoch 714/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7933 - val_loss: 330.8848\n",
      "Epoch 715/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8040 - val_loss: 330.8636\n",
      "Epoch 716/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7928 - val_loss: 330.8723\n",
      "Epoch 717/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8135 - val_loss: 330.8916\n",
      "Epoch 718/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8077 - val_loss: 330.9108\n",
      "Epoch 719/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7957 - val_loss: 330.8950\n",
      "Epoch 720/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8181 - val_loss: 330.8687\n",
      "Epoch 721/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8336 - val_loss: 330.8895\n",
      "Epoch 722/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7966 - val_loss: 330.8721\n",
      "Epoch 723/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8163 - val_loss: 330.8538\n",
      "Epoch 724/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7985 - val_loss: 330.8589\n",
      "Epoch 725/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8028 - val_loss: 330.8604\n",
      "Epoch 726/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7912 - val_loss: 330.8661\n",
      "Epoch 727/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7941 - val_loss: 330.8846\n",
      "Epoch 728/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7986 - val_loss: 330.8919\n",
      "Epoch 729/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8095 - val_loss: 330.8758\n",
      "Epoch 730/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 376.7937 - val_loss: 330.8878\n",
      "Epoch 731/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7923 - val_loss: 330.8865\n",
      "Epoch 732/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8221 - val_loss: 330.9018\n",
      "Epoch 733/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7890 - val_loss: 330.8867\n",
      "Epoch 734/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8122 - val_loss: 330.8521\n",
      "Epoch 735/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7982 - val_loss: 330.8665\n",
      "Epoch 736/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7895 - val_loss: 330.8724\n",
      "Epoch 737/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8286 - val_loss: 330.9042\n",
      "Epoch 738/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8408 - val_loss: 330.8704\n",
      "Epoch 739/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7948 - val_loss: 330.8677\n",
      "Epoch 740/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8819 - val_loss: 330.9023\n",
      "Epoch 741/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7945 - val_loss: 330.8840\n",
      "Epoch 742/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7894 - val_loss: 330.8738\n",
      "Epoch 743/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7924 - val_loss: 330.8640\n",
      "Epoch 744/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7964 - val_loss: 330.8674\n",
      "Epoch 745/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8230 - val_loss: 330.8648\n",
      "Epoch 746/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7981 - val_loss: 330.8431\n",
      "Epoch 747/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8337 - val_loss: 330.8054\n",
      "Epoch 748/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8151 - val_loss: 330.8387\n",
      "Epoch 749/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7938 - val_loss: 330.8415\n",
      "Epoch 750/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7930 - val_loss: 330.8570\n",
      "Epoch 751/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8044 - val_loss: 330.8561\n",
      "Epoch 752/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7931 - val_loss: 330.8718\n",
      "Epoch 753/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8018 - val_loss: 330.8643\n",
      "Epoch 754/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8111 - val_loss: 330.8692\n",
      "Epoch 755/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7901 - val_loss: 330.8873\n",
      "Epoch 756/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7938 - val_loss: 330.8993\n",
      "Epoch 757/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8114 - val_loss: 330.9177\n",
      "Epoch 758/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7974 - val_loss: 330.8912\n",
      "Epoch 759/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7915 - val_loss: 330.8876\n",
      "Epoch 760/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7982 - val_loss: 330.8780\n",
      "Epoch 761/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7953 - val_loss: 330.8760\n",
      "Epoch 762/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7988 - val_loss: 330.8543\n",
      "Epoch 763/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8166 - val_loss: 330.8788\n",
      "Epoch 764/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7976 - val_loss: 330.8923\n",
      "Epoch 765/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7880 - val_loss: 330.8781\n",
      "Epoch 766/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8238 - val_loss: 330.8851\n",
      "Epoch 767/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7897 - val_loss: 330.8523\n",
      "Epoch 768/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7956 - val_loss: 330.8247\n",
      "Epoch 769/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8296 - val_loss: 330.8462\n",
      "Epoch 770/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7880 - val_loss: 330.8449\n",
      "Epoch 771/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8047 - val_loss: 330.8321\n",
      "Epoch 772/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7913 - val_loss: 330.8539\n",
      "Epoch 773/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8000 - val_loss: 330.8567\n",
      "Epoch 774/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7931 - val_loss: 330.8616\n",
      "Epoch 775/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8206 - val_loss: 330.8987\n",
      "Epoch 776/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7881 - val_loss: 330.8880\n",
      "Epoch 777/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7990 - val_loss: 330.8674\n",
      "Epoch 778/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7906 - val_loss: 330.8625\n",
      "Epoch 779/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7883 - val_loss: 330.8499\n",
      "Epoch 780/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7877 - val_loss: 330.8480\n",
      "Epoch 781/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7845 - val_loss: 330.8405\n",
      "Epoch 782/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7888 - val_loss: 330.8295\n",
      "Epoch 783/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7918 - val_loss: 330.8277\n",
      "Epoch 784/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7909 - val_loss: 330.8455\n",
      "Epoch 785/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7976 - val_loss: 330.8519\n",
      "Epoch 786/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8111 - val_loss: 330.8417\n",
      "Epoch 787/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8013 - val_loss: 330.8674\n",
      "Epoch 788/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8208 - val_loss: 330.8898\n",
      "Epoch 789/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8021 - val_loss: 330.8696\n",
      "Epoch 790/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8068 - val_loss: 330.8868\n",
      "Epoch 791/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8114 - val_loss: 330.8918\n",
      "Epoch 792/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7987 - val_loss: 330.8743\n",
      "Epoch 793/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7885 - val_loss: 330.8578\n",
      "Epoch 794/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7945 - val_loss: 330.8274\n",
      "Epoch 795/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7941 - val_loss: 330.8196\n",
      "Epoch 796/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7939 - val_loss: 330.8178\n",
      "Epoch 797/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8070 - val_loss: 330.8159\n",
      "Epoch 798/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7888 - val_loss: 330.8396\n",
      "Epoch 799/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8086 - val_loss: 330.8809\n",
      "Epoch 800/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8635 - val_loss: 330.9204\n",
      "Epoch 801/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7879 - val_loss: 330.9028\n",
      "Epoch 802/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7967 - val_loss: 330.8820\n",
      "Epoch 803/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7913 - val_loss: 330.8728\n",
      "Epoch 804/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8955 - val_loss: 330.8124\n",
      "Epoch 805/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8129 - val_loss: 330.8052\n",
      "Epoch 806/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7911 - val_loss: 330.8228\n",
      "Epoch 807/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7899 - val_loss: 330.8434\n",
      "Epoch 808/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8014 - val_loss: 330.8608\n",
      "Epoch 809/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8064 - val_loss: 330.8984\n",
      "Epoch 810/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8233 - val_loss: 330.9279\n",
      "Epoch 811/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8105 - val_loss: 330.8927\n",
      "Epoch 812/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 376.8038 - val_loss: 330.9017\n",
      "Epoch 813/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7920 - val_loss: 330.8912\n",
      "Epoch 814/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7918 - val_loss: 330.8799\n",
      "Epoch 815/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7893 - val_loss: 330.8758\n",
      "Epoch 816/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8043 - val_loss: 330.8560\n",
      "Epoch 817/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7920 - val_loss: 330.8437\n",
      "Epoch 818/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7883 - val_loss: 330.8482\n",
      "Epoch 819/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7872 - val_loss: 330.8486\n",
      "Epoch 820/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8025 - val_loss: 330.8763\n",
      "Epoch 821/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8029 - val_loss: 330.8501\n",
      "Epoch 822/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8038 - val_loss: 330.8502\n",
      "Epoch 823/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8373 - val_loss: 330.8770\n",
      "Epoch 824/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8066 - val_loss: 330.8791\n",
      "Epoch 825/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7972 - val_loss: 330.8809\n",
      "Epoch 826/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7910 - val_loss: 330.8509\n",
      "Epoch 827/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8221 - val_loss: 330.8067\n",
      "Epoch 828/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8410 - val_loss: 330.7896\n",
      "Epoch 829/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7937 - val_loss: 330.8197\n",
      "Epoch 830/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7846 - val_loss: 330.8440\n",
      "Epoch 831/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7844 - val_loss: 330.8650\n",
      "Epoch 832/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8024 - val_loss: 330.9122\n",
      "Epoch 833/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7898 - val_loss: 330.9030\n",
      "Epoch 834/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7956 - val_loss: 330.9043\n",
      "Epoch 835/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7917 - val_loss: 330.8987\n",
      "Epoch 836/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8026 - val_loss: 330.8549\n",
      "Epoch 837/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8019 - val_loss: 330.8277\n",
      "Epoch 838/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8105 - val_loss: 330.8107\n",
      "Epoch 839/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8654 - val_loss: 330.8535\n",
      "Epoch 840/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8331 - val_loss: 330.8777\n",
      "Epoch 841/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7976 - val_loss: 330.8487\n",
      "Epoch 842/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7876 - val_loss: 330.8445\n",
      "Epoch 843/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7888 - val_loss: 330.8282\n",
      "Epoch 844/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7879 - val_loss: 330.8263\n",
      "Epoch 845/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8081 - val_loss: 330.8244\n",
      "Epoch 846/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8459 - val_loss: 330.8691\n",
      "Epoch 847/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7956 - val_loss: 330.8654\n",
      "Epoch 848/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7921 - val_loss: 330.8633\n",
      "Epoch 849/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8032 - val_loss: 330.8499\n",
      "Epoch 850/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8092 - val_loss: 330.8756\n",
      "Epoch 851/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8351 - val_loss: 330.8425\n",
      "Epoch 852/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8273 - val_loss: 330.8763\n",
      "Epoch 853/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7978 - val_loss: 330.8777\n",
      "Epoch 854/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7871 - val_loss: 330.8691\n",
      "Epoch 855/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8043 - val_loss: 330.8828\n",
      "Epoch 856/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7978 - val_loss: 330.8670\n",
      "Epoch 857/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7958 - val_loss: 330.8418\n",
      "Epoch 858/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8028 - val_loss: 330.8259\n",
      "Epoch 859/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7890 - val_loss: 330.8310\n",
      "Epoch 860/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7914 - val_loss: 330.8407\n",
      "Epoch 861/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7935 - val_loss: 330.8602\n",
      "Epoch 862/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7868 - val_loss: 330.8595\n",
      "Epoch 863/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8004 - val_loss: 330.8504\n",
      "Epoch 864/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7887 - val_loss: 330.8670\n",
      "Epoch 865/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7983 - val_loss: 330.8553\n",
      "Epoch 866/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7859 - val_loss: 330.8649\n",
      "Epoch 867/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8041 - val_loss: 330.8758\n",
      "Epoch 868/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8270 - val_loss: 330.8954\n",
      "Epoch 869/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8271 - val_loss: 330.8590\n",
      "Epoch 870/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7850 - val_loss: 330.8652\n",
      "Epoch 871/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8213 - val_loss: 330.8463\n",
      "Epoch 872/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8040 - val_loss: 330.8735\n",
      "Epoch 873/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7885 - val_loss: 330.8794\n",
      "Epoch 874/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8178 - val_loss: 330.8688\n",
      "Epoch 875/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8006 - val_loss: 330.8570\n",
      "Epoch 876/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7913 - val_loss: 330.8655\n",
      "Epoch 877/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7907 - val_loss: 330.8844\n",
      "Epoch 878/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8150 - val_loss: 330.8991\n",
      "Epoch 879/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8062 - val_loss: 330.8670\n",
      "Epoch 880/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7875 - val_loss: 330.8564\n",
      "Epoch 881/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7903 - val_loss: 330.8634\n",
      "Epoch 882/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7959 - val_loss: 330.8534\n",
      "Epoch 883/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8182 - val_loss: 330.8421\n",
      "Epoch 884/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7947 - val_loss: 330.8557\n",
      "Epoch 885/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7863 - val_loss: 330.8653\n",
      "Epoch 886/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8018 - val_loss: 330.8866\n",
      "Epoch 887/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8657 - val_loss: 330.9052\n",
      "Epoch 888/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8265 - val_loss: 330.9014\n",
      "Epoch 889/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7926 - val_loss: 330.8537\n",
      "Epoch 890/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7977 - val_loss: 330.8303\n",
      "Epoch 891/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7904 - val_loss: 330.8181\n",
      "Epoch 892/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7885 - val_loss: 330.8286\n",
      "Epoch 893/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8018 - val_loss: 330.8186\n",
      "Epoch 894/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 376.8042 - val_loss: 330.8242\n",
      "Epoch 895/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8028 - val_loss: 330.8340\n",
      "Epoch 896/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8658 - val_loss: 330.8747\n",
      "Epoch 897/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8014 - val_loss: 330.8660\n",
      "Epoch 898/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8224 - val_loss: 330.9033\n",
      "Epoch 899/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7940 - val_loss: 330.8898\n",
      "Epoch 900/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8124 - val_loss: 330.8867\n",
      "Epoch 901/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7912 - val_loss: 330.8740\n",
      "Epoch 902/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7929 - val_loss: 330.8809\n",
      "Epoch 903/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7975 - val_loss: 330.8635\n",
      "Epoch 904/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8008 - val_loss: 330.8651\n",
      "Epoch 905/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8404 - val_loss: 330.8736\n",
      "Epoch 906/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8116 - val_loss: 330.8386\n",
      "Epoch 907/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8232 - val_loss: 330.8366\n",
      "Epoch 908/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8398 - val_loss: 330.8776\n",
      "Epoch 909/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7861 - val_loss: 330.8646\n",
      "Epoch 910/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7876 - val_loss: 330.8614\n",
      "Epoch 911/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8032 - val_loss: 330.8426\n",
      "Epoch 912/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7872 - val_loss: 330.8375\n",
      "Epoch 913/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8141 - val_loss: 330.8633\n",
      "Epoch 914/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8268 - val_loss: 330.8392\n",
      "Epoch 915/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7934 - val_loss: 330.8290\n",
      "Epoch 916/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7916 - val_loss: 330.8382\n",
      "Epoch 917/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7981 - val_loss: 330.8478\n",
      "Epoch 918/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7960 - val_loss: 330.8466\n",
      "Epoch 919/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8598 - val_loss: 330.8888\n",
      "Epoch 920/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8075 - val_loss: 330.8554\n",
      "Epoch 921/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7954 - val_loss: 330.8512\n",
      "Epoch 922/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8067 - val_loss: 330.8274\n",
      "Epoch 923/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8051 - val_loss: 330.8661\n",
      "Epoch 924/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8338 - val_loss: 330.8382\n",
      "Epoch 925/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9167 - val_loss: 330.8928\n",
      "Epoch 926/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8021 - val_loss: 330.8717\n",
      "Epoch 927/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8119 - val_loss: 330.8480\n",
      "Epoch 928/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8054 - val_loss: 330.8565\n",
      "Epoch 929/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8055 - val_loss: 330.8431\n",
      "Epoch 930/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7920 - val_loss: 330.8436\n",
      "Epoch 931/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7998 - val_loss: 330.8762\n",
      "Epoch 932/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7997 - val_loss: 330.8793\n",
      "Epoch 933/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7897 - val_loss: 330.8769\n",
      "Epoch 934/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8036 - val_loss: 330.8832\n",
      "Epoch 935/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8055 - val_loss: 330.8475\n",
      "Epoch 936/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7999 - val_loss: 330.8426\n",
      "Epoch 937/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7946 - val_loss: 330.8371\n",
      "Epoch 938/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8134 - val_loss: 330.8238\n",
      "Epoch 939/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8025 - val_loss: 330.8284\n",
      "Epoch 940/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7852 - val_loss: 330.8474\n",
      "Epoch 941/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8224 - val_loss: 330.9090\n",
      "Epoch 942/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7913 - val_loss: 330.9074\n",
      "Epoch 943/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7904 - val_loss: 330.8856\n",
      "Epoch 944/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8323 - val_loss: 330.8521\n",
      "Epoch 945/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7861 - val_loss: 330.8563\n",
      "Epoch 946/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7941 - val_loss: 330.8608\n",
      "Epoch 947/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7938 - val_loss: 330.8477\n",
      "Epoch 948/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8089 - val_loss: 330.8717\n",
      "Epoch 949/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7895 - val_loss: 330.8668\n",
      "Epoch 950/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7959 - val_loss: 330.8649\n",
      "Epoch 951/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7853 - val_loss: 330.8558\n",
      "Epoch 952/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7850 - val_loss: 330.8452\n",
      "Epoch 953/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7905 - val_loss: 330.8338\n",
      "Epoch 954/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7952 - val_loss: 330.8262\n",
      "Epoch 955/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8352 - val_loss: 330.7999\n",
      "Epoch 956/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8244 - val_loss: 330.8407\n",
      "Epoch 957/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7873 - val_loss: 330.8491\n",
      "Epoch 958/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8149 - val_loss: 330.8414\n",
      "Epoch 959/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8108 - val_loss: 330.8769\n",
      "Epoch 960/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7907 - val_loss: 330.8782\n",
      "Epoch 961/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7869 - val_loss: 330.8688\n",
      "Epoch 962/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8285 - val_loss: 330.8847\n",
      "Epoch 963/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7996 - val_loss: 330.8476\n",
      "Epoch 964/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8216 - val_loss: 330.8191\n",
      "Epoch 965/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8074 - val_loss: 330.8271\n",
      "Epoch 966/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7870 - val_loss: 330.8540\n",
      "Epoch 967/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7946 - val_loss: 330.8520\n",
      "Epoch 968/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8511 - val_loss: 330.8369\n",
      "Epoch 969/1000\n",
      "246/246 [==============================] - 0s - loss: 376.9332 - val_loss: 330.9334\n",
      "Epoch 970/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8089 - val_loss: 330.8978\n",
      "Epoch 971/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7987 - val_loss: 330.9171\n",
      "Epoch 972/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7969 - val_loss: 330.8971\n",
      "Epoch 973/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8531 - val_loss: 330.8493\n",
      "Epoch 974/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8204 - val_loss: 330.8528\n",
      "Epoch 975/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8135 - val_loss: 330.8743\n",
      "Epoch 976/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s - loss: 376.7986 - val_loss: 330.8481\n",
      "Epoch 977/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8061 - val_loss: 330.8260\n",
      "Epoch 978/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7909 - val_loss: 330.8417\n",
      "Epoch 979/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8077 - val_loss: 330.8554\n",
      "Epoch 980/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7960 - val_loss: 330.8645\n",
      "Epoch 981/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7908 - val_loss: 330.8341\n",
      "Epoch 982/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7926 - val_loss: 330.8258\n",
      "Epoch 983/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8385 - val_loss: 330.8516\n",
      "Epoch 984/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7913 - val_loss: 330.8426\n",
      "Epoch 985/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8010 - val_loss: 330.8222\n",
      "Epoch 986/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8257 - val_loss: 330.8413\n",
      "Epoch 987/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7861 - val_loss: 330.8257\n",
      "Epoch 988/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8105 - val_loss: 330.8331\n",
      "Epoch 989/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8020 - val_loss: 330.8149\n",
      "Epoch 990/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7907 - val_loss: 330.8350\n",
      "Epoch 991/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8033 - val_loss: 330.8362\n",
      "Epoch 992/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7952 - val_loss: 330.8497\n",
      "Epoch 993/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7961 - val_loss: 330.8478\n",
      "Epoch 994/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7973 - val_loss: 330.8637\n",
      "Epoch 995/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8004 - val_loss: 330.8594\n",
      "Epoch 996/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7968 - val_loss: 330.8471\n",
      "Epoch 997/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7952 - val_loss: 330.8508\n",
      "Epoch 998/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8025 - val_loss: 330.8468\n",
      "Epoch 999/1000\n",
      "246/246 [==============================] - 0s - loss: 376.7919 - val_loss: 330.8524\n",
      "Epoch 1000/1000\n",
      "246/246 [==============================] - 0s - loss: 376.8081 - val_loss: 330.8820\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGHCAYAAACnPchFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMwy7/8B9v23goizWuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bba55f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "loss_fuc = 'mean_squared_error'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(output_dim))\n",
    "model.compile(loss=loss_fuc, optimizer='adam')\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, epochs=num_epochs, batch_size=64, validation_data=(x_test,y_test))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "# Make Predict Input Array\n",
    "\n",
    "df_test = pd.read_csv('./create_input_array/phase1_testing_vol_route_weather_joined_table.csv')\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "df_test['timeofday'] = df_test.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# Select 6am-8am and 3pm-5pm\n",
    "sel_rows_test = df_test[ lambda r : ((r.timeofday>= 6) & (r.timeofday<8)) | ((r.timeofday>=15) & (r.timeofday<17))]\n",
    "\n",
    "sel_rows_test = sel_rows_test[ using_cols ]\n",
    "sel_rows_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/Kin/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-58ede4c23701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0msel_rows_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# get numpy array from panda dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_phase1_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel_rows_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Kin/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0;31m# delete from the caches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Kin/anaconda/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m         \"\"\"\n\u001b[0;32m-> 3349\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Kin/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "del sel_rows_test['date']\n",
    "# get numpy array from panda dataframe\n",
    "test_phase1_arr = sel_rows_test.values\n",
    "\n",
    "\n",
    "test_phase1_arr_w_station = difference(test_phase1_arr, 1)\n",
    "\n",
    "# scale feature array to range -1 to 1\n",
    "test_phase1_arr_scaled = scaler.transform(test_phase1_arr_w_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 13)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phase1_arr_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_phase1_arr_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put into the model to get the prediction\n",
    "te\n",
    "for i in range(len(test_phase1_arr_scaled)):\n",
    "    test_phase1_input = []\n",
    "    test_phase1_input.append(test_phase1_arr_scaled[i:i+6])\n",
    "    for t in range(6):\n",
    "        model.predict(test_phase1_seqs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subsequence from the time series \n",
    "test_phase1_seqs = []\n",
    "test_phase1_nSeg = test_phase1_arr_scaled.shape[0]//12 # each segment holds 4hr data (12 datapoints, 20min each)\n",
    "for segment in range(test_phase1_nSeg):\n",
    "    for t in range(6):\n",
    "        startIdx = segment*12 + t\n",
    "        test_phase1_seqs.append(test_phase1_arr_scaled[startIdx: startIdx+7])\n",
    "test_phase1_seqs = np.stack(test_phase1_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 7, 13)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phase1_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_arr = []\n",
    "for i in range(6):\n",
    "    model.predict(test_phase1_seqs)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
