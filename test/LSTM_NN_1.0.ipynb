{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 22-5-2017\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note:\n",
    "# Because of the 十一黃金周. sth is strange with (2,0,'tot') and others vol from 1 Oct 00:00 to 7 Oct 23:59\n",
    "\n",
    "df_merged_volume = pd.read_csv(\"../data/preprocessed_input_interpolate_20min_phase1and2_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change \"Date\" to datetime object\n",
    "df_merged_volume['date'] = pd.to_datetime(df_merged_volume['date'])\n",
    "\n",
    "# construct \"time of day\"\n",
    "df_merged_volume['timeofday'] = df_merged_volume.date.apply( lambda d : d.hour+d.minute/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 0, 'cargocar')</th>\n",
       "      <th>(1, 0, 'etc')</th>\n",
       "      <th>(1, 0, 'motorcycle')</th>\n",
       "      <th>(1, 0, 'privatecar')</th>\n",
       "      <th>(1, 0, 'tot')</th>\n",
       "      <th>(1, 0, 'unknowncar')</th>\n",
       "      <th>(1, 1, 'cargocar')</th>\n",
       "      <th>(1, 1, 'etc')</th>\n",
       "      <th>(1, 1, 'motorcycle')</th>\n",
       "      <th>(1, 1, 'privatecar')</th>\n",
       "      <th>...</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.200000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.233333</td>\n",
       "      <td>1018.233333</td>\n",
       "      <td>342.444444</td>\n",
       "      <td>3.4</td>\n",
       "      <td>21.411111</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.266667</td>\n",
       "      <td>1018.266667</td>\n",
       "      <td>343.888889</td>\n",
       "      <td>3.3</td>\n",
       "      <td>21.722222</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.300000</td>\n",
       "      <td>1018.300000</td>\n",
       "      <td>345.333333</td>\n",
       "      <td>3.2</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>1018.333333</td>\n",
       "      <td>346.777778</td>\n",
       "      <td>3.1</td>\n",
       "      <td>22.344444</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.366667</td>\n",
       "      <td>1018.366667</td>\n",
       "      <td>348.222222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.655556</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.400000</td>\n",
       "      <td>1018.400000</td>\n",
       "      <td>349.666667</td>\n",
       "      <td>2.9</td>\n",
       "      <td>22.966667</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.433333</td>\n",
       "      <td>1018.433333</td>\n",
       "      <td>351.111111</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.277778</td>\n",
       "      <td>60.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.466667</td>\n",
       "      <td>1018.466667</td>\n",
       "      <td>352.555556</td>\n",
       "      <td>2.7</td>\n",
       "      <td>23.588889</td>\n",
       "      <td>59.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.500000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (1, 0, 'cargocar')  (1, 0, 'etc')  (1, 0, 'motorcycle')  \\\n",
       "0                   0            1.0                     0   \n",
       "1                   0            0.0                     0   \n",
       "2                   0            1.0                     0   \n",
       "3                   0            2.0                     0   \n",
       "4                   0            1.0                     0   \n",
       "5                   0            1.0                     0   \n",
       "6                   0            0.0                     0   \n",
       "7                   0            2.0                     0   \n",
       "8                   0            0.0                     0   \n",
       "9                   0            0.0                     0   \n",
       "\n",
       "   (1, 0, 'privatecar')  (1, 0, 'tot')  (1, 0, 'unknowncar')  \\\n",
       "0                     0           14.0                  14.0   \n",
       "1                     0           13.0                  13.0   \n",
       "2                     0            7.0                   7.0   \n",
       "3                     0            6.0                   6.0   \n",
       "4                     0            5.0                   5.0   \n",
       "5                     0            5.0                   5.0   \n",
       "6                     0            6.0                   6.0   \n",
       "7                     0            9.0                   9.0   \n",
       "8                     0            7.0                   7.0   \n",
       "9                     0           10.0                  10.0   \n",
       "\n",
       "   (1, 1, 'cargocar')  (1, 1, 'etc')  (1, 1, 'motorcycle')  \\\n",
       "0                38.0           25.0                  89.0   \n",
       "1                24.0           11.0                  41.0   \n",
       "2                10.0            7.0                  22.0   \n",
       "3                 3.0            0.0                   3.0   \n",
       "4                 5.0            0.0                   3.0   \n",
       "5                 3.0            1.0                   8.0   \n",
       "6                 6.0            1.0                   2.0   \n",
       "7                 8.0            1.0                   4.0   \n",
       "8                 4.0            1.0                   4.0   \n",
       "9                 2.0            0.0                   0.0   \n",
       "\n",
       "   (1, 1, 'privatecar')    ...         pressure  sea_pressure  wind_direction  \\\n",
       "0                  12.0    ...      1013.200000   1018.200000      341.000000   \n",
       "1                  15.0    ...      1013.233333   1018.233333      342.444444   \n",
       "2                   5.0    ...      1013.266667   1018.266667      343.888889   \n",
       "3                   0.0    ...      1013.300000   1018.300000      345.333333   \n",
       "4                   0.0    ...      1013.333333   1018.333333      346.777778   \n",
       "5                   0.0    ...      1013.366667   1018.366667      348.222222   \n",
       "6                   0.0    ...      1013.400000   1018.400000      349.666667   \n",
       "7                   0.0    ...      1013.433333   1018.433333      351.111111   \n",
       "8                   0.0    ...      1013.466667   1018.466667      352.555556   \n",
       "9                   0.0    ...      1013.500000   1018.500000      354.000000   \n",
       "\n",
       "   wind_speed  temperature  rel_humidity  precipitation  dayofweek  \\\n",
       "0         3.5    21.100000     68.000000            0.0          1   \n",
       "1         3.4    21.411111     66.888889            0.0          1   \n",
       "2         3.3    21.722222     65.777778            0.0          1   \n",
       "3         3.2    22.033333     64.666667            0.0          1   \n",
       "4         3.1    22.344444     63.555556            0.0          1   \n",
       "5         3.0    22.655556     62.444444            0.0          1   \n",
       "6         2.9    22.966667     61.333333            0.0          1   \n",
       "7         2.8    23.277778     60.222222            0.0          1   \n",
       "8         2.7    23.588889     59.111111            0.0          1   \n",
       "9         2.6    23.900000     58.000000            0.0          1   \n",
       "\n",
       "   is_holiday  timeofday  \n",
       "0           0   0.000000  \n",
       "1           0   0.333333  \n",
       "2           0   0.666667  \n",
       "3           0   1.000000  \n",
       "4           0   1.333333  \n",
       "5           0   1.666667  \n",
       "6           0   2.000000  \n",
       "7           0   2.333333  \n",
       "8           0   2.666667  \n",
       "9           0   3.000000  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_volume.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['(1, 0, 'cargocar')', '(1, 0, 'etc')', '(1, 0, 'motorcycle')',\n",
       "       '(1, 0, 'privatecar')', '(1, 0, 'tot')', '(1, 0, 'unknowncar')',\n",
       "       '(1, 1, 'cargocar')', '(1, 1, 'etc')', '(1, 1, 'motorcycle')',\n",
       "       '(1, 1, 'privatecar')', '(1, 1, 'tot')', '(1, 1, 'unknowncar')',\n",
       "       '(2, 0, 'cargocar')', '(2, 0, 'etc')', '(2, 0, 'motorcycle')',\n",
       "       '(2, 0, 'privatecar')', '(2, 0, 'tot')', '(2, 0, 'unknowncar')',\n",
       "       '(3, 0, 'cargocar')', '(3, 0, 'etc')', '(3, 0, 'motorcycle')',\n",
       "       '(3, 0, 'privatecar')', '(3, 0, 'tot')', '(3, 0, 'unknowncar')',\n",
       "       '(3, 1, 'cargocar')', '(3, 1, 'etc')', '(3, 1, 'motorcycle')',\n",
       "       '(3, 1, 'privatecar')', '(3, 1, 'tot')', '(3, 1, 'unknowncar')',\n",
       "       '('A', 2)', '('A', 3)', '('B', 1)', '('B', 3)', '('C', 1)', '('C', 3)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_volume.columns[0:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the dataset stationary\n",
    "\n",
    "df_merged_volume_copy = df_merged_volume.copy()\n",
    "\n",
    "for i in range(1, len(df_merged_volume_copy)):\n",
    "    df_merged_volume_copy.loc[i, df_merged_volume_copy.columns[0:36]] = df_merged_volume.loc[i, df_merged_volume.columns[0:36]] - df_merged_volume.loc[i-1, df_merged_volume.columns[0:36]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 0, 'cargocar')</th>\n",
       "      <th>(1, 0, 'etc')</th>\n",
       "      <th>(1, 0, 'motorcycle')</th>\n",
       "      <th>(1, 0, 'privatecar')</th>\n",
       "      <th>(1, 0, 'tot')</th>\n",
       "      <th>(1, 0, 'unknowncar')</th>\n",
       "      <th>(1, 1, 'cargocar')</th>\n",
       "      <th>(1, 1, 'etc')</th>\n",
       "      <th>(1, 1, 'motorcycle')</th>\n",
       "      <th>(1, 1, 'privatecar')</th>\n",
       "      <th>...</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.200000</td>\n",
       "      <td>1018.200000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.233333</td>\n",
       "      <td>1018.233333</td>\n",
       "      <td>342.444444</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>21.411111</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.266667</td>\n",
       "      <td>1018.266667</td>\n",
       "      <td>343.888889</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>21.722222</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.300000</td>\n",
       "      <td>1018.300000</td>\n",
       "      <td>345.333333</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>1018.333333</td>\n",
       "      <td>346.777778</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>22.344444</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.366667</td>\n",
       "      <td>1018.366667</td>\n",
       "      <td>348.222222</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>22.655556</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.400000</td>\n",
       "      <td>1018.400000</td>\n",
       "      <td>349.666667</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>22.966667</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.433333</td>\n",
       "      <td>1018.433333</td>\n",
       "      <td>351.111111</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>23.277778</td>\n",
       "      <td>60.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.466667</td>\n",
       "      <td>1018.466667</td>\n",
       "      <td>352.555556</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>23.588889</td>\n",
       "      <td>59.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.500000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.355556</td>\n",
       "      <td>1018.355556</td>\n",
       "      <td>315.444444</td>\n",
       "      <td>2.822222</td>\n",
       "      <td>24.233333</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.211111</td>\n",
       "      <td>1018.211111</td>\n",
       "      <td>276.888889</td>\n",
       "      <td>3.044444</td>\n",
       "      <td>24.566667</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.066667</td>\n",
       "      <td>1018.066667</td>\n",
       "      <td>238.333333</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.922222</td>\n",
       "      <td>1017.922222</td>\n",
       "      <td>199.777778</td>\n",
       "      <td>3.488889</td>\n",
       "      <td>25.233333</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.777778</td>\n",
       "      <td>1017.777778</td>\n",
       "      <td>161.222222</td>\n",
       "      <td>3.711111</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.633333</td>\n",
       "      <td>1017.633333</td>\n",
       "      <td>122.666667</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.488889</td>\n",
       "      <td>1017.488889</td>\n",
       "      <td>84.111111</td>\n",
       "      <td>4.155556</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.344444</td>\n",
       "      <td>1017.344444</td>\n",
       "      <td>45.555556</td>\n",
       "      <td>4.377778</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>44.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.200000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.244444</td>\n",
       "      <td>1017.244444</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>4.588889</td>\n",
       "      <td>26.733333</td>\n",
       "      <td>42.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.288889</td>\n",
       "      <td>1017.288889</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>4.577778</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>42.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.333333</td>\n",
       "      <td>1017.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.377778</td>\n",
       "      <td>1017.377778</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>42.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.422222</td>\n",
       "      <td>1017.422222</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>4.544444</td>\n",
       "      <td>26.066667</td>\n",
       "      <td>42.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.466667</td>\n",
       "      <td>1017.466667</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.511111</td>\n",
       "      <td>1017.511111</td>\n",
       "      <td>10.111111</td>\n",
       "      <td>4.522222</td>\n",
       "      <td>25.733333</td>\n",
       "      <td>42.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.555556</td>\n",
       "      <td>1017.555556</td>\n",
       "      <td>10.555556</td>\n",
       "      <td>4.511111</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>42.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.600000</td>\n",
       "      <td>1017.600000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.722222</td>\n",
       "      <td>1017.722222</td>\n",
       "      <td>12.555556</td>\n",
       "      <td>4.188889</td>\n",
       "      <td>25.033333</td>\n",
       "      <td>43.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.844444</td>\n",
       "      <td>1017.844444</td>\n",
       "      <td>14.111111</td>\n",
       "      <td>3.877778</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.786550</td>\n",
       "      <td>1015.786550</td>\n",
       "      <td>354.077973</td>\n",
       "      <td>3.349123</td>\n",
       "      <td>20.149123</td>\n",
       "      <td>86.226121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.780117</td>\n",
       "      <td>1015.780117</td>\n",
       "      <td>354.144250</td>\n",
       "      <td>3.350877</td>\n",
       "      <td>20.150877</td>\n",
       "      <td>86.218324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.773684</td>\n",
       "      <td>1015.773684</td>\n",
       "      <td>354.210526</td>\n",
       "      <td>3.352632</td>\n",
       "      <td>20.152632</td>\n",
       "      <td>86.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.767251</td>\n",
       "      <td>1015.767251</td>\n",
       "      <td>354.276803</td>\n",
       "      <td>3.354386</td>\n",
       "      <td>20.154386</td>\n",
       "      <td>86.202729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.760819</td>\n",
       "      <td>1015.760819</td>\n",
       "      <td>354.343080</td>\n",
       "      <td>3.356140</td>\n",
       "      <td>20.156140</td>\n",
       "      <td>86.194932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.754386</td>\n",
       "      <td>1015.754386</td>\n",
       "      <td>354.409357</td>\n",
       "      <td>3.357895</td>\n",
       "      <td>20.157895</td>\n",
       "      <td>86.187135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.747953</td>\n",
       "      <td>1015.747953</td>\n",
       "      <td>354.475634</td>\n",
       "      <td>3.359649</td>\n",
       "      <td>20.159649</td>\n",
       "      <td>86.179337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.741520</td>\n",
       "      <td>1015.741520</td>\n",
       "      <td>354.541910</td>\n",
       "      <td>3.361404</td>\n",
       "      <td>20.161404</td>\n",
       "      <td>86.171540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.735088</td>\n",
       "      <td>1015.735088</td>\n",
       "      <td>354.608187</td>\n",
       "      <td>3.363158</td>\n",
       "      <td>20.163158</td>\n",
       "      <td>86.163743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.728655</td>\n",
       "      <td>1015.728655</td>\n",
       "      <td>354.674464</td>\n",
       "      <td>3.364912</td>\n",
       "      <td>20.164912</td>\n",
       "      <td>86.155945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.722222</td>\n",
       "      <td>1015.722222</td>\n",
       "      <td>354.740741</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>86.148148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.715789</td>\n",
       "      <td>1015.715789</td>\n",
       "      <td>354.807018</td>\n",
       "      <td>3.368421</td>\n",
       "      <td>20.168421</td>\n",
       "      <td>86.140351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.709357</td>\n",
       "      <td>1015.709357</td>\n",
       "      <td>354.873294</td>\n",
       "      <td>3.370175</td>\n",
       "      <td>20.170175</td>\n",
       "      <td>86.132554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.702924</td>\n",
       "      <td>1015.702924</td>\n",
       "      <td>354.939571</td>\n",
       "      <td>3.371930</td>\n",
       "      <td>20.171930</td>\n",
       "      <td>86.124756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.696491</td>\n",
       "      <td>1015.696491</td>\n",
       "      <td>355.005848</td>\n",
       "      <td>3.373684</td>\n",
       "      <td>20.173684</td>\n",
       "      <td>86.116959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.690058</td>\n",
       "      <td>1015.690058</td>\n",
       "      <td>355.072125</td>\n",
       "      <td>3.375439</td>\n",
       "      <td>20.175439</td>\n",
       "      <td>86.109162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.683626</td>\n",
       "      <td>1015.683626</td>\n",
       "      <td>355.138402</td>\n",
       "      <td>3.377193</td>\n",
       "      <td>20.177193</td>\n",
       "      <td>86.101365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.677193</td>\n",
       "      <td>1015.677193</td>\n",
       "      <td>355.204678</td>\n",
       "      <td>3.378947</td>\n",
       "      <td>20.178947</td>\n",
       "      <td>86.093567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.670760</td>\n",
       "      <td>1015.670760</td>\n",
       "      <td>355.270955</td>\n",
       "      <td>3.380702</td>\n",
       "      <td>20.180702</td>\n",
       "      <td>86.085770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.664327</td>\n",
       "      <td>1015.664327</td>\n",
       "      <td>355.337232</td>\n",
       "      <td>3.382456</td>\n",
       "      <td>20.182456</td>\n",
       "      <td>86.077973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.657895</td>\n",
       "      <td>1015.657895</td>\n",
       "      <td>355.403509</td>\n",
       "      <td>3.384211</td>\n",
       "      <td>20.184211</td>\n",
       "      <td>86.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.651462</td>\n",
       "      <td>1015.651462</td>\n",
       "      <td>355.469786</td>\n",
       "      <td>3.385965</td>\n",
       "      <td>20.185965</td>\n",
       "      <td>86.062378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.645029</td>\n",
       "      <td>1015.645029</td>\n",
       "      <td>355.536062</td>\n",
       "      <td>3.387719</td>\n",
       "      <td>20.187719</td>\n",
       "      <td>86.054581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.638596</td>\n",
       "      <td>1015.638596</td>\n",
       "      <td>355.602339</td>\n",
       "      <td>3.389474</td>\n",
       "      <td>20.189474</td>\n",
       "      <td>86.046784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.632164</td>\n",
       "      <td>1015.632164</td>\n",
       "      <td>355.668616</td>\n",
       "      <td>3.391228</td>\n",
       "      <td>20.191228</td>\n",
       "      <td>86.038986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.625731</td>\n",
       "      <td>1015.625731</td>\n",
       "      <td>355.734893</td>\n",
       "      <td>3.392982</td>\n",
       "      <td>20.192982</td>\n",
       "      <td>86.031189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.619298</td>\n",
       "      <td>1015.619298</td>\n",
       "      <td>355.801170</td>\n",
       "      <td>3.394737</td>\n",
       "      <td>20.194737</td>\n",
       "      <td>86.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.612865</td>\n",
       "      <td>1015.612865</td>\n",
       "      <td>355.867446</td>\n",
       "      <td>3.396491</td>\n",
       "      <td>20.196491</td>\n",
       "      <td>86.015595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.606433</td>\n",
       "      <td>1015.606433</td>\n",
       "      <td>355.933723</td>\n",
       "      <td>3.398246</td>\n",
       "      <td>20.198246</td>\n",
       "      <td>86.007797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.600000</td>\n",
       "      <td>1015.600000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2521 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      (1, 0, 'cargocar')  (1, 0, 'etc')  (1, 0, 'motorcycle')  \\\n",
       "0                      0            1.0                     0   \n",
       "1                      0           -1.0                     0   \n",
       "2                      0            1.0                     0   \n",
       "3                      0            1.0                     0   \n",
       "4                      0           -1.0                     0   \n",
       "5                      0            0.0                     0   \n",
       "6                      0           -1.0                     0   \n",
       "7                      0            2.0                     0   \n",
       "8                      0           -2.0                     0   \n",
       "9                      0            0.0                     0   \n",
       "10                     0            0.0                     0   \n",
       "11                     0            0.0                     0   \n",
       "12                     0            0.0                     0   \n",
       "13                     0            1.0                     0   \n",
       "14                     0            0.0                     0   \n",
       "15                     0            0.0                     0   \n",
       "16                     0           -1.0                     0   \n",
       "17                     0            2.0                     0   \n",
       "18                     0            2.0                     0   \n",
       "19                     0            0.0                     0   \n",
       "20                     0            4.0                     0   \n",
       "21                     0            6.0                     0   \n",
       "22                     0           -7.0                     0   \n",
       "23                     0            2.0                     0   \n",
       "24                     0           12.0                     0   \n",
       "25                     0            0.0                     0   \n",
       "26                     0           -6.0                     0   \n",
       "27                     0           -2.0                     0   \n",
       "28                     0            8.0                     0   \n",
       "29                     0           -7.0                     0   \n",
       "...                  ...            ...                   ...   \n",
       "2491                   0           -7.0                     0   \n",
       "2492                   0            1.0                     0   \n",
       "2493                   0           -6.0                     0   \n",
       "2494                   0           10.0                     0   \n",
       "2495                   0           -6.0                     0   \n",
       "2496                   0            0.0                     0   \n",
       "2497                   0            8.0                     0   \n",
       "2498                   0           -4.0                     0   \n",
       "2499                   0           -5.0                     0   \n",
       "2500                   0            7.0                     0   \n",
       "2501                   0           -3.0                     0   \n",
       "2502                   0           -7.0                     0   \n",
       "2503                   0            0.0                     0   \n",
       "2504                   0            2.0                     0   \n",
       "2505                   0           -1.0                     0   \n",
       "2506                   0            0.0                     0   \n",
       "2507                   0           -2.0                     0   \n",
       "2508                   0            5.0                     0   \n",
       "2509                   0           -1.0                     0   \n",
       "2510                   0           -1.0                     0   \n",
       "2511                   0            0.0                     0   \n",
       "2512                   0           -2.0                     0   \n",
       "2513                   0           -2.0                     0   \n",
       "2514                   0            1.0                     0   \n",
       "2515                   0            0.0                     0   \n",
       "2516                   0            1.0                     0   \n",
       "2517                   0           -2.0                     0   \n",
       "2518                   0            0.0                     0   \n",
       "2519                   0           -1.0                     0   \n",
       "2520                   0            0.0                     0   \n",
       "\n",
       "      (1, 0, 'privatecar')  (1, 0, 'tot')  (1, 0, 'unknowncar')  \\\n",
       "0                        0           14.0                  14.0   \n",
       "1                        0           -1.0                  -1.0   \n",
       "2                        0           -6.0                  -6.0   \n",
       "3                        0           -1.0                  -1.0   \n",
       "4                        0           -1.0                  -1.0   \n",
       "5                        0            0.0                   0.0   \n",
       "6                        0            1.0                   1.0   \n",
       "7                        0            3.0                   3.0   \n",
       "8                        0           -2.0                  -2.0   \n",
       "9                        0            3.0                   3.0   \n",
       "10                       0           -2.0                  -2.0   \n",
       "11                       0           -3.0                  -3.0   \n",
       "12                       0            6.0                   6.0   \n",
       "13                       0           -3.0                  -3.0   \n",
       "14                       0            7.0                   7.0   \n",
       "15                       0           -6.0                  -6.0   \n",
       "16                       0           -5.0                  -5.0   \n",
       "17                       0            6.0                   6.0   \n",
       "18                       0            3.0                   3.0   \n",
       "19                       0            7.0                   7.0   \n",
       "20                       0            9.0                   9.0   \n",
       "21                       0            9.0                   9.0   \n",
       "22                       0           -8.0                  -8.0   \n",
       "23                       0            1.0                   1.0   \n",
       "24                       0           22.0                  22.0   \n",
       "25                       0           -3.0                  -3.0   \n",
       "26                       0           -5.0                  -5.0   \n",
       "27                       0            6.0                   6.0   \n",
       "28                       0           17.0                  17.0   \n",
       "29                       0           -9.0                  -9.0   \n",
       "...                    ...            ...                   ...   \n",
       "2491                     0            2.0                   2.0   \n",
       "2492                     0           -5.0                  -5.0   \n",
       "2493                     0            6.0                   6.0   \n",
       "2494                     0           -3.0                  -3.0   \n",
       "2495                     0          -21.0                 -21.0   \n",
       "2496                     0           23.0                  23.0   \n",
       "2497                     0            1.0                   1.0   \n",
       "2498                     0           -6.0                  -6.0   \n",
       "2499                     0          -12.0                 -12.0   \n",
       "2500                     0            7.0                   7.0   \n",
       "2501                     0          -10.0                 -10.0   \n",
       "2502                     0          -14.0                 -14.0   \n",
       "2503                     0            3.0                   3.0   \n",
       "2504                     0           -5.0                  -5.0   \n",
       "2505                     0           14.0                  14.0   \n",
       "2506                     0          -11.0                 -11.0   \n",
       "2507                     0           -6.0                  -6.0   \n",
       "2508                     0            8.0                   8.0   \n",
       "2509                     0            1.0                   1.0   \n",
       "2510                     0           -2.0                  -2.0   \n",
       "2511                     0            2.0                   2.0   \n",
       "2512                     0           -5.0                  -5.0   \n",
       "2513                     0            0.0                   0.0   \n",
       "2514                     0           -4.0                  -4.0   \n",
       "2515                     0            1.0                   1.0   \n",
       "2516                     0           -4.0                  -4.0   \n",
       "2517                     0            4.0                   4.0   \n",
       "2518                     0           -4.0                  -4.0   \n",
       "2519                     0           -5.0                  -5.0   \n",
       "2520                     0           -2.0                  -2.0   \n",
       "\n",
       "      (1, 1, 'cargocar')  (1, 1, 'etc')  (1, 1, 'motorcycle')  \\\n",
       "0                   38.0           25.0                  89.0   \n",
       "1                  -14.0          -14.0                 -48.0   \n",
       "2                  -14.0           -4.0                 -19.0   \n",
       "3                   -7.0           -7.0                 -19.0   \n",
       "4                    2.0            0.0                   0.0   \n",
       "5                   -2.0            1.0                   5.0   \n",
       "6                    3.0            0.0                  -6.0   \n",
       "7                    2.0            0.0                   2.0   \n",
       "8                   -4.0            0.0                   0.0   \n",
       "9                   -2.0           -1.0                  -4.0   \n",
       "10                  10.0            0.0                   1.0   \n",
       "11                   2.0            0.0                   3.0   \n",
       "12                  -1.0            0.0                   3.0   \n",
       "13                  -3.0            3.0                   0.0   \n",
       "14                   0.0            0.0                   3.0   \n",
       "15                   4.0           -3.0                   3.0   \n",
       "16                  13.0            1.0                  -3.0   \n",
       "17                  -3.0            3.0                   1.0   \n",
       "18                  -1.0            0.0                  13.0   \n",
       "19                   1.0           -1.0                  11.0   \n",
       "20                   4.0            6.0                   5.0   \n",
       "21                  -8.0           14.0                  25.0   \n",
       "22                   6.0            9.0                  31.0   \n",
       "23                  -6.0            8.0                  21.0   \n",
       "24                   4.0           -3.0                 -17.0   \n",
       "25                  -5.0           -5.0                  27.0   \n",
       "26                   3.0           -9.0                 -19.0   \n",
       "27                   6.0            6.0                   7.0   \n",
       "28                  -6.0           -1.0                  -2.0   \n",
       "29                  11.0            9.0                  17.0   \n",
       "...                  ...            ...                   ...   \n",
       "2491                -1.0            1.0                  17.0   \n",
       "2492                -1.0            2.0                  -9.0   \n",
       "2493                -8.0           10.0                   1.0   \n",
       "2494                13.0           -8.0                 -12.0   \n",
       "2495                 0.0           -1.0                  21.0   \n",
       "2496               -11.0           10.0                  10.0   \n",
       "2497                 5.0            7.0                  -1.0   \n",
       "2498                -2.0           -8.0                  -1.0   \n",
       "2499                -3.0            7.0                   8.0   \n",
       "2500                -3.0           11.0                   4.0   \n",
       "2501                 1.0            5.0                  22.0   \n",
       "2502                 3.0          -28.0                 -63.0   \n",
       "2503                -6.0            5.0                  19.0   \n",
       "2504                -6.0          -12.0                  -6.0   \n",
       "2505                 9.0            0.0                 -11.0   \n",
       "2506                -4.0           -5.0                 -21.0   \n",
       "2507                 1.0            2.0                   5.0   \n",
       "2508                -2.0           -6.0                 -17.0   \n",
       "2509                 0.0           10.0                  17.0   \n",
       "2510                -1.0           -7.0                 -11.0   \n",
       "2511                -2.0           -4.0                 -11.0   \n",
       "2512                -1.0            3.0                   4.0   \n",
       "2513                -2.0           -4.0                   1.0   \n",
       "2514                 3.0            3.0                  -1.0   \n",
       "2515                -2.0            0.0                   0.0   \n",
       "2516                 0.0            0.0                  -6.0   \n",
       "2517                -2.0           -3.0                  -1.0   \n",
       "2518                 3.0           -3.0                  -3.0   \n",
       "2519                -4.0            2.0                   3.0   \n",
       "2520                -1.0           -2.0                 -12.0   \n",
       "\n",
       "      (1, 1, 'privatecar')    ...         pressure  sea_pressure  \\\n",
       "0                     12.0    ...      1013.200000   1018.200000   \n",
       "1                      3.0    ...      1013.233333   1018.233333   \n",
       "2                    -10.0    ...      1013.266667   1018.266667   \n",
       "3                     -5.0    ...      1013.300000   1018.300000   \n",
       "4                      0.0    ...      1013.333333   1018.333333   \n",
       "5                      0.0    ...      1013.366667   1018.366667   \n",
       "6                      0.0    ...      1013.400000   1018.400000   \n",
       "7                      0.0    ...      1013.433333   1018.433333   \n",
       "8                      0.0    ...      1013.466667   1018.466667   \n",
       "9                      0.0    ...      1013.500000   1018.500000   \n",
       "10                     0.0    ...      1013.355556   1018.355556   \n",
       "11                     0.0    ...      1013.211111   1018.211111   \n",
       "12                     0.0    ...      1013.066667   1018.066667   \n",
       "13                     0.0    ...      1012.922222   1017.922222   \n",
       "14                     0.0    ...      1012.777778   1017.777778   \n",
       "15                     0.0    ...      1012.633333   1017.633333   \n",
       "16                     0.0    ...      1012.488889   1017.488889   \n",
       "17                     0.0    ...      1012.344444   1017.344444   \n",
       "18                     0.0    ...      1012.200000   1017.200000   \n",
       "19                     0.0    ...      1012.244444   1017.244444   \n",
       "20                     1.0    ...      1012.288889   1017.288889   \n",
       "21                     2.0    ...      1012.333333   1017.333333   \n",
       "22                     1.0    ...      1012.377778   1017.377778   \n",
       "23                    -2.0    ...      1012.422222   1017.422222   \n",
       "24                     6.0    ...      1012.466667   1017.466667   \n",
       "25                    -3.0    ...      1012.511111   1017.511111   \n",
       "26                    -4.0    ...      1012.555556   1017.555556   \n",
       "27                     6.0    ...      1012.600000   1017.600000   \n",
       "28                     0.0    ...      1012.722222   1017.722222   \n",
       "29                     3.0    ...      1012.844444   1017.844444   \n",
       "...                    ...    ...              ...           ...   \n",
       "2491                   0.0    ...      1010.786550   1015.786550   \n",
       "2492                   1.0    ...      1010.780117   1015.780117   \n",
       "2493                   3.0    ...      1010.773684   1015.773684   \n",
       "2494                  -3.0    ...      1010.767251   1015.767251   \n",
       "2495                   2.0    ...      1010.760819   1015.760819   \n",
       "2496                  -3.0    ...      1010.754386   1015.754386   \n",
       "2497                   5.0    ...      1010.747953   1015.747953   \n",
       "2498                  -2.0    ...      1010.741520   1015.741520   \n",
       "2499                   0.0    ...      1010.735088   1015.735088   \n",
       "2500                   3.0    ...      1010.728655   1015.728655   \n",
       "2501                  -1.0    ...      1010.722222   1015.722222   \n",
       "2502                  -5.0    ...      1010.715789   1015.715789   \n",
       "2503                   2.0    ...      1010.709357   1015.709357   \n",
       "2504                  -3.0    ...      1010.702924   1015.702924   \n",
       "2505                   1.0    ...      1010.696491   1015.696491   \n",
       "2506                   0.0    ...      1010.690058   1015.690058   \n",
       "2507                  -1.0    ...      1010.683626   1015.683626   \n",
       "2508                   3.0    ...      1010.677193   1015.677193   \n",
       "2509                  -1.0    ...      1010.670760   1015.670760   \n",
       "2510                  -2.0    ...      1010.664327   1015.664327   \n",
       "2511                   0.0    ...      1010.657895   1015.657895   \n",
       "2512                   0.0    ...      1010.651462   1015.651462   \n",
       "2513                   0.0    ...      1010.645029   1015.645029   \n",
       "2514                   0.0    ...      1010.638596   1015.638596   \n",
       "2515                   0.0    ...      1010.632164   1015.632164   \n",
       "2516                   0.0    ...      1010.625731   1015.625731   \n",
       "2517                   0.0    ...      1010.619298   1015.619298   \n",
       "2518                   0.0    ...      1010.612865   1015.612865   \n",
       "2519                   0.0    ...      1010.606433   1015.606433   \n",
       "2520                   0.0    ...      1010.600000   1015.600000   \n",
       "\n",
       "      wind_direction  wind_speed  temperature  rel_humidity  precipitation  \\\n",
       "0         341.000000    3.500000    21.100000     68.000000            0.0   \n",
       "1         342.444444    3.400000    21.411111     66.888889            0.0   \n",
       "2         343.888889    3.300000    21.722222     65.777778            0.0   \n",
       "3         345.333333    3.200000    22.033333     64.666667            0.0   \n",
       "4         346.777778    3.100000    22.344444     63.555556            0.0   \n",
       "5         348.222222    3.000000    22.655556     62.444444            0.0   \n",
       "6         349.666667    2.900000    22.966667     61.333333            0.0   \n",
       "7         351.111111    2.800000    23.277778     60.222222            0.0   \n",
       "8         352.555556    2.700000    23.588889     59.111111            0.0   \n",
       "9         354.000000    2.600000    23.900000     58.000000            0.0   \n",
       "10        315.444444    2.822222    24.233333     56.333333            0.0   \n",
       "11        276.888889    3.044444    24.566667     54.666667            0.0   \n",
       "12        238.333333    3.266667    24.900000     53.000000            0.0   \n",
       "13        199.777778    3.488889    25.233333     51.333333            0.0   \n",
       "14        161.222222    3.711111    25.566667     49.666667            0.0   \n",
       "15        122.666667    3.933333    25.900000     48.000000            0.0   \n",
       "16         84.111111    4.155556    26.233333     46.333333            0.0   \n",
       "17         45.555556    4.377778    26.566667     44.666667            0.0   \n",
       "18          7.000000    4.600000    26.900000     43.000000            0.0   \n",
       "19          7.444444    4.588889    26.733333     42.888889            0.0   \n",
       "20          7.888889    4.577778    26.566667     42.777778            0.0   \n",
       "21          8.333333    4.566667    26.400000     42.666667            0.0   \n",
       "22          8.777778    4.555556    26.233333     42.555556            0.0   \n",
       "23          9.222222    4.544444    26.066667     42.444444            0.0   \n",
       "24          9.666667    4.533333    25.900000     42.333333            0.0   \n",
       "25         10.111111    4.522222    25.733333     42.222222            0.0   \n",
       "26         10.555556    4.511111    25.566667     42.111111            0.0   \n",
       "27         11.000000    4.500000    25.400000     42.000000            0.0   \n",
       "28         12.555556    4.188889    25.033333     43.222222            0.0   \n",
       "29         14.111111    3.877778    24.666667     44.444444            0.0   \n",
       "...              ...         ...          ...           ...            ...   \n",
       "2491      354.077973    3.349123    20.149123     86.226121            0.0   \n",
       "2492      354.144250    3.350877    20.150877     86.218324            0.0   \n",
       "2493      354.210526    3.352632    20.152632     86.210526            0.0   \n",
       "2494      354.276803    3.354386    20.154386     86.202729            0.0   \n",
       "2495      354.343080    3.356140    20.156140     86.194932            0.0   \n",
       "2496      354.409357    3.357895    20.157895     86.187135            0.0   \n",
       "2497      354.475634    3.359649    20.159649     86.179337            0.0   \n",
       "2498      354.541910    3.361404    20.161404     86.171540            0.0   \n",
       "2499      354.608187    3.363158    20.163158     86.163743            0.0   \n",
       "2500      354.674464    3.364912    20.164912     86.155945            0.0   \n",
       "2501      354.740741    3.366667    20.166667     86.148148            0.0   \n",
       "2502      354.807018    3.368421    20.168421     86.140351            0.0   \n",
       "2503      354.873294    3.370175    20.170175     86.132554            0.0   \n",
       "2504      354.939571    3.371930    20.171930     86.124756            0.0   \n",
       "2505      355.005848    3.373684    20.173684     86.116959            0.0   \n",
       "2506      355.072125    3.375439    20.175439     86.109162            0.0   \n",
       "2507      355.138402    3.377193    20.177193     86.101365            0.0   \n",
       "2508      355.204678    3.378947    20.178947     86.093567            0.0   \n",
       "2509      355.270955    3.380702    20.180702     86.085770            0.0   \n",
       "2510      355.337232    3.382456    20.182456     86.077973            0.0   \n",
       "2511      355.403509    3.384211    20.184211     86.070175            0.0   \n",
       "2512      355.469786    3.385965    20.185965     86.062378            0.0   \n",
       "2513      355.536062    3.387719    20.187719     86.054581            0.0   \n",
       "2514      355.602339    3.389474    20.189474     86.046784            0.0   \n",
       "2515      355.668616    3.391228    20.191228     86.038986            0.0   \n",
       "2516      355.734893    3.392982    20.192982     86.031189            0.0   \n",
       "2517      355.801170    3.394737    20.194737     86.023392            0.0   \n",
       "2518      355.867446    3.396491    20.196491     86.015595            0.0   \n",
       "2519      355.933723    3.398246    20.198246     86.007797            0.0   \n",
       "2520      356.000000    3.400000    20.200000     86.000000            0.0   \n",
       "\n",
       "      dayofweek  is_holiday  timeofday  \n",
       "0             1           0   0.000000  \n",
       "1             1           0   0.333333  \n",
       "2             1           0   0.666667  \n",
       "3             1           0   1.000000  \n",
       "4             1           0   1.333333  \n",
       "5             1           0   1.666667  \n",
       "6             1           0   2.000000  \n",
       "7             1           0   2.333333  \n",
       "8             1           0   2.666667  \n",
       "9             1           0   3.000000  \n",
       "10            1           0   3.333333  \n",
       "11            1           0   3.666667  \n",
       "12            1           0   4.000000  \n",
       "13            1           0   4.333333  \n",
       "14            1           0   4.666667  \n",
       "15            1           0   5.000000  \n",
       "16            1           0   5.333333  \n",
       "17            1           0   5.666667  \n",
       "18            1           0   6.000000  \n",
       "19            1           0   6.333333  \n",
       "20            1           0   6.666667  \n",
       "21            1           0   7.000000  \n",
       "22            1           0   7.333333  \n",
       "23            1           0   7.666667  \n",
       "24            1           0   8.000000  \n",
       "25            1           0   8.333333  \n",
       "26            1           0   8.666667  \n",
       "27            1           0   9.000000  \n",
       "28            1           0   9.333333  \n",
       "29            1           0   9.666667  \n",
       "...         ...         ...        ...  \n",
       "2491          0           1  14.333333  \n",
       "2492          0           1  14.666667  \n",
       "2493          0           1  15.000000  \n",
       "2494          0           1  15.333333  \n",
       "2495          0           1  15.666667  \n",
       "2496          0           1  16.000000  \n",
       "2497          0           1  16.333333  \n",
       "2498          0           1  16.666667  \n",
       "2499          0           1  17.000000  \n",
       "2500          0           1  17.333333  \n",
       "2501          0           1  17.666667  \n",
       "2502          0           1  18.000000  \n",
       "2503          0           1  18.333333  \n",
       "2504          0           1  18.666667  \n",
       "2505          0           1  19.000000  \n",
       "2506          0           1  19.333333  \n",
       "2507          0           1  19.666667  \n",
       "2508          0           1  20.000000  \n",
       "2509          0           1  20.333333  \n",
       "2510          0           1  20.666667  \n",
       "2511          0           1  21.000000  \n",
       "2512          0           1  21.333333  \n",
       "2513          0           1  21.666667  \n",
       "2514          0           1  22.000000  \n",
       "2515          0           1  22.333333  \n",
       "2516          0           1  22.666667  \n",
       "2517          0           1  23.000000  \n",
       "2518          0           1  23.333333  \n",
       "2519          0           1  23.666667  \n",
       "2520          1           0   0.000000  \n",
       "\n",
       "[2521 rows x 48 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_volume_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 0, 'cargocar')\n",
      "1 (1, 0, 'etc')\n",
      "2 (1, 0, 'motorcycle')\n",
      "3 (1, 0, 'privatecar')\n",
      "4 (1, 0, 'tot')\n",
      "5 (1, 0, 'unknowncar')\n",
      "6 (1, 1, 'cargocar')\n",
      "7 (1, 1, 'etc')\n",
      "8 (1, 1, 'motorcycle')\n",
      "9 (1, 1, 'privatecar')\n",
      "10 (1, 1, 'tot')\n",
      "11 (1, 1, 'unknowncar')\n",
      "12 (2, 0, 'cargocar')\n",
      "13 (2, 0, 'etc')\n",
      "14 (2, 0, 'motorcycle')\n",
      "15 (2, 0, 'privatecar')\n",
      "16 (2, 0, 'tot')\n",
      "17 (2, 0, 'unknowncar')\n",
      "18 (3, 0, 'cargocar')\n",
      "19 (3, 0, 'etc')\n",
      "20 (3, 0, 'motorcycle')\n",
      "21 (3, 0, 'privatecar')\n",
      "22 (3, 0, 'tot')\n",
      "23 (3, 0, 'unknowncar')\n",
      "24 (3, 1, 'cargocar')\n",
      "25 (3, 1, 'etc')\n",
      "26 (3, 1, 'motorcycle')\n",
      "27 (3, 1, 'privatecar')\n",
      "28 (3, 1, 'tot')\n",
      "29 (3, 1, 'unknowncar')\n",
      "30 ('A', 2)\n",
      "31 ('A', 3)\n",
      "32 ('B', 1)\n",
      "33 ('B', 3)\n",
      "34 ('C', 1)\n",
      "35 ('C', 3)\n",
      "36 date\n",
      "37 hour\n",
      "38 pressure\n",
      "39 sea_pressure\n",
      "40 wind_direction\n",
      "41 wind_speed\n",
      "42 temperature\n",
      "43 rel_humidity\n",
      "44 precipitation\n",
      "45 dayofweek\n",
      "46 is_holiday\n",
      "47 timeofday\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(df_merged_volume.columns):\n",
    "    print(idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' OLD Function for converting it as stationary data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' OLD Function for converting it as stationary data'''\n",
    "\n",
    "# # create a differenced series for Volumn and traffic time [modified from http://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/]\n",
    "# def difference(dataset, interval=1):\n",
    "#     diff_array = np.ndarray(np.shape(dataset))\n",
    "#     for i in range(interval, len(dataset)):\n",
    "#         diff_array[i][0:2] = dataset[i][0:2] - dataset[i - interval][0:2]  # only select index 0 & 1\n",
    "#     return diff_array[1:]  # eliminate the first row (all zeros)\n",
    "\n",
    "# # invert differenced value\n",
    "# def inverse_difference(history, y_hat, interval=1):\n",
    "#     return y_hat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the time for training: 6:20-10:00 (5 + 6 timestamp) and 15:20-19:00 (5 + 6 timestamp)\n",
    "\n",
    "sel_rows = df_merged_volume_copy[ ((df_merged_volume_copy.timeofday>= 6.3) & (df_merged_volume_copy.timeofday<10)) |\n",
    "                            ((df_merged_volume_copy.timeofday>=15.3) & (df_merged_volume_copy.timeofday<19))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select using columns\n",
    "\n",
    "using_cols = [\n",
    "#                 \"(1, 0, 'cargocar')\",\n",
    "#                 \"(1, 0, 'etc')\",\n",
    "#                 \"(1, 0, 'motorcycle')\",\n",
    "#                 \"(1, 0, 'privatecar')\",\n",
    "#                 \"(1, 0, 'tot')\",\n",
    "#                 \"(1, 0, 'unknowncar')\",\n",
    "#                 \"(1, 1, 'cargocar')\",\n",
    "#                 \"(1, 1, 'etc')\",\n",
    "#                 \"(1, 1, 'motorcycle')\",\n",
    "#                 \"(1, 1, 'privatecar')\",\n",
    "#                 \"(1, 1, 'tot')\",\n",
    "#                 \"(1, 1, 'unknowncar')\",\n",
    "#                 \"(2, 0, 'cargocar')\",\n",
    "                \"(2, 0, 'etc')\",\n",
    "#                 \"(2, 0, 'motorcycle')\",\n",
    "#                 \"(2, 0, 'privatecar')\",\n",
    "                \"(2, 0, 'tot')\",\n",
    "#                 \"(2, 0, 'unknowncar')\",\n",
    "#                 \"(3, 0, 'cargocar')\",\n",
    "#                 \"(3, 0, 'etc')\",\n",
    "#                 \"(3, 0, 'motorcycle')\",\n",
    "#                 \"(3, 0, 'privatecar')\",\n",
    "#                 \"(3, 0, 'tot')\",\n",
    "#                 \"(3, 0, 'unknowncar')\",\n",
    "#                 \"(3, 1, 'cargocar')\",\n",
    "#                 \"(3, 1, 'etc')\",\n",
    "#                 \"(3, 1, 'motorcycle')\",\n",
    "#                 \"(3, 1, 'privatecar')\",\n",
    "#                 \"(3, 1, 'tot')\",\n",
    "#                 \"(3, 1, 'unknowncar')\",\n",
    "                \"('A', 2)\",\n",
    "#                 \"('A', 3)\",\n",
    "#                 \"('B', 1)\",\n",
    "#                 \"('B', 3)\",\n",
    "#                 \"('C', 1)\",\n",
    "#                 \"('C', 3)\",\n",
    "                'date',\n",
    "                'hour',\n",
    "                'pressure',\n",
    "                'sea_pressure',\n",
    "                'wind_direction',\n",
    "                'wind_speed',\n",
    "                'temperature',\n",
    "                'rel_humidity',\n",
    "                'precipitation',\n",
    "                'dayofweek',\n",
    "                'is_holiday',\n",
    "                'timeofday'\n",
    "              ]\n",
    "\n",
    "sel_rows = sel_rows[using_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(2, 0, 'etc')</th>\n",
       "      <th>(2, 0, 'tot')</th>\n",
       "      <th>('A', 2)</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>timeofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>22.582500</td>\n",
       "      <td>2016-09-20 06:20:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1012.244444</td>\n",
       "      <td>1017.244444</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>4.588889</td>\n",
       "      <td>26.733333</td>\n",
       "      <td>42.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-40.840000</td>\n",
       "      <td>2016-09-20 06:40:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1012.288889</td>\n",
       "      <td>1017.288889</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>4.577778</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>42.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.645000</td>\n",
       "      <td>2016-09-20 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1012.333333</td>\n",
       "      <td>1017.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-4.975000</td>\n",
       "      <td>2016-09-20 07:20:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1012.377778</td>\n",
       "      <td>1017.377778</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>42.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.285000</td>\n",
       "      <td>2016-09-20 07:40:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1012.422222</td>\n",
       "      <td>1017.422222</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>4.544444</td>\n",
       "      <td>26.066667</td>\n",
       "      <td>42.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-5.361667</td>\n",
       "      <td>2016-09-20 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1012.466667</td>\n",
       "      <td>1017.466667</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.203030</td>\n",
       "      <td>2016-09-20 08:20:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1012.511111</td>\n",
       "      <td>1017.511111</td>\n",
       "      <td>10.111111</td>\n",
       "      <td>4.522222</td>\n",
       "      <td>25.733333</td>\n",
       "      <td>42.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.908373</td>\n",
       "      <td>2016-09-20 08:40:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1012.555556</td>\n",
       "      <td>1017.555556</td>\n",
       "      <td>10.555556</td>\n",
       "      <td>4.511111</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>42.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>6.003089</td>\n",
       "      <td>2016-09-20 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1012.600000</td>\n",
       "      <td>1017.600000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-12.227826</td>\n",
       "      <td>2016-09-20 09:20:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1012.722222</td>\n",
       "      <td>1017.722222</td>\n",
       "      <td>12.555556</td>\n",
       "      <td>4.188889</td>\n",
       "      <td>25.033333</td>\n",
       "      <td>43.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>14.685385</td>\n",
       "      <td>2016-09-20 09:40:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1012.844444</td>\n",
       "      <td>1017.844444</td>\n",
       "      <td>14.111111</td>\n",
       "      <td>3.877778</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.949167</td>\n",
       "      <td>2016-09-20 15:20:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1013.977778</td>\n",
       "      <td>1018.977778</td>\n",
       "      <td>326.111111</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>20.377778</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.795227</td>\n",
       "      <td>2016-09-20 15:40:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1013.955556</td>\n",
       "      <td>1018.955556</td>\n",
       "      <td>326.222222</td>\n",
       "      <td>2.022222</td>\n",
       "      <td>20.255556</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.123904</td>\n",
       "      <td>2016-09-20 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1013.933333</td>\n",
       "      <td>1018.933333</td>\n",
       "      <td>326.333333</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>20.133333</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>19.482508</td>\n",
       "      <td>2016-09-20 16:20:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1013.911111</td>\n",
       "      <td>1018.911111</td>\n",
       "      <td>326.444444</td>\n",
       "      <td>1.844444</td>\n",
       "      <td>20.011111</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-23.384398</td>\n",
       "      <td>2016-09-20 16:40:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1013.888889</td>\n",
       "      <td>1018.888889</td>\n",
       "      <td>326.555556</td>\n",
       "      <td>1.755556</td>\n",
       "      <td>19.888889</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-4.865440</td>\n",
       "      <td>2016-09-20 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1013.866667</td>\n",
       "      <td>1018.866667</td>\n",
       "      <td>326.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>19.766667</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.381846</td>\n",
       "      <td>2016-09-20 17:20:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1013.844444</td>\n",
       "      <td>1018.844444</td>\n",
       "      <td>326.777778</td>\n",
       "      <td>1.577778</td>\n",
       "      <td>19.644444</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.833385</td>\n",
       "      <td>2016-09-20 17:40:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1013.822222</td>\n",
       "      <td>1018.822222</td>\n",
       "      <td>326.888889</td>\n",
       "      <td>1.488889</td>\n",
       "      <td>19.522222</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>16.904615</td>\n",
       "      <td>2016-09-20 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1013.800000</td>\n",
       "      <td>1018.800000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-12.804286</td>\n",
       "      <td>2016-09-20 18:20:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1013.833333</td>\n",
       "      <td>1018.833333</td>\n",
       "      <td>327.333333</td>\n",
       "      <td>1.355556</td>\n",
       "      <td>19.311111</td>\n",
       "      <td>74.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.814286</td>\n",
       "      <td>2016-09-20 18:40:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1013.866667</td>\n",
       "      <td>1018.866667</td>\n",
       "      <td>327.666667</td>\n",
       "      <td>1.311111</td>\n",
       "      <td>19.222222</td>\n",
       "      <td>75.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.008333</td>\n",
       "      <td>2016-09-21 06:20:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1013.111111</td>\n",
       "      <td>1018.111111</td>\n",
       "      <td>330.222222</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>26.055556</td>\n",
       "      <td>57.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.490952</td>\n",
       "      <td>2016-09-21 06:40:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1013.122222</td>\n",
       "      <td>1018.122222</td>\n",
       "      <td>332.444444</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>25.911111</td>\n",
       "      <td>58.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.001286</td>\n",
       "      <td>2016-09-21 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1013.133333</td>\n",
       "      <td>1018.133333</td>\n",
       "      <td>334.666667</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>25.766667</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>62.387000</td>\n",
       "      <td>2016-09-21 07:20:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1013.144444</td>\n",
       "      <td>1018.144444</td>\n",
       "      <td>336.888889</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>25.622222</td>\n",
       "      <td>60.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-64.485250</td>\n",
       "      <td>2016-09-21 07:40:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1013.155556</td>\n",
       "      <td>1018.155556</td>\n",
       "      <td>339.111111</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>25.477778</td>\n",
       "      <td>62.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-3.638750</td>\n",
       "      <td>2016-09-21 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1013.166667</td>\n",
       "      <td>1018.166667</td>\n",
       "      <td>341.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>11.301250</td>\n",
       "      <td>2016-09-21 08:20:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1013.177778</td>\n",
       "      <td>1018.177778</td>\n",
       "      <td>343.555556</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>25.188889</td>\n",
       "      <td>64.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-2.504028</td>\n",
       "      <td>2016-09-21 08:40:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1013.188889</td>\n",
       "      <td>1018.188889</td>\n",
       "      <td>345.777778</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.044444</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.882564</td>\n",
       "      <td>2016-10-23 16:20:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1011.211111</td>\n",
       "      <td>1016.211111</td>\n",
       "      <td>349.703704</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>20.033333</td>\n",
       "      <td>86.740741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-3.626905</td>\n",
       "      <td>2016-10-23 16:40:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1011.204678</td>\n",
       "      <td>1016.204678</td>\n",
       "      <td>349.769981</td>\n",
       "      <td>3.235088</td>\n",
       "      <td>20.035088</td>\n",
       "      <td>86.732943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.512238</td>\n",
       "      <td>2016-10-23 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1011.198246</td>\n",
       "      <td>1016.198246</td>\n",
       "      <td>349.836257</td>\n",
       "      <td>3.236842</td>\n",
       "      <td>20.036842</td>\n",
       "      <td>86.725146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>-14.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-26.206444</td>\n",
       "      <td>2016-10-23 17:20:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1011.191813</td>\n",
       "      <td>1016.191813</td>\n",
       "      <td>349.902534</td>\n",
       "      <td>3.238596</td>\n",
       "      <td>20.038596</td>\n",
       "      <td>86.717349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.782991</td>\n",
       "      <td>2016-10-23 17:40:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1011.185380</td>\n",
       "      <td>1016.185380</td>\n",
       "      <td>349.968811</td>\n",
       "      <td>3.240351</td>\n",
       "      <td>20.040351</td>\n",
       "      <td>86.709552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.882587</td>\n",
       "      <td>2016-10-23 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1011.178947</td>\n",
       "      <td>1016.178947</td>\n",
       "      <td>350.035088</td>\n",
       "      <td>3.242105</td>\n",
       "      <td>20.042105</td>\n",
       "      <td>86.701754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.651818</td>\n",
       "      <td>2016-10-23 18:20:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1011.172515</td>\n",
       "      <td>1016.172515</td>\n",
       "      <td>350.101365</td>\n",
       "      <td>3.243860</td>\n",
       "      <td>20.043860</td>\n",
       "      <td>86.693957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.121250</td>\n",
       "      <td>2016-10-23 18:40:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1011.166082</td>\n",
       "      <td>1016.166082</td>\n",
       "      <td>350.167641</td>\n",
       "      <td>3.245614</td>\n",
       "      <td>20.045614</td>\n",
       "      <td>86.686160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.927381</td>\n",
       "      <td>2016-10-24 06:20:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1010.940936</td>\n",
       "      <td>1015.940936</td>\n",
       "      <td>352.487329</td>\n",
       "      <td>3.307018</td>\n",
       "      <td>20.107018</td>\n",
       "      <td>86.413255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-12.947714</td>\n",
       "      <td>2016-10-24 06:40:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1010.934503</td>\n",
       "      <td>1015.934503</td>\n",
       "      <td>352.553606</td>\n",
       "      <td>3.308772</td>\n",
       "      <td>20.108772</td>\n",
       "      <td>86.405458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>24.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>38.515750</td>\n",
       "      <td>2016-10-24 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1010.928070</td>\n",
       "      <td>1015.928070</td>\n",
       "      <td>352.619883</td>\n",
       "      <td>3.310526</td>\n",
       "      <td>20.110526</td>\n",
       "      <td>86.397661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-21.650893</td>\n",
       "      <td>2016-10-24 07:20:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1010.921637</td>\n",
       "      <td>1015.921637</td>\n",
       "      <td>352.686160</td>\n",
       "      <td>3.312281</td>\n",
       "      <td>20.112281</td>\n",
       "      <td>86.389864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.245143</td>\n",
       "      <td>2016-10-24 07:40:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1010.915205</td>\n",
       "      <td>1015.915205</td>\n",
       "      <td>352.752437</td>\n",
       "      <td>3.314035</td>\n",
       "      <td>20.114035</td>\n",
       "      <td>86.382066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.692615</td>\n",
       "      <td>2016-10-24 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1010.908772</td>\n",
       "      <td>1015.908772</td>\n",
       "      <td>352.818713</td>\n",
       "      <td>3.315789</td>\n",
       "      <td>20.115789</td>\n",
       "      <td>86.374269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.737473</td>\n",
       "      <td>2016-10-24 08:20:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1010.902339</td>\n",
       "      <td>1015.902339</td>\n",
       "      <td>352.884990</td>\n",
       "      <td>3.317544</td>\n",
       "      <td>20.117544</td>\n",
       "      <td>86.366472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-21.995210</td>\n",
       "      <td>2016-10-24 08:40:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1010.895906</td>\n",
       "      <td>1015.895906</td>\n",
       "      <td>352.951267</td>\n",
       "      <td>3.319298</td>\n",
       "      <td>20.119298</td>\n",
       "      <td>86.358674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.849853</td>\n",
       "      <td>2016-10-24 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1010.889474</td>\n",
       "      <td>1015.889474</td>\n",
       "      <td>353.017544</td>\n",
       "      <td>3.321053</td>\n",
       "      <td>20.121053</td>\n",
       "      <td>86.350877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>-16.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.474500</td>\n",
       "      <td>2016-10-24 09:20:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1010.883041</td>\n",
       "      <td>1015.883041</td>\n",
       "      <td>353.083821</td>\n",
       "      <td>3.322807</td>\n",
       "      <td>20.122807</td>\n",
       "      <td>86.343080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-25.688455</td>\n",
       "      <td>2016-10-24 09:40:00</td>\n",
       "      <td>9</td>\n",
       "      <td>1010.876608</td>\n",
       "      <td>1015.876608</td>\n",
       "      <td>353.150097</td>\n",
       "      <td>3.324561</td>\n",
       "      <td>20.124561</td>\n",
       "      <td>86.335283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-43.298586</td>\n",
       "      <td>2016-10-24 15:20:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1010.767251</td>\n",
       "      <td>1015.767251</td>\n",
       "      <td>354.276803</td>\n",
       "      <td>3.354386</td>\n",
       "      <td>20.154386</td>\n",
       "      <td>86.202729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.185303</td>\n",
       "      <td>2016-10-24 15:40:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1010.760819</td>\n",
       "      <td>1015.760819</td>\n",
       "      <td>354.343080</td>\n",
       "      <td>3.356140</td>\n",
       "      <td>20.156140</td>\n",
       "      <td>86.194932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>42.616333</td>\n",
       "      <td>2016-10-24 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1010.754386</td>\n",
       "      <td>1015.754386</td>\n",
       "      <td>354.409357</td>\n",
       "      <td>3.357895</td>\n",
       "      <td>20.157895</td>\n",
       "      <td>86.187135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-19.589667</td>\n",
       "      <td>2016-10-24 16:20:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1010.747953</td>\n",
       "      <td>1015.747953</td>\n",
       "      <td>354.475634</td>\n",
       "      <td>3.359649</td>\n",
       "      <td>20.159649</td>\n",
       "      <td>86.179337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.917222</td>\n",
       "      <td>2016-10-24 16:40:00</td>\n",
       "      <td>16</td>\n",
       "      <td>1010.741520</td>\n",
       "      <td>1015.741520</td>\n",
       "      <td>354.541910</td>\n",
       "      <td>3.361404</td>\n",
       "      <td>20.161404</td>\n",
       "      <td>86.171540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>26.232222</td>\n",
       "      <td>2016-10-24 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1010.735088</td>\n",
       "      <td>1015.735088</td>\n",
       "      <td>354.608187</td>\n",
       "      <td>3.363158</td>\n",
       "      <td>20.163158</td>\n",
       "      <td>86.163743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-16.053333</td>\n",
       "      <td>2016-10-24 17:20:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1010.728655</td>\n",
       "      <td>1015.728655</td>\n",
       "      <td>354.674464</td>\n",
       "      <td>3.364912</td>\n",
       "      <td>20.164912</td>\n",
       "      <td>86.155945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.139167</td>\n",
       "      <td>2016-10-24 17:40:00</td>\n",
       "      <td>17</td>\n",
       "      <td>1010.722222</td>\n",
       "      <td>1015.722222</td>\n",
       "      <td>354.740741</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>86.148148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>11.297500</td>\n",
       "      <td>2016-10-24 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1010.715789</td>\n",
       "      <td>1015.715789</td>\n",
       "      <td>354.807018</td>\n",
       "      <td>3.368421</td>\n",
       "      <td>20.168421</td>\n",
       "      <td>86.140351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-3.789000</td>\n",
       "      <td>2016-10-24 18:20:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1010.709357</td>\n",
       "      <td>1015.709357</td>\n",
       "      <td>354.873294</td>\n",
       "      <td>3.370175</td>\n",
       "      <td>20.170175</td>\n",
       "      <td>86.132554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.500250</td>\n",
       "      <td>2016-10-24 18:40:00</td>\n",
       "      <td>18</td>\n",
       "      <td>1010.702924</td>\n",
       "      <td>1015.702924</td>\n",
       "      <td>354.939571</td>\n",
       "      <td>3.371930</td>\n",
       "      <td>20.171930</td>\n",
       "      <td>86.124756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      (2, 0, 'etc')  (2, 0, 'tot')   ('A', 2)                date  hour  \\\n",
       "19              4.0           -2.0  22.582500 2016-09-20 06:20:00     6   \n",
       "20              6.0           23.0 -40.840000 2016-09-20 06:40:00     6   \n",
       "21             15.0           25.0   9.645000 2016-09-20 07:00:00     7   \n",
       "22              6.0            7.0  -4.975000 2016-09-20 07:20:00     7   \n",
       "23              2.0            7.0   7.285000 2016-09-20 07:40:00     7   \n",
       "24             -3.0           19.0  -5.361667 2016-09-20 08:00:00     8   \n",
       "25              6.0           -1.0  11.203030 2016-09-20 08:20:00     8   \n",
       "26             -6.0            1.0   6.908373 2016-09-20 08:40:00     8   \n",
       "27             -7.0           -7.0   6.003089 2016-09-20 09:00:00     9   \n",
       "28             -1.0           -6.0 -12.227826 2016-09-20 09:20:00     9   \n",
       "29             -1.0          -12.0  14.685385 2016-09-20 09:40:00     9   \n",
       "46              1.0            7.0  14.949167 2016-09-20 15:20:00    15   \n",
       "47             -6.0            0.0  -2.795227 2016-09-20 15:40:00    15   \n",
       "48              8.0           13.0   5.123904 2016-09-20 16:00:00    16   \n",
       "49             -2.0           -6.0  19.482508 2016-09-20 16:20:00    16   \n",
       "50              4.0          -12.0 -23.384398 2016-09-20 16:40:00    16   \n",
       "51             -8.0           18.0  -4.865440 2016-09-20 17:00:00    17   \n",
       "52              8.0            4.0  -6.381846 2016-09-20 17:20:00    17   \n",
       "53             -1.0           -1.0   0.833385 2016-09-20 17:40:00    17   \n",
       "54              5.0           -8.0  16.904615 2016-09-20 18:00:00    18   \n",
       "55             -6.0          -17.0 -12.804286 2016-09-20 18:20:00    18   \n",
       "56              1.0           -4.0  -1.814286 2016-09-20 18:40:00    18   \n",
       "91              3.0            3.0  -5.008333 2016-09-21 06:20:00     6   \n",
       "92              5.0            5.0  -5.490952 2016-09-21 06:40:00     6   \n",
       "93             28.0           28.0  12.001286 2016-09-21 07:00:00     7   \n",
       "94             -5.0           -5.0  62.387000 2016-09-21 07:20:00     7   \n",
       "95              7.0            7.0 -64.485250 2016-09-21 07:40:00     7   \n",
       "96             -7.0           -7.0  -3.638750 2016-09-21 08:00:00     8   \n",
       "97             -7.0           -7.0  11.301250 2016-09-21 08:20:00     8   \n",
       "98             -2.0           36.0  -2.504028 2016-09-21 08:40:00     8   \n",
       "...             ...            ...        ...                 ...   ...   \n",
       "2425            7.0           11.0   3.882564 2016-10-23 16:20:00    16   \n",
       "2426           -8.0          -11.0  -3.626905 2016-10-23 16:40:00    16   \n",
       "2427           16.0           27.0  22.512238 2016-10-23 17:00:00    17   \n",
       "2428          -14.0          -32.0 -26.206444 2016-10-23 17:20:00    17   \n",
       "2429            3.0           -2.0  -4.782991 2016-10-23 17:40:00    17   \n",
       "2430            0.0           -5.0   0.882587 2016-10-23 18:00:00    18   \n",
       "2431           -9.0           -4.0  -3.651818 2016-10-23 18:20:00    18   \n",
       "2432            6.0            5.0   9.121250 2016-10-23 18:40:00    18   \n",
       "2467            5.0           12.0   8.927381 2016-10-24 06:20:00     6   \n",
       "2468            5.0           13.0 -12.947714 2016-10-24 06:40:00     6   \n",
       "2469           24.0           52.0  38.515750 2016-10-24 07:00:00     7   \n",
       "2470           10.0            7.0 -21.650893 2016-10-24 07:20:00     7   \n",
       "2471           -1.0           12.0  24.245143 2016-10-24 07:40:00     7   \n",
       "2472           -6.0           -1.0  -7.692615 2016-10-24 08:00:00     8   \n",
       "2473           -2.0            0.0  14.737473 2016-10-24 08:20:00     8   \n",
       "2474           -4.0          -14.0 -21.995210 2016-10-24 08:40:00     8   \n",
       "2475            3.0            5.0  15.849853 2016-10-24 09:00:00     9   \n",
       "2476          -16.0          -12.0  -2.474500 2016-10-24 09:20:00     9   \n",
       "2477           -1.0          -22.0 -25.688455 2016-10-24 09:40:00     9   \n",
       "2494            9.0            6.0 -43.298586 2016-10-24 15:20:00    15   \n",
       "2495           -4.0           -2.0  -7.185303 2016-10-24 15:40:00    15   \n",
       "2496           -1.0           -2.0  42.616333 2016-10-24 16:00:00    16   \n",
       "2497           -3.0            6.0 -19.589667 2016-10-24 16:20:00    16   \n",
       "2498            5.0            5.0  -8.917222 2016-10-24 16:40:00    16   \n",
       "2499           -3.0           -6.0  26.232222 2016-10-24 17:00:00    17   \n",
       "2500           -1.0          -14.0 -16.053333 2016-10-24 17:20:00    17   \n",
       "2501           11.0            1.0 -15.139167 2016-10-24 17:40:00    17   \n",
       "2502          -10.0           -8.0  11.297500 2016-10-24 18:00:00    18   \n",
       "2503           -4.0           -8.0  -3.789000 2016-10-24 18:20:00    18   \n",
       "2504            2.0           -8.0   1.500250 2016-10-24 18:40:00    18   \n",
       "\n",
       "         pressure  sea_pressure  wind_direction  wind_speed  temperature  \\\n",
       "19    1012.244444   1017.244444        7.444444    4.588889    26.733333   \n",
       "20    1012.288889   1017.288889        7.888889    4.577778    26.566667   \n",
       "21    1012.333333   1017.333333        8.333333    4.566667    26.400000   \n",
       "22    1012.377778   1017.377778        8.777778    4.555556    26.233333   \n",
       "23    1012.422222   1017.422222        9.222222    4.544444    26.066667   \n",
       "24    1012.466667   1017.466667        9.666667    4.533333    25.900000   \n",
       "25    1012.511111   1017.511111       10.111111    4.522222    25.733333   \n",
       "26    1012.555556   1017.555556       10.555556    4.511111    25.566667   \n",
       "27    1012.600000   1017.600000       11.000000    4.500000    25.400000   \n",
       "28    1012.722222   1017.722222       12.555556    4.188889    25.033333   \n",
       "29    1012.844444   1017.844444       14.111111    3.877778    24.666667   \n",
       "46    1013.977778   1018.977778      326.111111    2.111111    20.377778   \n",
       "47    1013.955556   1018.955556      326.222222    2.022222    20.255556   \n",
       "48    1013.933333   1018.933333      326.333333    1.933333    20.133333   \n",
       "49    1013.911111   1018.911111      326.444444    1.844444    20.011111   \n",
       "50    1013.888889   1018.888889      326.555556    1.755556    19.888889   \n",
       "51    1013.866667   1018.866667      326.666667    1.666667    19.766667   \n",
       "52    1013.844444   1018.844444      326.777778    1.577778    19.644444   \n",
       "53    1013.822222   1018.822222      326.888889    1.488889    19.522222   \n",
       "54    1013.800000   1018.800000      327.000000    1.400000    19.400000   \n",
       "55    1013.833333   1018.833333      327.333333    1.355556    19.311111   \n",
       "56    1013.866667   1018.866667      327.666667    1.311111    19.222222   \n",
       "91    1013.111111   1018.111111      330.222222    2.833333    26.055556   \n",
       "92    1013.122222   1018.122222      332.444444    2.666667    25.911111   \n",
       "93    1013.133333   1018.133333      334.666667    2.500000    25.766667   \n",
       "94    1013.144444   1018.144444      336.888889    2.333333    25.622222   \n",
       "95    1013.155556   1018.155556      339.111111    2.166667    25.477778   \n",
       "96    1013.166667   1018.166667      341.333333    2.000000    25.333333   \n",
       "97    1013.177778   1018.177778      343.555556    1.833333    25.188889   \n",
       "98    1013.188889   1018.188889      345.777778    1.666667    25.044444   \n",
       "...           ...           ...             ...         ...          ...   \n",
       "2425  1011.211111   1016.211111      349.703704    3.233333    20.033333   \n",
       "2426  1011.204678   1016.204678      349.769981    3.235088    20.035088   \n",
       "2427  1011.198246   1016.198246      349.836257    3.236842    20.036842   \n",
       "2428  1011.191813   1016.191813      349.902534    3.238596    20.038596   \n",
       "2429  1011.185380   1016.185380      349.968811    3.240351    20.040351   \n",
       "2430  1011.178947   1016.178947      350.035088    3.242105    20.042105   \n",
       "2431  1011.172515   1016.172515      350.101365    3.243860    20.043860   \n",
       "2432  1011.166082   1016.166082      350.167641    3.245614    20.045614   \n",
       "2467  1010.940936   1015.940936      352.487329    3.307018    20.107018   \n",
       "2468  1010.934503   1015.934503      352.553606    3.308772    20.108772   \n",
       "2469  1010.928070   1015.928070      352.619883    3.310526    20.110526   \n",
       "2470  1010.921637   1015.921637      352.686160    3.312281    20.112281   \n",
       "2471  1010.915205   1015.915205      352.752437    3.314035    20.114035   \n",
       "2472  1010.908772   1015.908772      352.818713    3.315789    20.115789   \n",
       "2473  1010.902339   1015.902339      352.884990    3.317544    20.117544   \n",
       "2474  1010.895906   1015.895906      352.951267    3.319298    20.119298   \n",
       "2475  1010.889474   1015.889474      353.017544    3.321053    20.121053   \n",
       "2476  1010.883041   1015.883041      353.083821    3.322807    20.122807   \n",
       "2477  1010.876608   1015.876608      353.150097    3.324561    20.124561   \n",
       "2494  1010.767251   1015.767251      354.276803    3.354386    20.154386   \n",
       "2495  1010.760819   1015.760819      354.343080    3.356140    20.156140   \n",
       "2496  1010.754386   1015.754386      354.409357    3.357895    20.157895   \n",
       "2497  1010.747953   1015.747953      354.475634    3.359649    20.159649   \n",
       "2498  1010.741520   1015.741520      354.541910    3.361404    20.161404   \n",
       "2499  1010.735088   1015.735088      354.608187    3.363158    20.163158   \n",
       "2500  1010.728655   1015.728655      354.674464    3.364912    20.164912   \n",
       "2501  1010.722222   1015.722222      354.740741    3.366667    20.166667   \n",
       "2502  1010.715789   1015.715789      354.807018    3.368421    20.168421   \n",
       "2503  1010.709357   1015.709357      354.873294    3.370175    20.170175   \n",
       "2504  1010.702924   1015.702924      354.939571    3.371930    20.171930   \n",
       "\n",
       "      rel_humidity  precipitation  dayofweek  is_holiday  timeofday  \n",
       "19       42.888889            0.0          1           0   6.333333  \n",
       "20       42.777778            0.0          1           0   6.666667  \n",
       "21       42.666667            0.0          1           0   7.000000  \n",
       "22       42.555556            0.0          1           0   7.333333  \n",
       "23       42.444444            0.0          1           0   7.666667  \n",
       "24       42.333333            0.0          1           0   8.000000  \n",
       "25       42.222222            0.0          1           0   8.333333  \n",
       "26       42.111111            0.0          1           0   8.666667  \n",
       "27       42.000000            0.0          1           0   9.000000  \n",
       "28       43.222222            0.0          1           0   9.333333  \n",
       "29       44.444444            0.0          1           0   9.666667  \n",
       "46       66.000000            0.0          1           0  15.333333  \n",
       "47       67.000000            0.0          1           0  15.666667  \n",
       "48       68.000000            0.0          1           0  16.000000  \n",
       "49       69.000000            0.0          1           0  16.333333  \n",
       "50       70.000000            0.0          1           0  16.666667  \n",
       "51       71.000000            0.0          1           0  17.000000  \n",
       "52       72.000000            0.0          1           0  17.333333  \n",
       "53       73.000000            0.0          1           0  17.666667  \n",
       "54       74.000000            0.0          1           0  18.000000  \n",
       "55       74.666667            0.0          1           0  18.333333  \n",
       "56       75.333333            0.0          1           0  18.666667  \n",
       "91       57.222222            0.0          2           0   6.333333  \n",
       "92       58.444444            0.0          2           0   6.666667  \n",
       "93       59.666667            0.0          2           0   7.000000  \n",
       "94       60.888889            0.0          2           0   7.333333  \n",
       "95       62.111111            0.0          2           0   7.666667  \n",
       "96       63.333333            0.0          2           0   8.000000  \n",
       "97       64.555556            0.0          2           0   8.333333  \n",
       "98       65.777778            0.0          2           0   8.666667  \n",
       "...            ...            ...        ...         ...        ...  \n",
       "2425     86.740741            0.0          6           1  16.333333  \n",
       "2426     86.732943            0.0          6           1  16.666667  \n",
       "2427     86.725146            0.0          6           1  17.000000  \n",
       "2428     86.717349            0.0          6           1  17.333333  \n",
       "2429     86.709552            0.0          6           1  17.666667  \n",
       "2430     86.701754            0.0          6           1  18.000000  \n",
       "2431     86.693957            0.0          6           1  18.333333  \n",
       "2432     86.686160            0.0          6           1  18.666667  \n",
       "2467     86.413255            0.0          0           1   6.333333  \n",
       "2468     86.405458            0.0          0           1   6.666667  \n",
       "2469     86.397661            0.0          0           1   7.000000  \n",
       "2470     86.389864            0.0          0           1   7.333333  \n",
       "2471     86.382066            0.0          0           1   7.666667  \n",
       "2472     86.374269            0.0          0           1   8.000000  \n",
       "2473     86.366472            0.0          0           1   8.333333  \n",
       "2474     86.358674            0.0          0           1   8.666667  \n",
       "2475     86.350877            0.0          0           1   9.000000  \n",
       "2476     86.343080            0.0          0           1   9.333333  \n",
       "2477     86.335283            0.0          0           1   9.666667  \n",
       "2494     86.202729            0.0          0           1  15.333333  \n",
       "2495     86.194932            0.0          0           1  15.666667  \n",
       "2496     86.187135            0.0          0           1  16.000000  \n",
       "2497     86.179337            0.0          0           1  16.333333  \n",
       "2498     86.171540            0.0          0           1  16.666667  \n",
       "2499     86.163743            0.0          0           1  17.000000  \n",
       "2500     86.155945            0.0          0           1  17.333333  \n",
       "2501     86.148148            0.0          0           1  17.666667  \n",
       "2502     86.140351            0.0          0           1  18.000000  \n",
       "2503     86.132554            0.0          0           1  18.333333  \n",
       "2504     86.124756            0.0          0           1  18.666667  \n",
       "\n",
       "[770 rows x 15 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split to train and test set\n",
    "train_rows = sel_rows[: -22*2]\n",
    "test_rows = sel_rows[-22*2:] # reserve 2 days for test\n",
    "\n",
    "# del date column for eliminate datetime\n",
    "del train_rows['date']\n",
    "del test_rows['date']\n",
    "\n",
    "# get numpy array from panda dataframe\n",
    "train_arr = train_rows.values\n",
    "test_arr = test_rows.values\n",
    "\n",
    "# np.shape(train_arr)\n",
    "# Out:\n",
    "# (726, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale feature array to range -1 to 1\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(train_arr)\n",
    "train_scaled_arr = scaler.transform(train_arr)\n",
    "\n",
    "test_scaled_arr = scaler.transform(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07407407,  0.02290076, -0.00362976,  0.83333333,  0.09170719,\n",
       "         0.09125637,  0.98183371, -0.09828989, -0.41798835,  0.58013324,\n",
       "        -1.        , -1.        ,  0.        ,  0.78378378],\n",
       "       [ 0.37037037,  0.2519084 ,  0.0028705 ,  0.83333333,  0.09076042,\n",
       "         0.09032258,  0.98220827, -0.0977918 , -0.41778631,  0.57985421,\n",
       "        -1.        , -1.        ,  0.        ,  0.83783784],\n",
       "       [-0.40740741,  0.11450382,  0.19085073,  1.        ,  0.08981366,\n",
       "         0.08938879,  0.98258283, -0.09729371, -0.41758427,  0.57957518,\n",
       "        -1.        , -1.        ,  0.        ,  0.89189189],\n",
       "       [-0.18518519,  0.11450382,  0.08357685,  1.        ,  0.08886689,\n",
       "         0.08845501,  0.98295739, -0.09679562, -0.41738223,  0.57929615,\n",
       "        -1.        , -1.        ,  0.        ,  0.94594595],\n",
       "       [ 0.03703704,  0.11450382,  0.12118653,  1.        ,  0.08792013,\n",
       "         0.08752122,  0.98333196, -0.09629753, -0.41718019,  0.57901713,\n",
       "        -1.        , -1.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled_arr[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample subsequence from the time series\n",
    "train_seqs = []\n",
    "nSegments = len(train_scaled_arr)//11 # each segment holds 4hr data (12 datapoints, 20min each)\n",
    "for segment in range(nSegments):\n",
    "    for t in range(6):\n",
    "        startIdx = segment*11 + t\n",
    "        train_seqs.append(train_scaled_arr[startIdx: startIdx+6])  # append 6 timestamps each time (5 timestamps for x, 1 timestamp for y)\n",
    "train_seqs = np.stack(train_seqs)\n",
    "\n",
    "test_seqs = []\n",
    "nSegments = len(test_scaled_arr)//11 # each segment holds 4hr data (12 datapoints, 20min each)\n",
    "for segment in range(nSegments):\n",
    "    for t in range(6):\n",
    "        startIdx = segment*11 + t\n",
    "        test_seqs.append(test_scaled_arr[startIdx: startIdx+6])\n",
    "test_seqs = np.stack(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs[:,:,0:6].shape  # checking for Index 0 to index 5 (6 values) for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras\n",
    "#https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "input_dim = len(using_cols) - 1  # The minus 1 is for deleted \"date\" feature\n",
    "output_dim = 3  # 2 vloumns and 1 traffic time output only\n",
    "timesteps = 5 # use 5 timesteps to predict the 6th\n",
    "\n",
    "x_train, y_train = train_seqs[:, 0:-1], train_seqs[:, -1,0:3]  # 0:3 is for output dimensions\n",
    "x_test , y_test  =  test_seqs[:, 0:-1],  test_seqs[:, -1,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "loss_fuc = 'mean_squared_error'\n",
    "\n",
    "# construct the callback\n",
    "filepath=\"best_epoch.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(output_dim))\n",
    "model.compile(loss=loss_fuc, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 5, 128)            73216     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 205,187\n",
      "Trainable params: 205,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 396 samples, validate on 24 samples\n",
      "Epoch 1/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0494Epoch 00000: val_loss improved from inf to 0.04364, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 2s - loss: 0.0494 - val_loss: 0.0436\n",
      "Epoch 2/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0423Epoch 00001: val_loss improved from 0.04364 to 0.04001, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0425 - val_loss: 0.0400\n",
      "Epoch 3/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0407Epoch 00002: val_loss improved from 0.04001 to 0.03822, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0413 - val_loss: 0.0382\n",
      "Epoch 4/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0412Epoch 00003: val_loss improved from 0.03822 to 0.03821, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0409 - val_loss: 0.0382\n",
      "Epoch 5/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0412Epoch 00004: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0405 - val_loss: 0.0387\n",
      "Epoch 6/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0404Epoch 00005: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0404\n",
      "Epoch 7/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0406Epoch 00006: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0403 - val_loss: 0.0398\n",
      "Epoch 8/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0399Epoch 00007: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0395 - val_loss: 0.0394\n",
      "Epoch 9/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0398Epoch 00008: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0394 - val_loss: 0.0390\n",
      "Epoch 10/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0399Epoch 00009: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0394\n",
      "Epoch 11/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0395Epoch 00010: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0391 - val_loss: 0.0394\n",
      "Epoch 12/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0387Epoch 00011: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0388 - val_loss: 0.0390\n",
      "Epoch 13/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0390Epoch 00012: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0384\n",
      "Epoch 14/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0393Epoch 00013: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0389 - val_loss: 0.0384\n",
      "Epoch 15/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0384Epoch 00014: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0398\n",
      "Epoch 16/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0381Epoch 00015: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0394\n",
      "Epoch 17/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0385Epoch 00016: val_loss improved from 0.03821 to 0.03778, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0383 - val_loss: 0.0378\n",
      "Epoch 18/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0389Epoch 00017: val_loss improved from 0.03778 to 0.03714, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0386 - val_loss: 0.0371\n",
      "Epoch 19/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0379Epoch 00018: val_loss improved from 0.03714 to 0.03658, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0366\n",
      "Epoch 20/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0381Epoch 00019: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0369\n",
      "Epoch 21/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0378Epoch 00020: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0374 - val_loss: 0.0372\n",
      "Epoch 22/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0363Epoch 00021: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0373 - val_loss: 0.0370\n",
      "Epoch 23/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0383Epoch 00022: val_loss improved from 0.03658 to 0.03512, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0380 - val_loss: 0.0351\n",
      "Epoch 24/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0371Epoch 00023: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0369\n",
      "Epoch 25/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0370Epoch 00024: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0369 - val_loss: 0.0383\n",
      "Epoch 26/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0375Epoch 00025: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0376 - val_loss: 0.0380\n",
      "Epoch 27/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0362Epoch 00026: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0363 - val_loss: 0.0361\n",
      "Epoch 28/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0363Epoch 00027: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0360 - val_loss: 0.0364\n",
      "Epoch 29/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0355Epoch 00028: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0360 - val_loss: 0.0363\n",
      "Epoch 30/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0354Epoch 00029: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0354 - val_loss: 0.0363\n",
      "Epoch 31/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0351Epoch 00030: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0353 - val_loss: 0.0355\n",
      "Epoch 32/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0356Epoch 00031: val_loss improved from 0.03512 to 0.03415, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0351 - val_loss: 0.0342\n",
      "Epoch 33/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0348Epoch 00032: val_loss improved from 0.03415 to 0.03410, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0347 - val_loss: 0.0341\n",
      "Epoch 34/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0341Epoch 00033: val_loss improved from 0.03410 to 0.03308, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0346 - val_loss: 0.0331\n",
      "Epoch 35/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0344Epoch 00034: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/396 [============================>.] - ETA: 0s - loss: 0.0340Epoch 00035: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0342 - val_loss: 0.0342\n",
      "Epoch 37/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0339Epoch 00036: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0341 - val_loss: 0.0343\n",
      "Epoch 38/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0339Epoch 00037: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0335 - val_loss: 0.0351\n",
      "Epoch 39/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0338Epoch 00038: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0336 - val_loss: 0.0351\n",
      "Epoch 40/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0328Epoch 00039: val_loss improved from 0.03308 to 0.03245, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0326 - val_loss: 0.0324\n",
      "Epoch 41/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0331Epoch 00040: val_loss improved from 0.03245 to 0.03048, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0328 - val_loss: 0.0305\n",
      "Epoch 42/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0320Epoch 00041: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0324 - val_loss: 0.0318\n",
      "Epoch 43/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0319Epoch 00042: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0320 - val_loss: 0.0319\n",
      "Epoch 44/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0317Epoch 00043: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0320 - val_loss: 0.0325\n",
      "Epoch 45/200\n",
      "320/396 [=======================>......] - ETA: 0s - loss: 0.0315Epoch 00044: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0323 - val_loss: 0.0309\n",
      "Epoch 46/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0310Epoch 00045: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0307 - val_loss: 0.0317\n",
      "Epoch 47/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0312Epoch 00046: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0307 - val_loss: 0.0312\n",
      "Epoch 48/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0307Epoch 00047: val_loss improved from 0.03048 to 0.03048, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0303 - val_loss: 0.0305\n",
      "Epoch 49/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0298Epoch 00048: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0295 - val_loss: 0.0323\n",
      "Epoch 50/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0289Epoch 00049: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0293 - val_loss: 0.0309\n",
      "Epoch 51/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0286Epoch 00050: val_loss improved from 0.03048 to 0.02926, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0286 - val_loss: 0.0293\n",
      "Epoch 52/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0288Epoch 00051: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0289 - val_loss: 0.0296\n",
      "Epoch 53/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0286Epoch 00052: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0287 - val_loss: 0.0312\n",
      "Epoch 54/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0280Epoch 00053: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0277 - val_loss: 0.0297\n",
      "Epoch 55/200\n",
      "320/396 [=======================>......] - ETA: 0s - loss: 0.0300Epoch 00054: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0294 - val_loss: 0.0311\n",
      "Epoch 56/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0289Epoch 00055: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0285 - val_loss: 0.0309\n",
      "Epoch 57/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0289Epoch 00056: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0285 - val_loss: 0.0317\n",
      "Epoch 58/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0266Epoch 00057: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0275 - val_loss: 0.0311\n",
      "Epoch 59/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0268Epoch 00058: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0280 - val_loss: 0.0305\n",
      "Epoch 60/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0277Epoch 00059: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0277 - val_loss: 0.0318\n",
      "Epoch 61/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0263Epoch 00060: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0263 - val_loss: 0.0304\n",
      "Epoch 62/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0257Epoch 00061: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0255 - val_loss: 0.0302\n",
      "Epoch 63/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0253Epoch 00062: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0255 - val_loss: 0.0297\n",
      "Epoch 64/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0254Epoch 00063: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0254 - val_loss: 0.0316\n",
      "Epoch 65/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0251Epoch 00064: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0249 - val_loss: 0.0301\n",
      "Epoch 66/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0245Epoch 00065: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0246 - val_loss: 0.0308\n",
      "Epoch 67/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0244Epoch 00066: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0244 - val_loss: 0.0302\n",
      "Epoch 68/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0237Epoch 00067: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0236 - val_loss: 0.0308\n",
      "Epoch 69/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0240Epoch 00068: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0237 - val_loss: 0.0302\n",
      "Epoch 70/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0236Epoch 00069: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0233 - val_loss: 0.0306\n",
      "Epoch 71/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0234Epoch 00070: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0233 - val_loss: 0.0298\n",
      "Epoch 72/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0244Epoch 00071: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0242 - val_loss: 0.0327\n",
      "Epoch 73/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0235Epoch 00072: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0235 - val_loss: 0.0294\n",
      "Epoch 74/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00073: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0235 - val_loss: 0.0310\n",
      "Epoch 75/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0223Epoch 00074: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0224 - val_loss: 0.0304\n",
      "Epoch 76/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0228Epoch 00075: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s - loss: 0.0232 - val_loss: 0.0310\n",
      "Epoch 77/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00076: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0220 - val_loss: 0.0307\n",
      "Epoch 78/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0204Epoch 00077: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0210 - val_loss: 0.0309\n",
      "Epoch 79/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00078: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0213 - val_loss: 0.0347\n",
      "Epoch 80/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00079: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0226 - val_loss: 0.0308\n",
      "Epoch 81/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00080: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0229 - val_loss: 0.0293\n",
      "Epoch 82/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0232Epoch 00081: val_loss improved from 0.02926 to 0.02917, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0234 - val_loss: 0.0292\n",
      "Epoch 83/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00082: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0222 - val_loss: 0.0322\n",
      "Epoch 84/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0214Epoch 00083: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0215 - val_loss: 0.0324\n",
      "Epoch 85/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0203Epoch 00084: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0204 - val_loss: 0.0307\n",
      "Epoch 86/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0205Epoch 00085: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0204 - val_loss: 0.0313\n",
      "Epoch 87/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0197Epoch 00086: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0195 - val_loss: 0.0348\n",
      "Epoch 88/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0191Epoch 00087: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0192 - val_loss: 0.0303\n",
      "Epoch 89/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0188Epoch 00088: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0188 - val_loss: 0.0331\n",
      "Epoch 90/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00089: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0192 - val_loss: 0.0337\n",
      "Epoch 91/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00090: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0200 - val_loss: 0.0315\n",
      "Epoch 92/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0194Epoch 00091: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0193 - val_loss: 0.0303\n",
      "Epoch 93/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00092: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0190 - val_loss: 0.0312\n",
      "Epoch 94/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0191Epoch 00093: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0189 - val_loss: 0.0340\n",
      "Epoch 95/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0191Epoch 00094: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0190 - val_loss: 0.0349\n",
      "Epoch 96/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00095: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0186 - val_loss: 0.0321\n",
      "Epoch 97/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0182Epoch 00096: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0184 - val_loss: 0.0349\n",
      "Epoch 98/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00097: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0209 - val_loss: 0.0314\n",
      "Epoch 99/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00098: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0179 - val_loss: 0.0301\n",
      "Epoch 100/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0178Epoch 00099: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0180 - val_loss: 0.0340\n",
      "Epoch 101/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0177Epoch 00100: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0174 - val_loss: 0.0303\n",
      "Epoch 102/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0171Epoch 00101: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0168 - val_loss: 0.0319\n",
      "Epoch 103/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0162Epoch 00102: val_loss improved from 0.02917 to 0.02860, saving model to best_epoch.hdf5\n",
      "396/396 [==============================] - 0s - loss: 0.0162 - val_loss: 0.0286\n",
      "Epoch 104/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0161Epoch 00103: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0164 - val_loss: 0.0324\n",
      "Epoch 105/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0164Epoch 00104: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0163 - val_loss: 0.0367\n",
      "Epoch 106/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0165Epoch 00105: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0167 - val_loss: 0.0353\n",
      "Epoch 107/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0168Epoch 00106: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0168 - val_loss: 0.0371\n",
      "Epoch 108/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0169Epoch 00107: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0168 - val_loss: 0.0320\n",
      "Epoch 109/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0154Epoch 00108: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0156 - val_loss: 0.0326\n",
      "Epoch 110/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0154Epoch 00109: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0156 - val_loss: 0.0348\n",
      "Epoch 111/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0173Epoch 00110: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0173 - val_loss: 0.0362\n",
      "Epoch 112/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0164Epoch 00111: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0165 - val_loss: 0.0336\n",
      "Epoch 113/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0155Epoch 00112: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0373\n",
      "Epoch 114/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0153Epoch 00113: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0153 - val_loss: 0.0328\n",
      "Epoch 115/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0163Epoch 00114: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0161 - val_loss: 0.0323\n",
      "Epoch 116/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0154Epoch 00115: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0153 - val_loss: 0.0331\n",
      "Epoch 117/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0149Epoch 00116: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s - loss: 0.0149 - val_loss: 0.0302\n",
      "Epoch 118/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0153Epoch 00117: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0309\n",
      "Epoch 119/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0150Epoch 00118: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0149 - val_loss: 0.0293\n",
      "Epoch 120/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0147Epoch 00119: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0144 - val_loss: 0.0343\n",
      "Epoch 121/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0142Epoch 00120: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 122/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0137Epoch 00121: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0137 - val_loss: 0.0342\n",
      "Epoch 123/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0142Epoch 00122: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0141 - val_loss: 0.0317\n",
      "Epoch 124/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0141Epoch 00123: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0140 - val_loss: 0.0350\n",
      "Epoch 125/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0137Epoch 00124: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0137 - val_loss: 0.0308\n",
      "Epoch 126/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0143Epoch 00125: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0142 - val_loss: 0.0363\n",
      "Epoch 127/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0145Epoch 00126: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0145 - val_loss: 0.0353\n",
      "Epoch 128/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0138Epoch 00127: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0137 - val_loss: 0.0319\n",
      "Epoch 129/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0132Epoch 00128: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0133 - val_loss: 0.0357\n",
      "Epoch 130/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0131Epoch 00129: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0131 - val_loss: 0.0323\n",
      "Epoch 131/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0131Epoch 00130: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0131 - val_loss: 0.0316\n",
      "Epoch 132/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0131Epoch 00131: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0131 - val_loss: 0.0351\n",
      "Epoch 133/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0131Epoch 00132: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0130 - val_loss: 0.0316\n",
      "Epoch 134/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0127Epoch 00133: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0129 - val_loss: 0.0338\n",
      "Epoch 135/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0136Epoch 00134: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0136 - val_loss: 0.0308\n",
      "Epoch 136/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0134Epoch 00135: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0137 - val_loss: 0.0302\n",
      "Epoch 137/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0136Epoch 00136: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0136 - val_loss: 0.0318\n",
      "Epoch 138/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0131Epoch 00137: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0132 - val_loss: 0.0373\n",
      "Epoch 139/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0135Epoch 00138: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0133 - val_loss: 0.0334\n",
      "Epoch 140/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0122Epoch 00139: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0123 - val_loss: 0.0328\n",
      "Epoch 141/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0117Epoch 00140: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0116 - val_loss: 0.0332\n",
      "Epoch 142/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0117Epoch 00141: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0116 - val_loss: 0.0369\n",
      "Epoch 143/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0117Epoch 00142: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0118 - val_loss: 0.0347\n",
      "Epoch 144/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0112Epoch 00143: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0113 - val_loss: 0.0345\n",
      "Epoch 145/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0110Epoch 00144: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0361\n",
      "Epoch 146/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00145: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0353\n",
      "Epoch 147/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0106Epoch 00146: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0106 - val_loss: 0.0346\n",
      "Epoch 148/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0113Epoch 00147: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0339\n",
      "Epoch 149/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0107Epoch 00148: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0337\n",
      "Epoch 150/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0106Epoch 00149: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0105 - val_loss: 0.0323\n",
      "Epoch 151/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00150: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0359\n",
      "Epoch 152/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0104Epoch 00151: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0365\n",
      "Epoch 153/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0103Epoch 00152: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0102 - val_loss: 0.0327\n",
      "Epoch 154/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00153: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0102 - val_loss: 0.0342\n",
      "Epoch 155/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0104Epoch 00154: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0340\n",
      "Epoch 156/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0099Epoch 00155: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0342\n",
      "Epoch 157/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00156: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0335\n",
      "Epoch 158/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00157: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0095 - val_loss: 0.0327\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/396 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00158: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0350\n",
      "Epoch 160/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00159: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0341\n",
      "Epoch 161/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00160: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0336\n",
      "Epoch 162/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0100Epoch 00161: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0366\n",
      "Epoch 163/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0093Epoch 00162: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0369\n",
      "Epoch 164/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00163: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0372\n",
      "Epoch 165/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00164: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0357\n",
      "Epoch 166/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00165: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0095 - val_loss: 0.0333\n",
      "Epoch 167/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00166: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0410\n",
      "Epoch 168/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00167: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0336\n",
      "Epoch 169/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00168: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0097 - val_loss: 0.0333\n",
      "Epoch 170/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00169: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0098 - val_loss: 0.0329\n",
      "Epoch 171/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0093Epoch 00170: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0384\n",
      "Epoch 172/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00171: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0087 - val_loss: 0.0324\n",
      "Epoch 173/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00172: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0087 - val_loss: 0.0328\n",
      "Epoch 174/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00173: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0078 - val_loss: 0.0354\n",
      "Epoch 175/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00174: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0340\n",
      "Epoch 176/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00175: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0348\n",
      "Epoch 177/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00176: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0340\n",
      "Epoch 178/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00177: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0356\n",
      "Epoch 179/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00178: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0344\n",
      "Epoch 180/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00179: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0379\n",
      "Epoch 181/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00180: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0323\n",
      "Epoch 182/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00181: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0348\n",
      "Epoch 183/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00182: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0071 - val_loss: 0.0315\n",
      "Epoch 184/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00183: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0316\n",
      "Epoch 185/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00184: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0357\n",
      "Epoch 186/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00185: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0350\n",
      "Epoch 187/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00186: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0070 - val_loss: 0.0304\n",
      "Epoch 188/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00187: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0369\n",
      "Epoch 189/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00188: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0339\n",
      "Epoch 190/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00189: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0335\n",
      "Epoch 191/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00190: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0071 - val_loss: 0.0294\n",
      "Epoch 192/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00191: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0070 - val_loss: 0.0324\n",
      "Epoch 193/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00192: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0369\n",
      "Epoch 194/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00193: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0300\n",
      "Epoch 195/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0064Epoch 00194: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0064 - val_loss: 0.0326\n",
      "Epoch 196/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00195: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0317\n",
      "Epoch 197/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0057Epoch 00196: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0057 - val_loss: 0.0395\n",
      "Epoch 198/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0059Epoch 00197: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0319\n",
      "Epoch 199/200\n",
      "384/396 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00198: val_loss did not improve\n",
      "396/396 [==============================] - 0s - loss: 0.0063 - val_loss: 0.0338\n",
      "Epoch 200/200\n",
      "320/396 [=======================>......] - ETA: 0s - loss: 0.0055Epoch 00199: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s - loss: 0.0055 - val_loss: 0.0314\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGHCAYAAACnPchFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqTOM6GRJEmdZ0IjSZI6z4RGkiR13n8Btnw+NKYobisAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129c79be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Start Training\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=64, validation_data=(x_test,y_test))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into the model\n",
    "model.load_weights(\"best_epoch.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:31: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "''' === Prediction ===\n",
    "Procedure:\n",
    "1. Load CSV\n",
    "2. to_datetime\n",
    "3. create timeofday column\n",
    "4. select the time for training: 6:00-8:00 (6 timestamps) and 15:00-17:00 (6 timestamps)\n",
    "5. change it to stationary\n",
    "6. Use using_cols to select the features\n",
    "7. del 'date'\n",
    "8. change to np array\n",
    "9. MinMaxScaler\n",
    "10. make the sequences tensor as input\n",
    "11. make a forloop for prediction\n",
    "\n",
    "'''\n",
    "# 1. Load CSV - Vol + Route + Weather (Only Weather is 24-hour data)\n",
    "df_pred = pd.read_csv('../data/preprocessed_input_interpolate_20min_phase2_test.csv')\n",
    "\n",
    "# 2. to_datetime\n",
    "df_pred['date'] = pd.to_datetime(df_pred['date'])\n",
    "\n",
    "# 3. create timeofday column\n",
    "df_pred['timeofday'] = df_pred.date.apply( lambda d : d.hour+d.minute/60.)\n",
    "\n",
    "# 4. select the time for training\n",
    "\n",
    "df_pred_sel_time = df_pred[ ((df_merged_volume_copy.timeofday>= 6) & (df_merged_volume_copy.timeofday<8)) |\n",
    "                            ((df_merged_volume_copy.timeofday>=15) & (df_merged_volume_copy.timeofday<17))]\n",
    "\n",
    "df_feedin_weather_sel_time = df_pred[ ((df_merged_volume_copy.timeofday>= 8) & (df_merged_volume_copy.timeofday<10)) |\n",
    "                            ((df_merged_volume_copy.timeofday>=17) & (df_merged_volume_copy.timeofday<19))]\n",
    "\n",
    "# 5. change it to stationary\n",
    "df_pred_sel_time = df_pred_sel_time.reset_index(drop=True)\n",
    "\n",
    "df_pred_sel_time_copy = df_pred_sel_time.copy()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df_pred_sel_time_copy)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        start_idx = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot\n",
    "        df_pred_sel_time_copy.loc[start_idx, df_pred_sel_time_copy.columns[0:36]] = df_pred_sel_time.loc[start_idx, df_pred_sel_time.columns[0:36]] - df_pred_sel_time.loc[start_idx-1, df_pred_sel_time.columns[0:36]]\n",
    "\n",
    "# 6. Use using_cols to select the features\n",
    "sel_rows_pred = df_pred_sel_time_copy[ using_cols ]\n",
    "\n",
    "sel_rows_feedin_weather = df_feedin_weather_sel_time[using_cols[3:]]\n",
    "\n",
    "# 7. del 'date'\n",
    "del sel_rows_pred['date']\n",
    "del sel_rows_feedin_weather['date']\n",
    "\n",
    "# 8. change to np array\n",
    "pred_arr = sel_rows_pred.values\n",
    "\n",
    "feedin_weather_arr = sel_rows_feedin_weather.values\n",
    "\n",
    "# 9. MinMaxScaler\n",
    "pred_arr_scaled = scaler.transform(pred_arr)\n",
    "\n",
    "# add some dummy cells in front of the weather_array for transform\n",
    "temp_arr = np.zeros((84,3))\n",
    "feedin_weather_arr = np.concatenate([temp_arr, feedin_weather_arr], axis=1)\n",
    "\n",
    "feedin_weather_arr_scaled = scaler.transform(feedin_weather_arr)\n",
    "\n",
    "# Now pred_arr_scaled is (84 x 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10. make the sequences tensor as input\n",
    "# Put into the model to get the prediction\n",
    "\n",
    "ans_arr = []  # For holding the output answer\n",
    "    \n",
    "for i in range(len(pred_arr_scaled)//6):  # make the loop for 14 time slots (2 different time slot x 7days)\n",
    "    # creating pre_seq\n",
    "    pred_seq = []\n",
    "    for t in range(5):  #  Do the \"difference\" 5 times every loop\n",
    "        k = i*6 + t + 1  # Add 1 is for starting it from index 1 in every 6-space time slot, to ignore the first index which is non-stationary\n",
    "        pred_seq.append(pred_arr_scaled[k])  # creating a sequence for a time slot\n",
    "    \n",
    "    # creating feedin_weather_seq\n",
    "    feedin_weather_seq = []\n",
    "    for t in range(6):  #  Do 6 times every loop\n",
    "        k = i*6 + t  #\n",
    "        feedin_weather_seq.append(feedin_weather_arr_scaled[k])\n",
    "\n",
    "\n",
    "    pred_seq = np.stack(pred_seq)  # change back to the numpy array (2D)\n",
    "    pred_seq = pred_seq.reshape(1, pred_seq.shape[0], pred_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    feedin_weather_seq = np.stack(feedin_weather_seq)  # change back to the numpy array (2D)\n",
    "    feedin_weather_seq = feedin_weather_seq.reshape(1, feedin_weather_seq.shape[0], feedin_weather_seq.shape[1])  # change to numpy 3D as input\n",
    "\n",
    "    for q in range(6):\n",
    "        # predict next timestamp\n",
    "        output_pred = model.predict(pred_seq)  # get one prediction output (size (1 x 3))\n",
    "        ans_arr.append(output_pred)\n",
    "\n",
    "        # update the input seq\n",
    "        for j in range(1,5):\n",
    "            pred_seq[0][j-1] = pred_seq[0][j]\n",
    "        pred_seq[0][4] = feedin_weather_seq[0][q]\n",
    "        pred_seq[0][4][0:3] = output_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09178533,  0.57434142,  0.27616766]], dtype=float32),\n",
       " array([[ 0.0100132 ,  0.04146061,  0.11796708]], dtype=float32),\n",
       " array([[-0.02719644,  0.23052379,  0.06897746]], dtype=float32),\n",
       " array([[-0.57010305, -0.14958984, -0.04910365]], dtype=float32),\n",
       " array([[-0.28493717, -0.03851455,  0.02867013]], dtype=float32),\n",
       " array([[ 0.01766457,  0.18894008,  0.05384346]], dtype=float32),\n",
       " array([[ 0.23889208,  0.17483574,  0.01915436]], dtype=float32),\n",
       " array([[-0.01182819,  0.10884801,  0.07484879]], dtype=float32),\n",
       " array([[ 0.03395049,  0.10900519, -0.0029909 ]], dtype=float32),\n",
       " array([[ 0.02008612,  0.09588168,  0.04419097]], dtype=float32),\n",
       " array([[ 0.046321  ,  0.08538169,  0.03013395]], dtype=float32),\n",
       " array([[-0.0008876 ,  0.05869978,  0.04465823]], dtype=float32),\n",
       " array([[-0.14282978,  0.42686313,  0.40127596]], dtype=float32),\n",
       " array([[-0.30298802,  0.20914447,  0.10426994]], dtype=float32),\n",
       " array([[-0.1298926 ,  0.18148777,  0.15711537]], dtype=float32),\n",
       " array([[ 0.08492962,  0.32689622,  0.11559489]], dtype=float32),\n",
       " array([[-0.25345138,  0.08329976,  0.1209757 ]], dtype=float32),\n",
       " array([[-0.35563394, -0.0761013 ,  0.12202836]], dtype=float32),\n",
       " array([[-0.0306238 ,  0.09385181,  0.01263735]], dtype=float32),\n",
       " array([[-0.03784174,  0.17647186,  0.19937816]], dtype=float32),\n",
       " array([[ 0.00437816,  0.18468633,  0.06862709]], dtype=float32),\n",
       " array([[-0.10713955,  0.104364  ,  0.14281933]], dtype=float32),\n",
       " array([[-0.0132603 ,  0.19370738,  0.1052229 ]], dtype=float32),\n",
       " array([[-0.10415202,  0.0959332 ,  0.11977381]], dtype=float32),\n",
       " array([[-0.5356583 ,  0.29780462,  0.16987905]], dtype=float32),\n",
       " array([[ 0.21757057,  0.31977639,  0.19429654]], dtype=float32),\n",
       " array([[-0.36645874,  0.11728866, -0.0279088 ]], dtype=float32),\n",
       " array([[-0.13188136,  0.02990691,  0.11092193]], dtype=float32),\n",
       " array([[-0.30098101,  0.04294401,  0.0408162 ]], dtype=float32),\n",
       " array([[-0.29416978, -0.06066681,  0.02565082]], dtype=float32),\n",
       " array([[-0.34567511,  0.07353421,  0.0669032 ]], dtype=float32),\n",
       " array([[-0.01312623,  0.25592616,  0.15313557]], dtype=float32),\n",
       " array([[ 0.06834196,  0.24617478,  0.12416315]], dtype=float32),\n",
       " array([[-0.12036801,  0.12205093,  0.10670263]], dtype=float32),\n",
       " array([[-0.00703186,  0.21439394,  0.12577829]], dtype=float32),\n",
       " array([[ 0.0099214 ,  0.25822747,  0.12581706]], dtype=float32),\n",
       " array([[-0.28100309,  0.17952642,  0.26976722]], dtype=float32),\n",
       " array([[ 0.01772287,  0.09446777,  0.21777409]], dtype=float32),\n",
       " array([[ 0.24946448,  0.22901961,  0.04312609]], dtype=float32),\n",
       " array([[ 0.0669027 ,  0.14499018,  0.01808264]], dtype=float32),\n",
       " array([[-0.4380919 , -0.03057199, -0.07414319]], dtype=float32),\n",
       " array([[-0.10310603,  0.27042493,  0.08037572]], dtype=float32),\n",
       " array([[ 0.08107265,  0.00015341,  0.07118311]], dtype=float32),\n",
       " array([[-0.06281249,  0.1274305 ,  0.13345382]], dtype=float32),\n",
       " array([[-0.23303339,  0.13896142,  0.18105829]], dtype=float32),\n",
       " array([[-0.13469636,  0.13752416,  0.16941994]], dtype=float32),\n",
       " array([[-0.09878879,  0.09746078,  0.17847478]], dtype=float32),\n",
       " array([[ 0.09654842,  0.1645115 ,  0.11958938]], dtype=float32),\n",
       " array([[-0.62800175, -0.10086133,  0.07958767]], dtype=float32),\n",
       " array([[-0.6148051 ,  0.00469426,  0.06452361]], dtype=float32),\n",
       " array([[ 0.00819221,  0.22795221,  0.10819078]], dtype=float32),\n",
       " array([[ 0.1625796 ,  0.18322638,  0.02943975]], dtype=float32),\n",
       " array([[-0.21412948,  0.00496388, -0.061969  ]], dtype=float32),\n",
       " array([[-0.51806724, -0.03021087, -0.08122865]], dtype=float32),\n",
       " array([[-0.14374179,  0.18504256,  0.27669537]], dtype=float32),\n",
       " array([[-0.2102918 ,  0.13609582,  0.25394282]], dtype=float32),\n",
       " array([[-0.06504644,  0.21139017,  0.21234807]], dtype=float32),\n",
       " array([[-0.05743138,  0.16844055,  0.19956937]], dtype=float32),\n",
       " array([[-0.09873379,  0.16388309,  0.2035839 ]], dtype=float32),\n",
       " array([[-0.09756079,  0.18824685,  0.1905812 ]], dtype=float32),\n",
       " array([[-0.14369038,  0.20865342,  0.14992419]], dtype=float32),\n",
       " array([[-0.01168874,  0.2519286 ,  0.1569263 ]], dtype=float32),\n",
       " array([[-0.02567058,  0.21058637,  0.11509376]], dtype=float32),\n",
       " array([[-0.0427242 ,  0.17773071,  0.10853066]], dtype=float32),\n",
       " array([[-0.13772294,  0.12371831,  0.1035478 ]], dtype=float32),\n",
       " array([[-0.19929397,  0.07306884,  0.08414595]], dtype=float32),\n",
       " array([[-0.25356108,  0.0338751 ,  0.12830761]], dtype=float32),\n",
       " array([[-0.04701651,  0.146881  ,  0.14948937]], dtype=float32),\n",
       " array([[-0.04354258,  0.15602416,  0.11572136]], dtype=float32),\n",
       " array([[-0.20161864,  0.04311949,  0.10951749]], dtype=float32),\n",
       " array([[-0.03612093,  0.1945897 ,  0.1139325 ]], dtype=float32),\n",
       " array([[-0.12758288,  0.12466409,  0.09919359]], dtype=float32),\n",
       " array([[-0.60943168, -0.01838681,  0.35427299]], dtype=float32),\n",
       " array([[-0.03698568,  0.22958437,  0.18964449]], dtype=float32),\n",
       " array([[-0.05831598,  0.00336836,  0.04707788]], dtype=float32),\n",
       " array([[-0.07524402,  0.11117286,  0.16785964]], dtype=float32),\n",
       " array([[-0.38966399,  0.00901246,  0.24287254]], dtype=float32),\n",
       " array([[-0.42543486, -0.17462984,  0.12801063]], dtype=float32),\n",
       " array([[-0.030252  , -0.03766977,  0.1889703 ]], dtype=float32),\n",
       " array([[-0.05111726,  0.0419236 ,  0.17308018]], dtype=float32),\n",
       " array([[-0.15334329,  0.01916544,  0.16791975]], dtype=float32),\n",
       " array([[-0.04008542,  0.08919666,  0.13430148]], dtype=float32),\n",
       " array([[-0.18042547,  0.00339655,  0.15801114]], dtype=float32),\n",
       " array([[-0.07684187,  0.10244059,  0.14945364]], dtype=float32)]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Problems, in row 48 (= Day 5)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ -1.47820382  22.11936277  23.29607761]\n",
      "1 [  1.27035641 -12.78432974   1.04748131]\n",
      "2 [ 0.2656961  -0.40069143 -5.84219108]\n",
      "3 [-14.39278233 -25.2981343  -22.44857135]\n",
      "4 [ -6.69330367 -18.02270283 -11.51082672]\n",
      "5 [ 1.4769434  -3.12442489 -7.97056653]\n",
      "6 [  7.45008612  -4.04825893 -12.84908018]\n",
      "7 [ 0.68063893 -8.37045564 -5.01647506]\n",
      "8 [  1.9166631   -8.36016003 -15.96348762]\n",
      "9 [ 1.54232516 -9.21975008 -9.32804871]\n",
      "10 [  2.25066692  -9.90749903 -11.30496197]\n",
      "11 [  0.97603493 -11.65516422  -9.26233498]\n",
      "12 [ -2.85640395  12.45953527  40.89072879]\n",
      "13 [-7.18067661 -1.80103701 -0.87881987]\n",
      "14 [-2.50710027 -3.61255114  6.55311569]\n",
      "15 [ 3.29309961  5.91170244  0.71386746]\n",
      "16 [ -5.84318718 -10.04386549   1.47060057]\n",
      "17 [ -8.6021165  -20.48463487   1.61864112]\n",
      "18 [  0.17315727  -9.3527063  -13.76560247]\n",
      "19 [ -0.02172691  -3.94109322  12.49675879]\n",
      "20 [ 1.11821038 -3.40304519 -5.89146563]\n",
      "21 [-1.89276785 -8.66415798  4.54258695]\n",
      "22 [ 0.64197197 -2.81216682 -0.74480117]\n",
      "23 [-1.81210464 -9.21637547  1.30157227]\n",
      "24 [-13.4627741    4.00620286   8.3481403 ]\n",
      "25 [  6.87440547   5.44535328  11.78210279]\n",
      "26 [ -8.89438608  -7.817593   -19.46782504]\n",
      "27 [ -2.56079662 -13.54109728   0.05668517]\n",
      "28 [ -7.1264874  -12.68716757  -9.80266022]\n",
      "29 [ -6.94258416 -19.47367586 -11.93544855]\n",
      "30 [ -8.33322799 -10.68350905  -6.13390594]\n",
      "31 [ 0.64559184  1.26316361  5.99341478]\n",
      "32 [ 2.845233    0.62444825  1.91886844]\n",
      "33 [-2.2499363  -7.50566434 -0.53669776]\n",
      "34 [ 0.81013983 -1.4571967   2.14601379]\n",
      "35 [ 1.2678778   1.41389912  2.15146662]\n",
      "36 [ -6.58708337  -3.74101959  22.39594996]\n",
      "37 [  1.47851752  -9.31236129  15.08387712]\n",
      "38 [ 7.73554102 -0.49921541 -9.47780768]\n",
      "39 [  2.80637302  -6.00314347 -12.99980236]\n",
      "40 [-10.82848141 -17.50246533 -25.97001593]\n",
      "41 [-1.78386279  2.21283306 -4.23919221]\n",
      "42 [  3.18896157 -15.48995132  -5.53199904]\n",
      "43 [-0.69593729 -7.15330234  3.22546452]\n",
      "44 [-5.2919015  -6.39802703  9.92033655]\n",
      "45 [-2.63680184 -6.49216767  8.28357393]\n",
      "46 [-1.66729734 -9.11631865  9.55700457]\n",
      "47 [ 3.60680722 -4.72449663  1.27563466]\n",
      "48 [-15.95604724 -22.10641733  -4.35002066]\n",
      "49 [-15.59973776 -15.19252593  -6.46855922]\n",
      "50 [ 1.22118961 -0.56913011 -0.32741136]\n",
      "51 [  5.38964909  -3.49867232 -11.40259057]\n",
      "52 [ -4.7814959  -15.17486571 -24.2578934 ]\n",
      "53 [-12.9878155  -17.47881231 -26.96648179]\n",
      "54 [ -2.88102823  -3.37971231  23.37029226]\n",
      "55 [ -4.67787868  -6.58572367  20.17047872]\n",
      "56 [ -0.756254    -1.65394403  14.32078692]\n",
      "57 [ -0.55064729  -4.46714394  12.52364991]\n",
      "58 [ -1.66581234  -4.7656576   13.08823364]\n",
      "59 [ -1.63414142  -3.16983157  11.25959512]\n",
      "60 [-2.87964019 -1.83320098  5.54178133]\n",
      "61 [ 0.68440405  1.00132315  6.52652633]\n",
      "62 [ 0.30689427 -1.70659283  0.64339142]\n",
      "63 [-0.15355339 -3.85863854 -0.27961318]\n",
      "64 [-2.71851936 -7.39645044 -0.9803783 ]\n",
      "65 [ -4.38093722 -10.71399083  -3.70896529]\n",
      "66 [ -5.84614915 -13.28118069   2.50172606]\n",
      "67 [-0.26944574 -5.87929455  5.48063084]\n",
      "68 [-0.17564963 -5.28041765  0.73165412]\n",
      "69 [ -4.44370332 -12.6756734   -0.14082903]\n",
      "70 [ 0.02473492 -2.75437437  0.48007686]\n",
      "71 [-2.4447377  -7.33450207 -1.59273506]\n",
      "72 [-15.45465547 -16.70433612  34.28044897]\n",
      "73 [  1.38672814e-03  -4.62224022e-01   1.11278596e+01]\n",
      "74 [ -0.57453159 -15.27937224  -8.92204655]\n",
      "75 [-1.03158846 -8.21817752  8.0641405 ]\n",
      "76 [ -9.52092785 -14.90968382  18.61360572]\n",
      "77 [-10.48674116 -26.93825434   2.45996015]\n",
      "78 [  0.18319594 -17.96736996  11.03304493]\n",
      "79 [ -0.38016612 -12.75400388   8.79833196]\n",
      "80 [ -3.14026883 -14.24466359   8.07259427]\n",
      "81 [-0.08230634 -9.65761879  3.34467654]\n",
      "82 [ -3.87148756 -15.27752584   6.6790924 ]\n",
      "83 [-1.07473045 -8.79014147  5.47560552]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ans_arr)):\n",
    "    print(i, backward_scaler(ans_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_scaler(nn_output):\n",
    "    tmp = np.zeros(14)\n",
    "    tmp[0:3] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return tmp[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(last_timestamp_values, nn_output):\n",
    "    tmp = np.zeros(14)\n",
    "    tmp[0:3] = nn_output\n",
    "    tmp = scaler.inverse_transform(tmp)\n",
    "    return last_timestamp_values + tmp[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# create the non-stationary 6:40 and 16:40 for decoding\n",
    "df_non_station_sel_time = df_pred[ ((df_merged_volume_copy.timeofday>= 7.5) & (df_merged_volume_copy.timeofday<8)) |\n",
    "                            ((df_merged_volume_copy.timeofday>=16.5) & (df_merged_volume_copy.timeofday<17))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 'etc')    19.000000\n",
       "(2, 0, 'tot')    61.000000\n",
       "('A', 2)         53.751765\n",
       "Name: 311, dtype: float64"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_station_sel_time[using_cols[0:3]].iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Kin/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "tmp = df_non_station_sel_time[using_cols[0:3]].values\n",
    "allAns = []\n",
    "for i in range(len(tmp)):\n",
    "    seed = tmp[i]  # non-stationary for reconstructing a sequence\n",
    "    segmentAns = []\n",
    "    for timestep in range(6):\n",
    "        seed = decode(seed, ans_arr[i*6+timestep])\n",
    "        segmentAns.append(seed)\n",
    "    allAns.append(segmentAns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  3.65217962e+01,   1.15119363e+02,   8.38948276e+01],\n",
       "        [  3.77921526e+01,   1.02335033e+02,   8.49423089e+01],\n",
       "        [  3.80578487e+01,   1.01934342e+02,   7.91001178e+01],\n",
       "        [  2.36650664e+01,   7.66362073e+01,   5.66515465e+01],\n",
       "        [  1.69717627e+01,   5.86135045e+01,   4.51407198e+01],\n",
       "        [  1.84487061e+01,   5.54890796e+01,   3.71701532e+01]],\n",
       "\n",
       "       [[  2.44500861e+01,   6.49517411e+01,   5.63886976e+01],\n",
       "        [  2.51307250e+01,   5.65812854e+01,   5.13722225e+01],\n",
       "        [  2.70473881e+01,   4.82211254e+01,   3.54087349e+01],\n",
       "        [  2.85897133e+01,   3.90013753e+01,   2.60806862e+01],\n",
       "        [  3.08403802e+01,   2.90938763e+01,   1.47757242e+01],\n",
       "        [  3.18164152e+01,   1.74387121e+01,   5.51338926e+00]],\n",
       "\n",
       "       [[  3.91435961e+01,   1.11459535e+02,   8.74340621e+01],\n",
       "        [  3.19629194e+01,   1.09658498e+02,   8.65552423e+01],\n",
       "        [  2.94558192e+01,   1.06045947e+02,   9.31083580e+01],\n",
       "        [  3.27489188e+01,   1.11957650e+02,   9.38222254e+01],\n",
       "        [  2.69057316e+01,   1.01913784e+02,   9.52928260e+01],\n",
       "        [  1.83036151e+01,   8.14291492e+01,   9.69114671e+01]],\n",
       "\n",
       "       [[  2.31731573e+01,   8.06472937e+01,   5.72472547e+01],\n",
       "        [  2.31514304e+01,   7.67062005e+01,   6.97440135e+01],\n",
       "        [  2.42696407e+01,   7.33031553e+01,   6.38525478e+01],\n",
       "        [  2.23768729e+01,   6.46389973e+01,   6.83951348e+01],\n",
       "        [  2.30188449e+01,   6.18268305e+01,   6.76503336e+01],\n",
       "        [  2.12067402e+01,   5.26104550e+01,   6.89519059e+01]],\n",
       "\n",
       "       [[  2.75372259e+01,   1.18006203e+02,   8.88650634e+01],\n",
       "        [  3.44116314e+01,   1.23451556e+02,   1.00647166e+02],\n",
       "        [  2.55172453e+01,   1.15633963e+02,   8.11793411e+01],\n",
       "        [  2.29564487e+01,   1.02092866e+02,   8.12360263e+01],\n",
       "        [  1.58299613e+01,   8.94056983e+01,   7.14333661e+01],\n",
       "        [  8.88737711e+00,   6.99320224e+01,   5.94979175e+01]],\n",
       "\n",
       "       [[  2.06667720e+01,   7.23164910e+01,   6.59353798e+01],\n",
       "        [  2.13123639e+01,   7.35796546e+01,   7.19287946e+01],\n",
       "        [  2.41575969e+01,   7.42041028e+01,   7.38476630e+01],\n",
       "        [  2.19076605e+01,   6.66984385e+01,   7.33109652e+01],\n",
       "        [  2.27178004e+01,   6.52412418e+01,   7.54569790e+01],\n",
       "        [  2.39856782e+01,   6.66551409e+01,   7.76084456e+01]],\n",
       "\n",
       "       [[  2.94129166e+01,   9.32589804e+01,   8.75692833e+01],\n",
       "        [  3.08914342e+01,   8.39466191e+01,   1.02653160e+02],\n",
       "        [  3.86269752e+01,   8.34474037e+01,   9.31753527e+01],\n",
       "        [  4.14333482e+01,   7.74442602e+01,   8.01755504e+01],\n",
       "        [  3.06048668e+01,   5.99417949e+01,   5.42055345e+01],\n",
       "        [  2.88210040e+01,   6.21546280e+01,   4.99663422e+01]],\n",
       "\n",
       "       [[  3.31889616e+01,   9.45100487e+01,   8.31972602e+01],\n",
       "        [  3.24930243e+01,   8.73567463e+01,   8.64227247e+01],\n",
       "        [  2.72011228e+01,   8.09587193e+01,   9.63430613e+01],\n",
       "        [  2.45643209e+01,   7.44665516e+01,   1.04626635e+02],\n",
       "        [  2.28970236e+01,   6.53502330e+01,   1.14183640e+02],\n",
       "        [  2.65038308e+01,   6.06257364e+01,   1.15459274e+02]],\n",
       "\n",
       "       [[  3.04395276e+00,   3.88935827e+01,   4.94017440e+01],\n",
       "        [ -1.25557850e+01,   2.37010567e+01,   4.29331848e+01],\n",
       "        [ -1.13345954e+01,   2.31319266e+01,   4.26057735e+01],\n",
       "        [ -5.94494630e+00,   1.96332543e+01,   3.12031829e+01],\n",
       "        [ -1.07264422e+01,   4.45838860e+00,   6.94528950e+00],\n",
       "        [ -2.37142577e+01,  -1.30204237e+01,  -2.00211923e+01]],\n",
       "\n",
       "       [[  1.81189718e+01,   7.96202877e+01,   9.27908805e+01],\n",
       "        [  1.34410931e+01,   7.30345640e+01,   1.12961359e+02],\n",
       "        [  1.26848391e+01,   7.13806200e+01,   1.27282146e+02],\n",
       "        [  1.21341918e+01,   6.69134760e+01,   1.39805796e+02],\n",
       "        [  1.04683795e+01,   6.21478184e+01,   1.52894030e+02],\n",
       "        [  8.83423804e+00,   5.89779869e+01,   1.64153625e+02]],\n",
       "\n",
       "       [[  1.31203598e+01,   1.41667990e+01,   5.66472359e+01],\n",
       "        [  1.38047639e+01,   1.51681222e+01,   6.31737622e+01],\n",
       "        [  1.41116581e+01,   1.34615293e+01,   6.38171536e+01],\n",
       "        [  1.39581047e+01,   9.60289080e+00,   6.35375404e+01],\n",
       "        [  1.12395854e+01,   2.20644036e+00,   6.25571621e+01],\n",
       "        [  6.85864816e+00,  -8.50755047e+00,   5.88481969e+01]],\n",
       "\n",
       "       [[  1.31538509e+01,   5.71881931e+00,   6.63524953e+01],\n",
       "        [  1.28844051e+01,  -1.60475237e-01,   7.18331261e+01],\n",
       "        [  1.27087555e+01,  -5.44089289e+00,   7.25647802e+01],\n",
       "        [  8.26505216e+00,  -1.81165663e+01,   7.24239512e+01],\n",
       "        [  8.28978708e+00,  -2.08709407e+01,   7.29040281e+01],\n",
       "        [  5.84504938e+00,  -2.82054427e+01,   7.13112930e+01]],\n",
       "\n",
       "       [[  4.25453445e+01,   4.12956639e+01,   9.80215601e+01],\n",
       "        [  4.25467313e+01,   4.08334399e+01,   1.09149420e+02],\n",
       "        [  4.19721997e+01,   2.55540676e+01,   1.00227373e+02],\n",
       "        [  4.09406112e+01,   1.73358901e+01,   1.08291514e+02],\n",
       "        [  3.14196834e+01,   2.42620628e+00,   1.26905119e+02],\n",
       "        [  2.09329422e+01,  -2.45120481e+01,   1.29365080e+02]],\n",
       "\n",
       "       [[  1.71831959e+01,  -9.67369962e-01,   6.37570449e+01],\n",
       "        [  1.68030298e+01,  -1.37213738e+01,   7.25553769e+01],\n",
       "        [  1.36627610e+01,  -2.79660374e+01,   8.06279712e+01],\n",
       "        [  1.35804546e+01,  -3.76236562e+01,   8.39726477e+01],\n",
       "        [  9.70896709e+00,  -5.29011821e+01,   9.06517401e+01],\n",
       "        [  8.63423664e+00,  -6.16913235e+01,   9.61273456e+01]]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allAns = np.array(allAns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  36.52179618  115.11936277   83.89482761]\n",
      " [  37.79215259  102.33503303   84.94230892]\n",
      " [  38.05784869  101.9343416    79.10011784]\n",
      " [  23.66506636   76.6362073    56.65154649]\n",
      " [  16.97176268   58.61350447   45.14071977]\n",
      " [  18.44870608   55.48907958   37.17015323]]\n",
      "[[ 24.45008612  64.95174107  56.3886976 ]\n",
      " [ 25.13072504  56.58128543  51.37222254]\n",
      " [ 27.04738815  48.22112541  35.40873492]\n",
      " [ 28.58971331  39.00137533  26.08068621]\n",
      " [ 30.84038023  29.09387629  14.77572424]\n",
      " [ 31.81641516  17.43871207   5.51338926]]\n",
      "[[  39.14359605  111.45953527   87.43406213]\n",
      " [  31.96291944  109.65849826   86.55524226]\n",
      " [  29.45581917  106.04594712   93.10835795]\n",
      " [  32.74891879  111.95764956   93.82222541]\n",
      " [  26.9057316   101.91378407   95.29282598]\n",
      " [  18.30361511   81.4291492    96.9114671 ]]\n",
      "[[ 23.17315727  80.6472937   57.24725467]\n",
      " [ 23.15143036  76.70620048  69.74401346]\n",
      " [ 24.26964075  73.30315528  63.85254783]\n",
      " [ 22.37687289  64.6389973   68.39513478]\n",
      " [ 23.01884486  61.82683048  67.65033361]\n",
      " [ 21.20674022  52.61045501  68.95190588]]\n",
      "[[  27.5372259   118.00620286   88.86506338]\n",
      " [  34.41163138  123.45155615  100.64716617]\n",
      " [  25.51724529  115.63396315   81.17934113]\n",
      " [  22.95644867  102.09286587   81.23602629]\n",
      " [  15.82996127   89.4056983    71.43336607]\n",
      " [   8.88737711   69.93202245   59.49791752]]\n",
      "[[ 20.66677201  72.31649095  65.93537977]\n",
      " [ 21.31236385  73.57965456  71.92879455]\n",
      " [ 24.15759685  74.20410282  73.84766299]\n",
      " [ 21.90766055  66.69843848  73.31096523]\n",
      " [ 22.71780038  65.24124178  75.45697902]\n",
      " [ 23.98567818  66.6551409   77.60844564]]\n",
      "[[  29.41291663   93.25898041   87.5692833 ]\n",
      " [  30.89143415   83.94661912  102.65316042]\n",
      " [  38.62697517   83.44740371   93.17535274]\n",
      " [  41.43334819   77.44426024   80.17555038]\n",
      " [  30.60486678   59.94179491   54.20553446]\n",
      " [  28.82100399   62.15462797   49.96634225]]\n",
      "[[  33.18896157   94.51004868   83.19726022]\n",
      " [  32.49302428   87.35674633   86.42272474]\n",
      " [  27.20112278   80.95871931   96.34306129]\n",
      " [  24.56432094   74.46655164  104.62663522]\n",
      " [  22.8970236    65.350233    114.18363979]\n",
      " [  26.50383083   60.62573636  115.45927445]]\n",
      "[[  3.04395276  38.89358267  49.40174405]\n",
      " [-12.555785    23.70105674  42.93318483]\n",
      " [-11.33459539  23.13192663  42.60577346]\n",
      " [ -5.9449463   19.63325431  31.2031829 ]\n",
      " [-10.7264422    4.4583886    6.9452895 ]\n",
      " [-23.7142577  -13.02042371 -20.02119229]]\n",
      "[[  18.11897177   79.62028769   92.7908805 ]\n",
      " [  13.44109309   73.03456402  112.96135922]\n",
      " [  12.68483908   71.38061999  127.28214614]\n",
      " [  12.13419179   66.91347605  139.80579605]\n",
      " [  10.46837946   62.14781845  152.89402969]\n",
      " [   8.83423804   58.97798687  164.15362482]]\n",
      "[[ 13.12035981  14.16679902  56.64723587]\n",
      " [ 13.80476385  15.16812217  63.17376221]\n",
      " [ 14.11165813  13.46152934  63.81715362]\n",
      " [ 13.95810474   9.6028908   63.53754044]\n",
      " [ 11.23958538   2.20644036  62.55716214]\n",
      " [  6.85864816  -8.50755047  58.84819686]]\n",
      "[[ 13.15385085   5.71881931  66.35249529]\n",
      " [ 12.88440511  -0.16047524  71.83312613]\n",
      " [ 12.70875549  -5.44089289  72.56478025]\n",
      " [  8.26505216 -18.11656628  72.42395122]\n",
      " [  8.28978708 -20.87094065  72.90402808]\n",
      " [  5.84504938 -28.20544272  71.31129303]]\n",
      "[[  42.54534453   41.29566388   98.02156008]\n",
      " [  42.54673126   40.83343986  109.1494197 ]\n",
      " [  41.97219967   25.55406762  100.22737315]\n",
      " [  40.94061121   17.3358901   108.29151364]\n",
      " [  31.41968336    2.42620628  126.90511936]\n",
      " [  20.93294221  -24.51204806  129.36507951]]\n",
      "[[ 17.18319594  -0.96736996  63.75704493]\n",
      " [ 16.80302982 -13.72137384  72.55537689]\n",
      " [ 13.66276099 -27.96603743  80.62797115]\n",
      " [ 13.58045465 -37.62365622  83.97264769]\n",
      " [  9.70896709 -52.90118206  90.65174009]\n",
      " [  8.63423664 -61.69132354  96.12734561]]\n"
     ]
    }
   ],
   "source": [
    "for i in allAns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
